{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IREE \u00b6 IREE ( I ntermediate R epresentation E xecution E nvironment 1 ) is an MLIR -based end-to-end compiler that lowers Machine Learning (ML) models to a unified IR optimized for real-time inference on mobile/edge devices against heterogeneous hardware accelerators. IREE also provides flexible deployment solutions for its compiled ML models. Key features \u00b6 Ahead-of-time compilation of scheduling and execution logic together Support for dynamic shapes, flow control, streaming, and other advanced model features Optimized for many CPU and GPU architectures Low overhead, pipelined execution for efficient power and resource usage Binary size as low as 30KB on embedded systems Debugging and profiling support Support matrix \u00b6 IREE supports importing from a variety of ML frameworks: TensorFlow TensorFlow Lite JAX PyTorch The IREE compiler tools run on Linux, Windows, and macOS and can generate efficient code for a variety of runtime platforms: Linux Windows macOS iOS Android Bare metal WebAssembly and architectures: ARM x86 RISC-V Support for hardware accelerators and APIs is also included: Vulkan CUDA Metal WebGPU Project architecture \u00b6 IREE adopts a holistic approach towards ML model compilation: the IR produced contains both the scheduling logic, required to communicate data dependencies to low-level parallel pipelined hardware/API like Vulkan , and the execution logic, encoding dense computation on the hardware in the form of hardware/API-specific binaries like SPIR-V . Workflow overview \u00b6 Using IREE involves these general steps: Import your model Work in your framework of choice, then run your model through one of IREE's import tools. Select your deployment configuration Identify your target platform, accelerator(s), and other constraints. Compile your model Compile through IREE, picking compilation targets based on your deployment configuration. Run your model Use IREE's runtime components to execute your compiled model. Importing models from ML frameworks \u00b6 IREE supports importing models from a growing list of ML frameworks and model formats: TensorFlow TensorFlow Lite JAX Selecting deployment configurations \u00b6 IREE provides a flexible set of tools for various deployment scenarios. Fully featured environments can use IREE for dynamic model deployments taking advantage of multi-threaded hardware, while embedded systems can bypass IREE's runtime entirely or interface with custom accelerators. What platforms are you targeting? Desktop? Mobile? An embedded system? What hardware should the bulk of your model run on? CPU? GPU? How fixed is your model itself? Can the weights be changed? Do you want to support loading different model architectures dynamically? IREE supports the full set of these configurations using the same underlying technology. Compiling models \u00b6 Model compilation is performed ahead-of-time on a host machine for any combination of targets . The compilation process converts from layers and operators used by high level frameworks down into optimized native code and associated scheduling logic. For example, compiling for GPU execution using Vulkan generates SPIR-V kernels and Vulkan API calls. For CPU execution , native code with static or dynamic linkage and the associated function calls are generated. Running models \u00b6 IREE offers a low level C API, as well as several specialized sets of bindings for running IREE models using other languages: C API Python TensorFlow Lite Communication channels \u00b6 GitHub issues : Feature requests, bugs, and other work tracking IREE Discord server : Daily development discussions with the core team and collaborators iree-discuss email list : Announcements, general and low-priority discussion Roadmap \u00b6 IREE is in the early stages of development and is not yet ready for broad adoption. Check out the long-term design roadmap to get a sense of where we're headed. We plan on a quarterly basis using OKRs . Review our latest objectives to see what we're up to. We use GitHub Projects to track progress on IREE components and specific efforts and GitHub Milestones to track the work associated with plans for each quarter. Pronounced \"eerie\" and often styled with the emoji \u21a9","title":"Home"},{"location":"#iree","text":"IREE ( I ntermediate R epresentation E xecution E nvironment 1 ) is an MLIR -based end-to-end compiler that lowers Machine Learning (ML) models to a unified IR optimized for real-time inference on mobile/edge devices against heterogeneous hardware accelerators. IREE also provides flexible deployment solutions for its compiled ML models.","title":"IREE"},{"location":"#key-features","text":"Ahead-of-time compilation of scheduling and execution logic together Support for dynamic shapes, flow control, streaming, and other advanced model features Optimized for many CPU and GPU architectures Low overhead, pipelined execution for efficient power and resource usage Binary size as low as 30KB on embedded systems Debugging and profiling support","title":"Key features"},{"location":"#support-matrix","text":"IREE supports importing from a variety of ML frameworks: TensorFlow TensorFlow Lite JAX PyTorch The IREE compiler tools run on Linux, Windows, and macOS and can generate efficient code for a variety of runtime platforms: Linux Windows macOS iOS Android Bare metal WebAssembly and architectures: ARM x86 RISC-V Support for hardware accelerators and APIs is also included: Vulkan CUDA Metal WebGPU","title":"Support matrix"},{"location":"#project-architecture","text":"IREE adopts a holistic approach towards ML model compilation: the IR produced contains both the scheduling logic, required to communicate data dependencies to low-level parallel pipelined hardware/API like Vulkan , and the execution logic, encoding dense computation on the hardware in the form of hardware/API-specific binaries like SPIR-V .","title":"Project architecture"},{"location":"#workflow-overview","text":"Using IREE involves these general steps: Import your model Work in your framework of choice, then run your model through one of IREE's import tools. Select your deployment configuration Identify your target platform, accelerator(s), and other constraints. Compile your model Compile through IREE, picking compilation targets based on your deployment configuration. Run your model Use IREE's runtime components to execute your compiled model.","title":"Workflow overview"},{"location":"#importing-models-from-ml-frameworks","text":"IREE supports importing models from a growing list of ML frameworks and model formats: TensorFlow TensorFlow Lite JAX","title":"Importing models from ML frameworks"},{"location":"#selecting-deployment-configurations","text":"IREE provides a flexible set of tools for various deployment scenarios. Fully featured environments can use IREE for dynamic model deployments taking advantage of multi-threaded hardware, while embedded systems can bypass IREE's runtime entirely or interface with custom accelerators. What platforms are you targeting? Desktop? Mobile? An embedded system? What hardware should the bulk of your model run on? CPU? GPU? How fixed is your model itself? Can the weights be changed? Do you want to support loading different model architectures dynamically? IREE supports the full set of these configurations using the same underlying technology.","title":"Selecting deployment configurations"},{"location":"#compiling-models","text":"Model compilation is performed ahead-of-time on a host machine for any combination of targets . The compilation process converts from layers and operators used by high level frameworks down into optimized native code and associated scheduling logic. For example, compiling for GPU execution using Vulkan generates SPIR-V kernels and Vulkan API calls. For CPU execution , native code with static or dynamic linkage and the associated function calls are generated.","title":"Compiling models"},{"location":"#running-models","text":"IREE offers a low level C API, as well as several specialized sets of bindings for running IREE models using other languages: C API Python TensorFlow Lite","title":"Running models"},{"location":"#communication-channels","text":"GitHub issues : Feature requests, bugs, and other work tracking IREE Discord server : Daily development discussions with the core team and collaborators iree-discuss email list : Announcements, general and low-priority discussion","title":"Communication channels"},{"location":"#roadmap","text":"IREE is in the early stages of development and is not yet ready for broad adoption. Check out the long-term design roadmap to get a sense of where we're headed. We plan on a quarterly basis using OKRs . Review our latest objectives to see what we're up to. We use GitHub Projects to track progress on IREE components and specific efforts and GitHub Milestones to track the work associated with plans for each quarter. Pronounced \"eerie\" and often styled with the emoji \u21a9","title":"Roadmap"},{"location":"bindings/c-api/","text":"C API bindings \u00b6 IREE provides a low level C API for its runtime 1 , which can be used directly or through higher level APIs and language bindings built on top of it. API header files are organized by runtime component: Component header file Overview iree/base/api.h Core API, type definitions, ownership policies, utilities iree/vm/api.h VM APIs: loading modules, I/O, calling functions iree/hal/api.h HAL APIs: device management, synchronization, accessing hardware features The iree/samples/ directory demonstrates several ways to use IREE's C API. Prerequisites \u00b6 To use IREE's C API, you will need to build the runtime from source . The iree-template-cpp community project also shows how to integrate IREE into an external project using CMake 2 . Concepts \u00b6 By default, IREE uses its own tiny Virtual Machine (VM) at runtime to interpret program instructions on the host system. VM instructions may also be lowered further to LLVM IR, C, or other representations for static or resource constrained deployment. The VM supports generic operations like loads, stores, arithmetic, function calls, and control flow. It builds streams of more complex program logic and dense math into command buffers that are dispatched to hardware backends through the Hardware Abstraction Layer (HAL) interface. Most interaction with IREE's C API involves either the VM or the HAL. IREE VM \u00b6 VM instances can serve multiple isolated execution contexts VM contexts are effectively sandboxes for loading modules and running programs VM modules provide extra functionality to execution contexts , such as access to hardware accelerators through the HAL. Compiled user programs are also modules. IREE HAL \u00b6 HAL drivers are used to enumerate and create HAL devices HAL devices interface with hardware, such as by allocating device memory, preparing executables, recording and dispatching command buffers, and synchronizing with the host HAL buffers and buffer views represent storage and shaped/typed views into that storage (aka \"tensors\") Using the C API \u00b6 Setup \u00b6 Include headers: #include \"iree/base/api.h\" #include \"iree/hal/api.h\" #include \"iree/vm/api.h\" // The VM bytecode and HAL modules will typically be included, along // with those for the specific HAL drivers your application uses. // Functionality extensions can be used via custom modules. #include \"iree/modules/hal/hal_module.h\" #include \"iree/hal/dylib/registration/driver_module.h\" #include \"iree/vm/bytecode_module.h\" Check the API version and register components: // Statically linking the runtime should never have version conflicts, // however dynamically linking against a runtime shared object must // always verify that the version is as expected. iree_api_version_t actual_version ; IREE_CHECK_OK ( iree_api_version_check ( IREE_API_VERSION_LATEST , & actual_version )); // Modules with custom types must be statically registered before use. IREE_CHECK_OK ( iree_hal_module_register_types ()); // Device drivers are managed through registries. // Applications may use multiple registries to more finely control driver // lifetimes and visibility. IREE_CHECK_OK ( iree_hal_dylib_driver_module_register ( iree_hal_driver_registry_default ())); Tip The IREE_CHECK_OK() macro calls assert() if an error occurs. Applications should propagate errors and handle or report them as desired. Configure stateful objects \u00b6 Create a VM instance along with a HAL driver and device: // Applications should try to reuse instances so resource usage across contexts // is handled and extraneous device interaction is avoided. iree_vm_instance_t * instance = NULL ; IREE_CHECK_OK ( iree_vm_instance_create ( iree_allocator_system (), & instance )); // We use the CPU \"dylib\" driver in this example, but could use a different // driver like the GPU \"vulkan\" driver. The driver(s) used should match with // the target(s) specified during compilation. iree_hal_driver_t * driver = NULL ; IREE_CHECK_OK ( iree_hal_driver_registry_try_create_by_name ( iree_hal_driver_registry_default (), iree_string_view_literal ( \"dylib\" ), iree_allocator_system (), & driver )); // Drivers may support multiple devices, such as when a machine has multiple // GPUs. You may either enumerate devices and select based on their properties, // or just use the default device. iree_hal_device_t * device = NULL ; IREE_CHECK_OK ( iree_hal_driver_create_default_device ( driver , iree_allocator_system (), & device )); // Create a HAL module initialized to use the newly created device. // We'll load this module into a VM context later. iree_vm_module_t * hal_module = NULL ; IREE_CHECK_OK ( iree_hal_module_create ( device , iree_allocator_system (), & hal_module )); // The reference to the driver can be released now. iree_hal_driver_release ( driver ); Tip The default iree_allocator_system() is backed by malloc and free , but custom allocators may also be used. Load a vmfb bytecode module containing program data: // (Application-specific loading into memory, such as loading from a file) iree_vm_module_t * bytecode_module = NULL ; IREE_CHECK_OK ( iree_vm_bytecode_module_create ( iree_const_byte_span_t { module_data , module_size }, /*flatbuffer_allocator=*/ iree_allocator_null (), /*allocator=*/ iree_allocator_system (), & bytecode_module )); Note Many IREE samples use c_embed_data to embed vmfb files as C code to avoid file I/O and ease portability. Applications should use what makes sense for their platforms and deployment configurations. Create a VM context and load modules into it: iree_vm_context_t * context = NULL ; iree_vm_module_t * modules [ 2 ] = { hal_module , bytecode_module }; IREE_CHECK_OK ( iree_vm_context_create_with_modules ( instance , modules , IREE_ARRAYSIZE ( modules ), iree_allocator_system (), & context )); // References to the modules can be released now. iree_vm_module_release ( hal_module ); iree_vm_module_release ( bytecode_module ); Look up the function(s) to call: iree_vm_function_t main_function ; IREE_CHECK_OK ( iree_vm_context_resolve_function ( context , iree_string_view_literal ( \"module.main_function\" ), & main_function )); Invoke functions \u00b6 // (Application-specific I/O buffer setup, making data available to the device) IREE_CHECK_OK ( iree_vm_invoke ( context , main_function , /*policy=*/ NULL , inputs , outputs , iree_allocator_system ())); // (Application-specific output buffer retrieval and reading back from the device) Cleanup resources \u00b6 iree_hal_device_release ( device ); iree_vm_context_release ( context ); iree_vm_instance_release ( instance ); We are exploring adding a C API for IREE's compiler, see this GitHub issue \u21a9 We plan on deploying via vcpkg in the future too, see this GitHub project \u21a9","title":"C API"},{"location":"bindings/c-api/#c-api-bindings","text":"IREE provides a low level C API for its runtime 1 , which can be used directly or through higher level APIs and language bindings built on top of it. API header files are organized by runtime component: Component header file Overview iree/base/api.h Core API, type definitions, ownership policies, utilities iree/vm/api.h VM APIs: loading modules, I/O, calling functions iree/hal/api.h HAL APIs: device management, synchronization, accessing hardware features The iree/samples/ directory demonstrates several ways to use IREE's C API.","title":"C API bindings"},{"location":"bindings/c-api/#prerequisites","text":"To use IREE's C API, you will need to build the runtime from source . The iree-template-cpp community project also shows how to integrate IREE into an external project using CMake 2 .","title":"Prerequisites"},{"location":"bindings/c-api/#concepts","text":"By default, IREE uses its own tiny Virtual Machine (VM) at runtime to interpret program instructions on the host system. VM instructions may also be lowered further to LLVM IR, C, or other representations for static or resource constrained deployment. The VM supports generic operations like loads, stores, arithmetic, function calls, and control flow. It builds streams of more complex program logic and dense math into command buffers that are dispatched to hardware backends through the Hardware Abstraction Layer (HAL) interface. Most interaction with IREE's C API involves either the VM or the HAL.","title":"Concepts"},{"location":"bindings/c-api/#iree-vm","text":"VM instances can serve multiple isolated execution contexts VM contexts are effectively sandboxes for loading modules and running programs VM modules provide extra functionality to execution contexts , such as access to hardware accelerators through the HAL. Compiled user programs are also modules.","title":"IREE VM"},{"location":"bindings/c-api/#iree-hal","text":"HAL drivers are used to enumerate and create HAL devices HAL devices interface with hardware, such as by allocating device memory, preparing executables, recording and dispatching command buffers, and synchronizing with the host HAL buffers and buffer views represent storage and shaped/typed views into that storage (aka \"tensors\")","title":"IREE HAL"},{"location":"bindings/c-api/#using-the-c-api","text":"","title":"Using the C API"},{"location":"bindings/c-api/#setup","text":"Include headers: #include \"iree/base/api.h\" #include \"iree/hal/api.h\" #include \"iree/vm/api.h\" // The VM bytecode and HAL modules will typically be included, along // with those for the specific HAL drivers your application uses. // Functionality extensions can be used via custom modules. #include \"iree/modules/hal/hal_module.h\" #include \"iree/hal/dylib/registration/driver_module.h\" #include \"iree/vm/bytecode_module.h\" Check the API version and register components: // Statically linking the runtime should never have version conflicts, // however dynamically linking against a runtime shared object must // always verify that the version is as expected. iree_api_version_t actual_version ; IREE_CHECK_OK ( iree_api_version_check ( IREE_API_VERSION_LATEST , & actual_version )); // Modules with custom types must be statically registered before use. IREE_CHECK_OK ( iree_hal_module_register_types ()); // Device drivers are managed through registries. // Applications may use multiple registries to more finely control driver // lifetimes and visibility. IREE_CHECK_OK ( iree_hal_dylib_driver_module_register ( iree_hal_driver_registry_default ())); Tip The IREE_CHECK_OK() macro calls assert() if an error occurs. Applications should propagate errors and handle or report them as desired.","title":"Setup"},{"location":"bindings/c-api/#configure-stateful-objects","text":"Create a VM instance along with a HAL driver and device: // Applications should try to reuse instances so resource usage across contexts // is handled and extraneous device interaction is avoided. iree_vm_instance_t * instance = NULL ; IREE_CHECK_OK ( iree_vm_instance_create ( iree_allocator_system (), & instance )); // We use the CPU \"dylib\" driver in this example, but could use a different // driver like the GPU \"vulkan\" driver. The driver(s) used should match with // the target(s) specified during compilation. iree_hal_driver_t * driver = NULL ; IREE_CHECK_OK ( iree_hal_driver_registry_try_create_by_name ( iree_hal_driver_registry_default (), iree_string_view_literal ( \"dylib\" ), iree_allocator_system (), & driver )); // Drivers may support multiple devices, such as when a machine has multiple // GPUs. You may either enumerate devices and select based on their properties, // or just use the default device. iree_hal_device_t * device = NULL ; IREE_CHECK_OK ( iree_hal_driver_create_default_device ( driver , iree_allocator_system (), & device )); // Create a HAL module initialized to use the newly created device. // We'll load this module into a VM context later. iree_vm_module_t * hal_module = NULL ; IREE_CHECK_OK ( iree_hal_module_create ( device , iree_allocator_system (), & hal_module )); // The reference to the driver can be released now. iree_hal_driver_release ( driver ); Tip The default iree_allocator_system() is backed by malloc and free , but custom allocators may also be used. Load a vmfb bytecode module containing program data: // (Application-specific loading into memory, such as loading from a file) iree_vm_module_t * bytecode_module = NULL ; IREE_CHECK_OK ( iree_vm_bytecode_module_create ( iree_const_byte_span_t { module_data , module_size }, /*flatbuffer_allocator=*/ iree_allocator_null (), /*allocator=*/ iree_allocator_system (), & bytecode_module )); Note Many IREE samples use c_embed_data to embed vmfb files as C code to avoid file I/O and ease portability. Applications should use what makes sense for their platforms and deployment configurations. Create a VM context and load modules into it: iree_vm_context_t * context = NULL ; iree_vm_module_t * modules [ 2 ] = { hal_module , bytecode_module }; IREE_CHECK_OK ( iree_vm_context_create_with_modules ( instance , modules , IREE_ARRAYSIZE ( modules ), iree_allocator_system (), & context )); // References to the modules can be released now. iree_vm_module_release ( hal_module ); iree_vm_module_release ( bytecode_module ); Look up the function(s) to call: iree_vm_function_t main_function ; IREE_CHECK_OK ( iree_vm_context_resolve_function ( context , iree_string_view_literal ( \"module.main_function\" ), & main_function ));","title":"Configure stateful objects"},{"location":"bindings/c-api/#invoke-functions","text":"// (Application-specific I/O buffer setup, making data available to the device) IREE_CHECK_OK ( iree_vm_invoke ( context , main_function , /*policy=*/ NULL , inputs , outputs , iree_allocator_system ())); // (Application-specific output buffer retrieval and reading back from the device)","title":"Invoke functions"},{"location":"bindings/c-api/#cleanup-resources","text":"iree_hal_device_release ( device ); iree_vm_context_release ( context ); iree_vm_instance_release ( instance ); We are exploring adding a C API for IREE's compiler, see this GitHub issue \u21a9 We plan on deploying via vcpkg in the future too, see this GitHub project \u21a9","title":"Cleanup resources"},{"location":"bindings/python/","text":"Python bindings \u00b6 IREE offers Python bindings split into several packages, covering different components: PIP package name Description iree-compiler-snapshot IREE's generic compiler tools and helpers iree-runtime-snapshot IREE's runtime, including CPU and GPU backends iree-tools-tf-snapshot Tools for importing from TensorFlow iree-tools-tflite-snapshot Tools for importing from TensorFlow Lite iree-tools-xla-snapshot Tools for importing from XLA Collectively, these packages allow for importing from frontends, compiling towards various targets, and executing compiled code on IREE's backends. Prerequisites \u00b6 To use IREE's Python bindings, you will first need to install Python 3 and pip , as needed. Tip We recommend using virtual environments to manage python packages, such as through venv ( about , tutorial ): Linux python -m venv .venv source .venv/bin/activate Windows python -m venv . venv . venv \\ Scripts \\ activate . bat When done, run deactivate . Next, install packages: python -m pip install --upgrade pip python -m pip install numpy absl-py Installing IREE packages \u00b6 Prebuilt packages \u00b6 For now, packages can be installed from our GitHub releases : Minimal To install just the core IREE packages: python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ --find-links https://github.com/google/iree/releases All packages To install IREE packages with tools for all frontends: python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ iree-tools-tf-snapshot \\ iree-tools-tflite-snapshot \\ iree-tools-xla-snapshot \\ --find-links https://github.com/google/iree/releases Info We plan to publish packages on PyPI as they become more stable. Building from source \u00b6 See Building Python bindings page for instructions for building from source. Using the Python bindings \u00b6 Troubleshooting \u00b6","title":"Python"},{"location":"bindings/python/#python-bindings","text":"IREE offers Python bindings split into several packages, covering different components: PIP package name Description iree-compiler-snapshot IREE's generic compiler tools and helpers iree-runtime-snapshot IREE's runtime, including CPU and GPU backends iree-tools-tf-snapshot Tools for importing from TensorFlow iree-tools-tflite-snapshot Tools for importing from TensorFlow Lite iree-tools-xla-snapshot Tools for importing from XLA Collectively, these packages allow for importing from frontends, compiling towards various targets, and executing compiled code on IREE's backends.","title":"Python bindings"},{"location":"bindings/python/#prerequisites","text":"To use IREE's Python bindings, you will first need to install Python 3 and pip , as needed. Tip We recommend using virtual environments to manage python packages, such as through venv ( about , tutorial ): Linux python -m venv .venv source .venv/bin/activate Windows python -m venv . venv . venv \\ Scripts \\ activate . bat When done, run deactivate . Next, install packages: python -m pip install --upgrade pip python -m pip install numpy absl-py","title":"Prerequisites"},{"location":"bindings/python/#installing-iree-packages","text":"","title":"Installing IREE packages"},{"location":"bindings/python/#prebuilt-packages","text":"For now, packages can be installed from our GitHub releases : Minimal To install just the core IREE packages: python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ --find-links https://github.com/google/iree/releases All packages To install IREE packages with tools for all frontends: python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ iree-tools-tf-snapshot \\ iree-tools-tflite-snapshot \\ iree-tools-xla-snapshot \\ --find-links https://github.com/google/iree/releases Info We plan to publish packages on PyPI as they become more stable.","title":"Prebuilt packages"},{"location":"bindings/python/#building-from-source","text":"See Building Python bindings page for instructions for building from source.","title":"Building from source"},{"location":"bindings/python/#using-the-python-bindings","text":"","title":"Using the Python bindings"},{"location":"bindings/python/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"bindings/tensorflow-lite/","text":"TensorFlow Lite bindings \u00b6 Todo Issue#5462 : write this documentation","title":"TensorFlow Lite"},{"location":"bindings/tensorflow-lite/#tensorflow-lite-bindings","text":"Todo Issue#5462 : write this documentation","title":"TensorFlow Lite bindings"},{"location":"building-from-source/","text":"Building IREE from source \u00b6 Under construction.","title":"Building IREE from source"},{"location":"building-from-source/#building-iree-from-source","text":"Under construction.","title":"Building IREE from source"},{"location":"building-from-source/android/","text":"Android cross-compilation \u00b6 Running on a platform like Android involves cross-compiling from a host platform (e.g. Linux) to a target platform (a specific Android version and system architecture): IREE's compiler is built on the host and is used there to generate modules for the target IREE's runtime is built on the host for the target. The runtime is then either pushed to the target to run natively or is bundled into an Android APK Prerequisites \u00b6 Host environment setup \u00b6 You should already be able to build IREE from source on your host platform. Please make sure you have followed the getting started steps. Install Android NDK and ADB \u00b6 The Android Native Developer Kit (NDK) is needed to use native C/C++ code on Android. You can download it here , or, if you have installed Android Studio , you can follow this guide instead. Note Make sure the ANDROID_NDK environment variable is set after installing the NDK. ADB (the Android Debug Bridge) is also needed to communicate with Android devices from the command line. Install it following the official user guide . Configure and build \u00b6 Host configuration \u00b6 Build and install on your host machine: cmake -B ../iree-build/ -DCMAKE_INSTALL_PREFIX = ../iree-build/install . cmake --build ../iree-build/ --target install Target configuration \u00b6 Build the runtime using the Android NDK toolchain: Linux cmake -B ../iree-build-android/ \\ -DCMAKE_TOOLCHAIN_FILE = \" ${ ANDROID_NDK ? } /build/cmake/android.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = \" $PWD /../iree-build/install\" \\ -DANDROID_ABI = \"arm64-v8a\" \\ -DANDROID_PLATFORM = \"android-29\" \\ -DIREE_BUILD_COMPILER = OFF \\ . cmake --build ../iree-build-android/ Windows cmake -B ../iree-build-android/ \\ -DCMAKE_TOOLCHAIN_FILE = \"%ANDROID_NDK%/build/cmake/android.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = \"%CD%/../iree-build/install\" \\ -DANDROID_ABI = \"arm64-v8a\" \\ -DANDROID_PLATFORM = \"android-29\" \\ -DIREE_BUILD_COMPILER = OFF \\ . cmake --build ../iree-build-android/ Note See the Android NDK CMake guide and Android Studio CMake guide for details on configuring CMake for Android. The specific ANDROID_ABI and ANDROID_PLATFORM used should match your target device. Running Android tests \u00b6 Make sure you enable developer options and USB debugging on your Android device and can see your it when you run adb devices , then run all built tests through CTest : cd ../iree-build-android/ ctest --output-on-failure This will automatically upload build artifacts to the connected Android device, run the tests, then report the status back to your host machine. Running tools directly \u00b6 Invoke the host compiler tools produce input files: ../iree-build/install/bin/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = vmla \\ iree/tools/test/iree-run-module.mlir \\ -o /tmp/iree-run-module-vmla.vmfb Push the Android runtime tools to the device, along with any input files: adb push ../iree-build-android/iree/tools/iree-run-module /data/local/tmp/ adb shell chmod +x /data/local/tmp/iree-run-module adb push /tmp/iree-run-module-vmla.vmfb /data/local/tmp/ Run the tool: adb shell /data/local/tmp/iree-run-module -driver = vmla \\ -module_file = /data/local/tmp/iree-run-module-vmla.vmfb \\ -entry_function = abs \\ -function_inputs = \"i32=-5\"","title":"Android cross-compilation"},{"location":"building-from-source/android/#android-cross-compilation","text":"Running on a platform like Android involves cross-compiling from a host platform (e.g. Linux) to a target platform (a specific Android version and system architecture): IREE's compiler is built on the host and is used there to generate modules for the target IREE's runtime is built on the host for the target. The runtime is then either pushed to the target to run natively or is bundled into an Android APK","title":"Android cross-compilation"},{"location":"building-from-source/android/#prerequisites","text":"","title":"Prerequisites"},{"location":"building-from-source/android/#host-environment-setup","text":"You should already be able to build IREE from source on your host platform. Please make sure you have followed the getting started steps.","title":"Host environment setup"},{"location":"building-from-source/android/#install-android-ndk-and-adb","text":"The Android Native Developer Kit (NDK) is needed to use native C/C++ code on Android. You can download it here , or, if you have installed Android Studio , you can follow this guide instead. Note Make sure the ANDROID_NDK environment variable is set after installing the NDK. ADB (the Android Debug Bridge) is also needed to communicate with Android devices from the command line. Install it following the official user guide .","title":"Install Android NDK and ADB"},{"location":"building-from-source/android/#configure-and-build","text":"","title":"Configure and build"},{"location":"building-from-source/android/#host-configuration","text":"Build and install on your host machine: cmake -B ../iree-build/ -DCMAKE_INSTALL_PREFIX = ../iree-build/install . cmake --build ../iree-build/ --target install","title":"Host configuration"},{"location":"building-from-source/android/#target-configuration","text":"Build the runtime using the Android NDK toolchain: Linux cmake -B ../iree-build-android/ \\ -DCMAKE_TOOLCHAIN_FILE = \" ${ ANDROID_NDK ? } /build/cmake/android.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = \" $PWD /../iree-build/install\" \\ -DANDROID_ABI = \"arm64-v8a\" \\ -DANDROID_PLATFORM = \"android-29\" \\ -DIREE_BUILD_COMPILER = OFF \\ . cmake --build ../iree-build-android/ Windows cmake -B ../iree-build-android/ \\ -DCMAKE_TOOLCHAIN_FILE = \"%ANDROID_NDK%/build/cmake/android.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = \"%CD%/../iree-build/install\" \\ -DANDROID_ABI = \"arm64-v8a\" \\ -DANDROID_PLATFORM = \"android-29\" \\ -DIREE_BUILD_COMPILER = OFF \\ . cmake --build ../iree-build-android/ Note See the Android NDK CMake guide and Android Studio CMake guide for details on configuring CMake for Android. The specific ANDROID_ABI and ANDROID_PLATFORM used should match your target device.","title":"Target configuration"},{"location":"building-from-source/android/#running-android-tests","text":"Make sure you enable developer options and USB debugging on your Android device and can see your it when you run adb devices , then run all built tests through CTest : cd ../iree-build-android/ ctest --output-on-failure This will automatically upload build artifacts to the connected Android device, run the tests, then report the status back to your host machine.","title":"Running Android tests"},{"location":"building-from-source/android/#running-tools-directly","text":"Invoke the host compiler tools produce input files: ../iree-build/install/bin/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = vmla \\ iree/tools/test/iree-run-module.mlir \\ -o /tmp/iree-run-module-vmla.vmfb Push the Android runtime tools to the device, along with any input files: adb push ../iree-build-android/iree/tools/iree-run-module /data/local/tmp/ adb shell chmod +x /data/local/tmp/iree-run-module adb push /tmp/iree-run-module-vmla.vmfb /data/local/tmp/ Run the tool: adb shell /data/local/tmp/iree-run-module -driver = vmla \\ -module_file = /data/local/tmp/iree-run-module-vmla.vmfb \\ -entry_function = abs \\ -function_inputs = \"i32=-5\"","title":"Running tools directly"},{"location":"building-from-source/getting-started/","text":"Getting started \u00b6 Prerequisites \u00b6 You will need to install CMake , along with a C/C++ compiler: Linux sudo apt install cmake clang export CC = clang export CXX = clang++ Windows Install CMake from the official downloads page Install MSVC from Visual Studio or \"Tools for Visual Studio\" on the official downloads page Note You will need to initialize MSVC by running vcvarsall.bat to use it from the command line. See the official documentation for details. Clone and build \u00b6 Use Git to clone the IREE repository and initialize its submodules: git clone https://github.com/google/iree.git cd iree git submodule update --init Configure then build all targets using CMake: cmake -B ../iree-build/ . cmake --build ../iree-build/ Tip Most IREE Core devs use Ninja as the CMake generator. The benefit is that it works the same across all platforms and automatically takes advantage of parallelism. to use it, add a -GNinja argument to your initial cmake command (and make sure to install ninja-build from either your favorite OS package manager, or generically via python -m pip install ninja ). What's next? \u00b6 Running tests \u00b6 Run all built tests through CTest : cd ../iree-build/ ctest --output-on-failure Take a look around \u00b6 Check out the contents of the 'tools' build directory: ls ../iree-build/iree/tools/ ../iree-build/iree/tools/iree-translate --help","title":"Getting started"},{"location":"building-from-source/getting-started/#getting-started","text":"","title":"Getting started"},{"location":"building-from-source/getting-started/#prerequisites","text":"You will need to install CMake , along with a C/C++ compiler: Linux sudo apt install cmake clang export CC = clang export CXX = clang++ Windows Install CMake from the official downloads page Install MSVC from Visual Studio or \"Tools for Visual Studio\" on the official downloads page Note You will need to initialize MSVC by running vcvarsall.bat to use it from the command line. See the official documentation for details.","title":"Prerequisites"},{"location":"building-from-source/getting-started/#clone-and-build","text":"Use Git to clone the IREE repository and initialize its submodules: git clone https://github.com/google/iree.git cd iree git submodule update --init Configure then build all targets using CMake: cmake -B ../iree-build/ . cmake --build ../iree-build/ Tip Most IREE Core devs use Ninja as the CMake generator. The benefit is that it works the same across all platforms and automatically takes advantage of parallelism. to use it, add a -GNinja argument to your initial cmake command (and make sure to install ninja-build from either your favorite OS package manager, or generically via python -m pip install ninja ).","title":"Clone and build"},{"location":"building-from-source/getting-started/#whats-next","text":"","title":"What's next?"},{"location":"building-from-source/getting-started/#running-tests","text":"Run all built tests through CTest : cd ../iree-build/ ctest --output-on-failure","title":"Running tests"},{"location":"building-from-source/getting-started/#take-a-look-around","text":"Check out the contents of the 'tools' build directory: ls ../iree-build/iree/tools/ ../iree-build/iree/tools/iree-translate --help","title":"Take a look around"},{"location":"building-from-source/optional-features/","text":"Optional Features \u00b6 This page details the optional features and build modes for the project. Most of these are controlled by various CMake options, sometimes requiring extra setup or preparation. Each section extends the basic build steps in the getting started page. Building Python Bindings \u00b6 This section describes how to build and interactively use built-from-source Python bindings for the following packages: Python Import Description import iree.compiler IREE's generic compiler tools and helpers import iree.runtime IREE's runtime, including CPU and GPU backends Also see instructions for installing pre-built binaries . Pre-requisites: A relatively recent Python3 installation (we aim to support non-eol Python versions ). Installation of python dependencies as specified in bindings/python/build_requirements.txt . CMake Variables: IREE_BUILD_PYTHON_BINDINGS : BOOL Enables building of Python bindings under bindings/python in the repository. Defaults to OFF . Python3_EXECUTABLE : PATH Full path to the Python3 executable to build against. If not specified, CMake will auto-detect this, which often yields incorrect results on systems with multiple Python versions. Explicitly setting this is recommended. Note that mixed case of the option. Setup We recommend using virtual environments to manage python packages, such as through venv , which may need to be installed via your system package manager ( about , tutorial ): Linux and MacOS # Make sure your 'python' is what you expect. Note that on multi-python # systems, this may have a version suffix, and on many Linuxes and MacOS where # python2 and python3 co-exist, you may also want to use `python3`. which python python --version # Create a persistent virtual environment (first time only). python -m venv iree.venv # Activate the virtual environment (per shell). # Now the `python` command will resolve to your virtual environment # (even on systems where you typically use `python3`). source iree.venv/bin/activate # Upgrade PIP. On Linux, many packages cannot be installed for older # PIP versions. See: https://github.com/pypa/manylinux python -m pip install --upgrade pip # Install IREE build pre-requisites. python -m pip install -r ./bindings/python/build_requirements.txt Windows python -m venv . venv . venv \\ Scripts \\ activate . bat python -m pip install - -upgrade pip python -m pip install -r bindings \\ python \\ build_requirements . txt When done, close your shell or run deactivate . Usage From the iree-build directory: Linux and MacOS cmake -DIREE_BUILD_PYTHON_BINDINGS = ON -DPython3_EXECUTABLE = \" $( which python ) \" . cmake --build . # Add ./bindings/python to PYTHONPATH and use the API. export PYTHONPATH = \" $PWD /bindings/python\" python -c \"import iree.compiler\" python -c \"import iree.runtime\" Windows cmake -DIREE_BUILD_PYTHON_BINDINGS = ON . cmake - -build . # Add bindings\\python to PYTHONPATH and use the API. set PYTHONPATH = \"$pwd\\bindings\\python;%PYTHONPATH%\" python -c \"import iree.compiler\" python -c \"import iree.runtime\" Tests can now be run individually via python or via ctest. Building TensorFlow Frontend Bindings \u00b6 This section describes how to build compiler tools and Python bindings for importing models from various TensorFlow-ecosystem frontends, including TensorFlow, XLA (used for JAX), and TFLite. It extends the instructions in Building Python Bindings above with additional steps that are TensorFlow specific. There are various ways to achieve these ends, but this section describes the canonical method that the core developers recommend. Upon completing these steps, you will have access to additional Python packages: Python Import Description import iree.compiler.tools.tf Tools for importing from TensorFlow import iree.compiler.tools.tflite Tools for importing from TensorFlow Lite import iree.compiler.tools.xla Tools for importing from XLA These tools packages are needed in order for the frontend specific, high-level APIs under import iree.compiler.tf , import iree.compiler.tflite , import iree.compiler.xla , and import iree.jax to be fully functional. Setup A relatively recent tf-nightly release is needed to run tests. Linux and MacOS python -m pip install -r ./integrations/tensorflow/bindings/python/build_requirements.txt Windows python -m pip install -r integrations \\ tensorflow \\ bindings \\ python \\ build_requirements . txt Tensorflow TensorFlow frontends can only be built with Bazel , and this must be done as a manual step (we used to have automation for this, but Bazel integrates poorly with automation and it made diagnosis and cross platform usage unreliable). The recommended version of Bazel (used by CI systems) can be found in the .bazelversion file. In addition, Bazel is hard to use out of tree, so these steps will involve working from the source tree (instead of the build tree). Linux and MacOS # From the iree source directory. cd integrations/tensorflow python ../../configure_bazel.py bazel build iree_tf_compiler:importer-binaries Windows # From the iree source directory. cd integrations \\ tensorflow python ..\\..\\ configure_bazel . py bazel build iree_tf_compiler : importer-binaries Importer binaries can be found under bazel-bin/iree_tf_compiler and can be used from the command line if desired. Iree The main IREE build will embed binaries built above and enable additional Python APIs. Within the build, the binaries are symlinked, so can be rebuilt per above without re-running these steps for edit-and-continue style work. # From the iree-build/ directory. cmake -DIREE_BUILD_TENSORFLOW_ALL = ON . cmake --build . # Validate. python -c \"import iree.tools.tf as _; print(_.get_tool('iree-tf-import'))\" python -c \"import iree.tools.tflite as _; print(_.get_tool('iree-import-tflite'))\" python -c \"import iree.tools.xla as _; print(_.get_tool('iree-import-xla'))\"","title":"Optional features"},{"location":"building-from-source/optional-features/#optional-features","text":"This page details the optional features and build modes for the project. Most of these are controlled by various CMake options, sometimes requiring extra setup or preparation. Each section extends the basic build steps in the getting started page.","title":"Optional Features"},{"location":"building-from-source/optional-features/#building-python-bindings","text":"This section describes how to build and interactively use built-from-source Python bindings for the following packages: Python Import Description import iree.compiler IREE's generic compiler tools and helpers import iree.runtime IREE's runtime, including CPU and GPU backends Also see instructions for installing pre-built binaries . Pre-requisites: A relatively recent Python3 installation (we aim to support non-eol Python versions ). Installation of python dependencies as specified in bindings/python/build_requirements.txt . CMake Variables: IREE_BUILD_PYTHON_BINDINGS : BOOL Enables building of Python bindings under bindings/python in the repository. Defaults to OFF . Python3_EXECUTABLE : PATH Full path to the Python3 executable to build against. If not specified, CMake will auto-detect this, which often yields incorrect results on systems with multiple Python versions. Explicitly setting this is recommended. Note that mixed case of the option. Setup We recommend using virtual environments to manage python packages, such as through venv , which may need to be installed via your system package manager ( about , tutorial ): Linux and MacOS # Make sure your 'python' is what you expect. Note that on multi-python # systems, this may have a version suffix, and on many Linuxes and MacOS where # python2 and python3 co-exist, you may also want to use `python3`. which python python --version # Create a persistent virtual environment (first time only). python -m venv iree.venv # Activate the virtual environment (per shell). # Now the `python` command will resolve to your virtual environment # (even on systems where you typically use `python3`). source iree.venv/bin/activate # Upgrade PIP. On Linux, many packages cannot be installed for older # PIP versions. See: https://github.com/pypa/manylinux python -m pip install --upgrade pip # Install IREE build pre-requisites. python -m pip install -r ./bindings/python/build_requirements.txt Windows python -m venv . venv . venv \\ Scripts \\ activate . bat python -m pip install - -upgrade pip python -m pip install -r bindings \\ python \\ build_requirements . txt When done, close your shell or run deactivate . Usage From the iree-build directory: Linux and MacOS cmake -DIREE_BUILD_PYTHON_BINDINGS = ON -DPython3_EXECUTABLE = \" $( which python ) \" . cmake --build . # Add ./bindings/python to PYTHONPATH and use the API. export PYTHONPATH = \" $PWD /bindings/python\" python -c \"import iree.compiler\" python -c \"import iree.runtime\" Windows cmake -DIREE_BUILD_PYTHON_BINDINGS = ON . cmake - -build . # Add bindings\\python to PYTHONPATH and use the API. set PYTHONPATH = \"$pwd\\bindings\\python;%PYTHONPATH%\" python -c \"import iree.compiler\" python -c \"import iree.runtime\" Tests can now be run individually via python or via ctest.","title":"Building Python Bindings"},{"location":"building-from-source/optional-features/#building-tensorflow-frontend-bindings","text":"This section describes how to build compiler tools and Python bindings for importing models from various TensorFlow-ecosystem frontends, including TensorFlow, XLA (used for JAX), and TFLite. It extends the instructions in Building Python Bindings above with additional steps that are TensorFlow specific. There are various ways to achieve these ends, but this section describes the canonical method that the core developers recommend. Upon completing these steps, you will have access to additional Python packages: Python Import Description import iree.compiler.tools.tf Tools for importing from TensorFlow import iree.compiler.tools.tflite Tools for importing from TensorFlow Lite import iree.compiler.tools.xla Tools for importing from XLA These tools packages are needed in order for the frontend specific, high-level APIs under import iree.compiler.tf , import iree.compiler.tflite , import iree.compiler.xla , and import iree.jax to be fully functional. Setup A relatively recent tf-nightly release is needed to run tests. Linux and MacOS python -m pip install -r ./integrations/tensorflow/bindings/python/build_requirements.txt Windows python -m pip install -r integrations \\ tensorflow \\ bindings \\ python \\ build_requirements . txt Tensorflow TensorFlow frontends can only be built with Bazel , and this must be done as a manual step (we used to have automation for this, but Bazel integrates poorly with automation and it made diagnosis and cross platform usage unreliable). The recommended version of Bazel (used by CI systems) can be found in the .bazelversion file. In addition, Bazel is hard to use out of tree, so these steps will involve working from the source tree (instead of the build tree). Linux and MacOS # From the iree source directory. cd integrations/tensorflow python ../../configure_bazel.py bazel build iree_tf_compiler:importer-binaries Windows # From the iree source directory. cd integrations \\ tensorflow python ..\\..\\ configure_bazel . py bazel build iree_tf_compiler : importer-binaries Importer binaries can be found under bazel-bin/iree_tf_compiler and can be used from the command line if desired. Iree The main IREE build will embed binaries built above and enable additional Python APIs. Within the build, the binaries are symlinked, so can be rebuilt per above without re-running these steps for edit-and-continue style work. # From the iree-build/ directory. cmake -DIREE_BUILD_TENSORFLOW_ALL = ON . cmake --build . # Validate. python -c \"import iree.tools.tf as _; print(_.get_tool('iree-tf-import'))\" python -c \"import iree.tools.tflite as _; print(_.get_tool('iree-import-tflite'))\" python -c \"import iree.tools.xla as _; print(_.get_tool('iree-import-xla'))\"","title":"Building TensorFlow Frontend Bindings"},{"location":"community/projects/","text":"Community projects \u00b6 The IREE C++ Template demonstrates how to integrate IREE into a third-party project with CMake. The project demonstrates the usage of runtime support and how to use a custom dialect alongside with the runtime. The IREE LLVM Sandbox contains experimental work by the IREE team closely related to LLVM and MLIR, usually with the aim of contributing back to those upstream projects in some form.","title":"Projects"},{"location":"community/projects/#community-projects","text":"The IREE C++ Template demonstrates how to integrate IREE into a third-party project with CMake. The project demonstrates the usage of runtime support and how to use a custom dialect alongside with the runtime. The IREE LLVM Sandbox contains experimental work by the IREE team closely related to LLVM and MLIR, usually with the aim of contributing back to those upstream projects in some form.","title":"Community projects"},{"location":"deployment-configurations/cpu-dylib/","text":"Dynamic Library CPU HAL Driver \u00b6 IREE supports efficient model execution on CPU. IREE uses LLVM to compile dense computation in the model into highly optimized CPU native instruction streams, which are embedded in IREE's deployable format as dynamic libraries (dylibs). IREE uses its own low-overhead minimal dynamic library loader to load them and then schedule them with concrete workloads onto various CPU cores. Todo Add IREE's CPU support matrix: what architectures are supported; what architectures are well optimized; etc. Get runtime and compiler \u00b6 Get IREE runtime with dylib HAL driver \u00b6 You will need to get an IREE runtime that supports the dylib HAL driver so it can execute the model on CPU via dynamic libraries containing native CPU instructions. Build runtime from source \u00b6 Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The dylib HAL driver is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add DyLib to the IREE_HAL_DRIVERS_TO_BUILD CMake list variable when configuring (for target). Get compiler for CPU native instructions \u00b6 Download as Python package \u00b6 Python packages for various IREE functionalities are regularly published on IREE's GitHub Releases page. Right now these are just snapshots of the main development branch. You can install the Python package containing the LLVM-based dylib compiler by python -m pip install iree-compiler-snapshot \\ -f https://github.com/google/iree/releases Tip iree-translate is installed as /path/to/python/site-packages/iree/tools/core/iree-translate . You can find out the full path to the site-packages directory via the python -m site command. Build compiler from source \u00b6 Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The dylib compiler backend is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add DYLIB-LLVM-AOT to the IREE_TARGET_BACKENDS_TO_BUILD CMake list variable when configuring (for host). Compile and run the model \u00b6 With the compiler and runtime for dynamic libraries, we can now compile a model and run it on the CPU. Compile the model \u00b6 IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by main IREE compilers first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then, Compile using the command-line \u00b6 In the build directory, run the following command: iree/tools/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = dylib-llvm-aot \\ iree_input.mlir -o mobilenet-dylib.vmfb Todo Choose the suitable target triple for the current CPU where iree_input.mlir is the model's initial MLIR representation generated by IREE's TensorFlow importer. Run the model \u00b6 Run using the command-line \u00b6 In the build directory, run the following command: iree/tools/iree-run-module \\ --driver = dylib \\ --module_file = mobilenet-dylib.vmfb \\ --entry_function = predict \\ --function_inputs = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values.","title":"CPU - Dylib"},{"location":"deployment-configurations/cpu-dylib/#dynamic-library-cpu-hal-driver","text":"IREE supports efficient model execution on CPU. IREE uses LLVM to compile dense computation in the model into highly optimized CPU native instruction streams, which are embedded in IREE's deployable format as dynamic libraries (dylibs). IREE uses its own low-overhead minimal dynamic library loader to load them and then schedule them with concrete workloads onto various CPU cores. Todo Add IREE's CPU support matrix: what architectures are supported; what architectures are well optimized; etc.","title":"Dynamic Library CPU HAL Driver"},{"location":"deployment-configurations/cpu-dylib/#get-runtime-and-compiler","text":"","title":"Get runtime and compiler"},{"location":"deployment-configurations/cpu-dylib/#get-iree-runtime-with-dylib-hal-driver","text":"You will need to get an IREE runtime that supports the dylib HAL driver so it can execute the model on CPU via dynamic libraries containing native CPU instructions.","title":"Get IREE runtime with dylib HAL driver"},{"location":"deployment-configurations/cpu-dylib/#build-runtime-from-source","text":"Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The dylib HAL driver is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add DyLib to the IREE_HAL_DRIVERS_TO_BUILD CMake list variable when configuring (for target).","title":"Build runtime from source"},{"location":"deployment-configurations/cpu-dylib/#get-compiler-for-cpu-native-instructions","text":"","title":"Get compiler for CPU native instructions"},{"location":"deployment-configurations/cpu-dylib/#download-as-python-package","text":"Python packages for various IREE functionalities are regularly published on IREE's GitHub Releases page. Right now these are just snapshots of the main development branch. You can install the Python package containing the LLVM-based dylib compiler by python -m pip install iree-compiler-snapshot \\ -f https://github.com/google/iree/releases Tip iree-translate is installed as /path/to/python/site-packages/iree/tools/core/iree-translate . You can find out the full path to the site-packages directory via the python -m site command.","title":"Download as Python package"},{"location":"deployment-configurations/cpu-dylib/#build-compiler-from-source","text":"Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The dylib compiler backend is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add DYLIB-LLVM-AOT to the IREE_TARGET_BACKENDS_TO_BUILD CMake list variable when configuring (for host).","title":"Build compiler from source"},{"location":"deployment-configurations/cpu-dylib/#compile-and-run-the-model","text":"With the compiler and runtime for dynamic libraries, we can now compile a model and run it on the CPU.","title":"Compile and run the model"},{"location":"deployment-configurations/cpu-dylib/#compile-the-model","text":"IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by main IREE compilers first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then,","title":"Compile the model"},{"location":"deployment-configurations/cpu-dylib/#compile-using-the-command-line","text":"In the build directory, run the following command: iree/tools/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = dylib-llvm-aot \\ iree_input.mlir -o mobilenet-dylib.vmfb Todo Choose the suitable target triple for the current CPU where iree_input.mlir is the model's initial MLIR representation generated by IREE's TensorFlow importer.","title":"Compile using the command-line"},{"location":"deployment-configurations/cpu-dylib/#run-the-model","text":"","title":"Run the model"},{"location":"deployment-configurations/cpu-dylib/#run-using-the-command-line","text":"In the build directory, run the following command: iree/tools/iree-run-module \\ --driver = dylib \\ --module_file = mobilenet-dylib.vmfb \\ --entry_function = predict \\ --function_inputs = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values.","title":"Run using the command-line"},{"location":"deployment-configurations/gpu-vulkan/","text":"Vulkan GPU HAL Driver \u00b6 IREE can accelerate model execution on GPUs via Vulkan , a low-overhead graphics and compute API. Vulkan is cross-platform: it is available on many operating systems, including Android, Linux, and Windows. Vulkan is also cross-vendor: it is supported by most GPU vendors, including AMD, ARM, Intel, NVIDIA, and Qualcomm. Todo Add IREE's GPU support matrix: what GPUs are supported; what GPUs are well optimized; etc. Prerequisites \u00b6 In order to use Vulkan to drive the GPU, you need to have a functional Vulkan environment. IREE requires Vulkan 1.1 on Android and 1.2 elsewhere. It can be verified by the following steps: Android Android mandates Vulkan 1.1 support since Android 10. You just need to make sure the device's Android version is 10 or higher. Linux Run the following command in a shell: vulkaninfo | grep apiVersion If vulkaninfo does not exist, you will need to install the latest Vulkan SDK . For Ubuntu 18.04/20.04, installing via LunarG's package repository is recommended, as it places Vulkan libraries and tools under system paths so it's easy to discover. If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU. Windows Run the following command in a shell: vulkaninfo | grep apiVersion If vulkaninfo does not exist, you will need to install the latest Vulkan SDK . If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU. Get runtime and compiler \u00b6 Get IREE runtime with Vulkan HAL driver \u00b6 Next you will need to get an IREE runtime that supports the Vulkan HAL driver so it can execute the model on GPU via Vulkan. Build runtime from source \u00b6 Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The Vulkan HAL driver is compiled in by default on non-Apple platforms. If you want to explicitly specify HAL drivers to support, you will need to add Vulkan to the IREE_HAL_DRIVERS_TO_BUILD CMake list variable when configuring (for target). Get compiler for SPIR-V exchange format \u00b6 Vulkan expects the program running on GPU to be expressed by the SPIR-V binary exchange format, which the model must be compiled into. Download as Python package \u00b6 Python packages for various IREE functionalities are regularly published on IREE's GitHub Releases page. Right now these are just snapshots of the main development branch. You can install the Python package containing the SPIR-V compiler by python -m pip install iree-compiler-snapshot \\ -f https://github.com/google/iree/releases Tip iree-translate is installed as /path/to/python/site-packages/iree/tools/core/iree-translate . You can find out the full path to the site-packages directory via the python -m site command. Build compiler from source \u00b6 Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The SPIR-V compiler backend is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add Vulkan-SPIRV to the IREE_TARGET_BACKENDS_TO_BUILD CMake list variable when configuring (for host). Compile and run the model \u00b6 With the compiler for SPIR-V and runtime for Vulkan, we can now compile a model and run it on the GPU. Compile the model \u00b6 IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by main IREE compilers first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then, Compile using the command-line \u00b6 In the build directory, run the following command: iree/tools/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = vulkan-spirv \\ iree_input.mlir -o mobilenet-vulkan.vmfb Todo Choose the suitable target triple for the current GPU where iree_input.mlir is the model's initial MLIR representation generated by IREE's TensorFlow importer. Run the model \u00b6 Run using the command-line \u00b6 In the build directory, run the following command: iree/tools/iree-run-module \\ --driver = vulkan \\ --module_file = mobilenet-vulkan.vmfb \\ --entry_function = predict \\ --function_inputs = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values.","title":"GPU - Vulkan"},{"location":"deployment-configurations/gpu-vulkan/#vulkan-gpu-hal-driver","text":"IREE can accelerate model execution on GPUs via Vulkan , a low-overhead graphics and compute API. Vulkan is cross-platform: it is available on many operating systems, including Android, Linux, and Windows. Vulkan is also cross-vendor: it is supported by most GPU vendors, including AMD, ARM, Intel, NVIDIA, and Qualcomm. Todo Add IREE's GPU support matrix: what GPUs are supported; what GPUs are well optimized; etc.","title":"Vulkan GPU HAL Driver"},{"location":"deployment-configurations/gpu-vulkan/#prerequisites","text":"In order to use Vulkan to drive the GPU, you need to have a functional Vulkan environment. IREE requires Vulkan 1.1 on Android and 1.2 elsewhere. It can be verified by the following steps: Android Android mandates Vulkan 1.1 support since Android 10. You just need to make sure the device's Android version is 10 or higher. Linux Run the following command in a shell: vulkaninfo | grep apiVersion If vulkaninfo does not exist, you will need to install the latest Vulkan SDK . For Ubuntu 18.04/20.04, installing via LunarG's package repository is recommended, as it places Vulkan libraries and tools under system paths so it's easy to discover. If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU. Windows Run the following command in a shell: vulkaninfo | grep apiVersion If vulkaninfo does not exist, you will need to install the latest Vulkan SDK . If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU.","title":"Prerequisites"},{"location":"deployment-configurations/gpu-vulkan/#get-runtime-and-compiler","text":"","title":"Get runtime and compiler"},{"location":"deployment-configurations/gpu-vulkan/#get-iree-runtime-with-vulkan-hal-driver","text":"Next you will need to get an IREE runtime that supports the Vulkan HAL driver so it can execute the model on GPU via Vulkan.","title":"Get IREE runtime with Vulkan HAL driver"},{"location":"deployment-configurations/gpu-vulkan/#build-runtime-from-source","text":"Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The Vulkan HAL driver is compiled in by default on non-Apple platforms. If you want to explicitly specify HAL drivers to support, you will need to add Vulkan to the IREE_HAL_DRIVERS_TO_BUILD CMake list variable when configuring (for target).","title":"Build runtime from source"},{"location":"deployment-configurations/gpu-vulkan/#get-compiler-for-spir-v-exchange-format","text":"Vulkan expects the program running on GPU to be expressed by the SPIR-V binary exchange format, which the model must be compiled into.","title":"Get compiler for SPIR-V exchange format"},{"location":"deployment-configurations/gpu-vulkan/#download-as-python-package","text":"Python packages for various IREE functionalities are regularly published on IREE's GitHub Releases page. Right now these are just snapshots of the main development branch. You can install the Python package containing the SPIR-V compiler by python -m pip install iree-compiler-snapshot \\ -f https://github.com/google/iree/releases Tip iree-translate is installed as /path/to/python/site-packages/iree/tools/core/iree-translate . You can find out the full path to the site-packages directory via the python -m site command.","title":"Download as Python package"},{"location":"deployment-configurations/gpu-vulkan/#build-compiler-from-source","text":"Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The SPIR-V compiler backend is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add Vulkan-SPIRV to the IREE_TARGET_BACKENDS_TO_BUILD CMake list variable when configuring (for host).","title":"Build compiler from source"},{"location":"deployment-configurations/gpu-vulkan/#compile-and-run-the-model","text":"With the compiler for SPIR-V and runtime for Vulkan, we can now compile a model and run it on the GPU.","title":"Compile and run the model"},{"location":"deployment-configurations/gpu-vulkan/#compile-the-model","text":"IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by main IREE compilers first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then,","title":"Compile the model"},{"location":"deployment-configurations/gpu-vulkan/#compile-using-the-command-line","text":"In the build directory, run the following command: iree/tools/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = vulkan-spirv \\ iree_input.mlir -o mobilenet-vulkan.vmfb Todo Choose the suitable target triple for the current GPU where iree_input.mlir is the model's initial MLIR representation generated by IREE's TensorFlow importer.","title":"Compile using the command-line"},{"location":"deployment-configurations/gpu-vulkan/#run-the-model","text":"","title":"Run the model"},{"location":"deployment-configurations/gpu-vulkan/#run-using-the-command-line","text":"In the build directory, run the following command: iree/tools/iree-run-module \\ --driver = vulkan \\ --module_file = mobilenet-vulkan.vmfb \\ --entry_function = predict \\ --function_inputs = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values.","title":"Run using the command-line"},{"location":"ml-frameworks/jax/","text":"JAX Integration \u00b6 Todo Issue#5454 : write this documentation","title":"JAX"},{"location":"ml-frameworks/jax/#jax-integration","text":"Todo Issue#5454 : write this documentation","title":"JAX Integration"},{"location":"ml-frameworks/tensorflow-lite/","text":"TensorFlow Lite Integration \u00b6 Todo Issue#5455 : write this documentation","title":"TensorFlow Lite"},{"location":"ml-frameworks/tensorflow-lite/#tensorflow-lite-integration","text":"Todo Issue#5455 : write this documentation","title":"TensorFlow Lite Integration"},{"location":"ml-frameworks/tensorflow/","text":"TensorFlow Integration \u00b6 IREE supports compiling and running TensorFlow programs represented as tf.Module classes or stored in the SavedModel format . Prerequisites \u00b6 Install TensorFlow by following the official documentation : python -m pip install tf-nightly Install IREE pip packages, either from pip or by building from source : python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ iree-tools-tf-snapshot \\ -f https://github.com/google/iree/releases Importing models \u00b6 IREE compilers transform a model into its final deployable format in several sequential steps. The first step for a TensorFlow model is to use either the iree-tf-import command-line tool or IREE's Python APIs to import the model into a format (i.e., MLIR ) compatible with the generic IREE compilers. From SavedModel on TensorFlow Hub \u00b6 IREE supports importing and using SavedModels from TensorFlow Hub . Using the command-line tool \u00b6 First download the SavedModel and load it to get the serving signature, which is used as the entry point for IREE compilation flow: import tensorflow.compat.v2 as tf loaded_model = tf . saved_model . load ( '/path/to/downloaded/model/' ) print ( list ( imported_with_signatures . signatures . keys ())) Note If there are no serving signatures in the original SavedModel, you may add them by yourself by following \"Missing serving signature in SavedModel\" . Then you can import the model with iree-tf-import . You can read the options supported via iree-tf-import -help . Using MobileNet v2 as an example and assuming the serving signature is predict : iree-tf-import -tf-import-type = savedmodel_v1 \\ -tf-savedmodel-exported-names = predict \\ /path/to/savedmodel -o iree_input.mlir Tip iree-tf-import is installed as /path/to/python/site-packages/iree/tools/tf/iree-tf-import . You can find out the full path to the site-packages directory via the python -m site command. -tf-import-type needs to match the SavedModel version. You can try both v1 and v2 if you see one of them gives an empty dump. Afterwards you can further compile the model in iree_input.mlir for CPU or GPU . Training \u00b6 Samples \u00b6 Colab notebooks Training an MNIST digits classifier Edge detection module Pretrained ResNet50 inference End-to-end execution tests can be found in IREE's integrations/tensorflow/e2e/ directory. Troubleshooting \u00b6 Missing serving signature in SavedModel \u00b6 Sometimes SavedModels are exported without explicit serving signatures . This happens by default for TensorFlow Hub SavedModels. However, serving signatures are required as entry points for IREE compilation flow. You can use Python to load and re-export the SavedModel to give it serving signatures. For example, for MobileNet v2 , assuming we want the serving signature to be predict and operating on a 224x224 RGB image: import tensorflow.compat.v2 as tf loaded_model = tf . saved_model . load ( '/path/to/downloaded/model/' ) call = loaded_model . __call__ . get_concrete_function ( tf . TensorSpec ([ 1 , 224 , 224 , 3 ], tf . float32 )) signatures = { 'predict' : call } tf . saved_model . save ( loaded_model , '/path/to/resaved/model/' , signatures = signatures ) The above will create a new SavedModel with a serving signature, predict , and save it to /path/to/resaved/model/ .","title":"TensorFlow"},{"location":"ml-frameworks/tensorflow/#tensorflow-integration","text":"IREE supports compiling and running TensorFlow programs represented as tf.Module classes or stored in the SavedModel format .","title":"TensorFlow Integration"},{"location":"ml-frameworks/tensorflow/#prerequisites","text":"Install TensorFlow by following the official documentation : python -m pip install tf-nightly Install IREE pip packages, either from pip or by building from source : python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ iree-tools-tf-snapshot \\ -f https://github.com/google/iree/releases","title":"Prerequisites"},{"location":"ml-frameworks/tensorflow/#importing-models","text":"IREE compilers transform a model into its final deployable format in several sequential steps. The first step for a TensorFlow model is to use either the iree-tf-import command-line tool or IREE's Python APIs to import the model into a format (i.e., MLIR ) compatible with the generic IREE compilers.","title":"Importing models"},{"location":"ml-frameworks/tensorflow/#from-savedmodel-on-tensorflow-hub","text":"IREE supports importing and using SavedModels from TensorFlow Hub .","title":"From SavedModel on TensorFlow Hub"},{"location":"ml-frameworks/tensorflow/#using-the-command-line-tool","text":"First download the SavedModel and load it to get the serving signature, which is used as the entry point for IREE compilation flow: import tensorflow.compat.v2 as tf loaded_model = tf . saved_model . load ( '/path/to/downloaded/model/' ) print ( list ( imported_with_signatures . signatures . keys ())) Note If there are no serving signatures in the original SavedModel, you may add them by yourself by following \"Missing serving signature in SavedModel\" . Then you can import the model with iree-tf-import . You can read the options supported via iree-tf-import -help . Using MobileNet v2 as an example and assuming the serving signature is predict : iree-tf-import -tf-import-type = savedmodel_v1 \\ -tf-savedmodel-exported-names = predict \\ /path/to/savedmodel -o iree_input.mlir Tip iree-tf-import is installed as /path/to/python/site-packages/iree/tools/tf/iree-tf-import . You can find out the full path to the site-packages directory via the python -m site command. -tf-import-type needs to match the SavedModel version. You can try both v1 and v2 if you see one of them gives an empty dump. Afterwards you can further compile the model in iree_input.mlir for CPU or GPU .","title":"Using the command-line tool"},{"location":"ml-frameworks/tensorflow/#training","text":"","title":"Training"},{"location":"ml-frameworks/tensorflow/#samples","text":"Colab notebooks Training an MNIST digits classifier Edge detection module Pretrained ResNet50 inference End-to-end execution tests can be found in IREE's integrations/tensorflow/e2e/ directory.","title":"Samples"},{"location":"ml-frameworks/tensorflow/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"ml-frameworks/tensorflow/#missing-serving-signature-in-savedmodel","text":"Sometimes SavedModels are exported without explicit serving signatures . This happens by default for TensorFlow Hub SavedModels. However, serving signatures are required as entry points for IREE compilation flow. You can use Python to load and re-export the SavedModel to give it serving signatures. For example, for MobileNet v2 , assuming we want the serving signature to be predict and operating on a 224x224 RGB image: import tensorflow.compat.v2 as tf loaded_model = tf . saved_model . load ( '/path/to/downloaded/model/' ) call = loaded_model . __call__ . get_concrete_function ( tf . TensorSpec ([ 1 , 224 , 224 , 3 ], tf . float32 )) signatures = { 'predict' : call } tf . saved_model . save ( loaded_model , '/path/to/resaved/model/' , signatures = signatures ) The above will create a new SavedModel with a serving signature, predict , and save it to /path/to/resaved/model/ .","title":"Missing serving signature in SavedModel"}]}