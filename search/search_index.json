{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IREE \u00b6 IREE ( I ntermediate R epresentation E xecution E nvironment 1 ) is an MLIR -based end-to-end compiler and runtime that lowers Machine Learning (ML) models to a unified IR that scales up to meet the needs of the datacenter and down to satisfy the constraints and special considerations of mobile and edge deployments. Key features \u00b6 Ahead-of-time compilation of scheduling and execution logic together Support for dynamic shapes, flow control, streaming, and other advanced model features Optimized for many CPU and GPU architectures Low overhead, pipelined execution for efficient power and resource usage Binary size as low as 30KB on embedded systems Debugging and profiling support Support matrix \u00b6 IREE supports importing from a variety of ML frameworks: TensorFlow TensorFlow Lite JAX PyTorch (planned) ONNX (hoped for) The IREE compiler tools run on Linux, Windows, and macOS and can generate efficient code for a variety of runtime platforms: Linux Windows Android macOS (planned) iOS (planned) Bare metal WebAssembly (planned) and architectures: ARM x86 RISC-V Support for hardware accelerators and APIs is also included: Vulkan CUDA Metal (planned) WebGPU (planned) Project architecture \u00b6 IREE adopts a holistic approach towards ML model compilation: the IR produced contains both the scheduling logic, required to communicate data dependencies to low-level parallel pipelined hardware/API like Vulkan , and the execution logic, encoding dense computation on the hardware in the form of hardware/API-specific binaries like SPIR-V . Workflow overview \u00b6 Specific examples outlining IREE's workflow can be found in the User Getting Started Guide . Using IREE involves the following general steps: Import your model Develop your program using one of the supported frameworks , then run your model using one of IREE's import tools. Select your deployment configuration Identify your target platform, accelerator(s), and other constraints. Compile your model Compile through IREE, picking compilation targets based on your deployment configuration. Run your model Use IREE's runtime components to execute your compiled model. Importing models from ML frameworks \u00b6 IREE supports importing models from a growing list of ML frameworks and model formats: TensorFlow TensorFlow Lite JAX Selecting deployment configurations \u00b6 IREE provides a flexible set of tools for various deployment scenarios. Fully featured environments can use IREE for dynamic model deployments taking advantage of multi-threaded hardware, while embedded systems can bypass IREE's runtime entirely or interface with custom accelerators. What platforms are you targeting? Desktop? Mobile? An embedded system? What hardware should the bulk of your model run on? CPU? GPU? How fixed is your model itself? Can the weights be changed? Do you want to support loading different model architectures dynamically? IREE supports the full set of these configurations using the same underlying technology. Compiling models \u00b6 Model compilation is performed ahead-of-time on a host machine for any combination of targets . The compilation process converts from layers and operators used by high level frameworks down into optimized native code and associated scheduling logic. For example, compiling for GPU execution using Vulkan generates SPIR-V kernels and Vulkan API calls. For CPU execution , native code with static or dynamic linkage and the associated function calls are generated. Running models \u00b6 IREE offers a low level C API, as well as several specialized sets of bindings for running IREE models using other languages: C API Python TensorFlow Lite Communication channels \u00b6 GitHub issues : Feature requests, bugs, and other work tracking IREE Discord server : Daily development discussions with the core team and collaborators iree-discuss email list : Announcements, general and low-priority discussion Roadmap \u00b6 IREE is in the early stages of development and is not yet ready for broad adoption. Check out the long-term design roadmap to get a sense of where we're headed. We plan on a quarterly basis using OKRs . Review our latest objectives to see what we're up to. We use GitHub Projects to track progress on IREE components and specific efforts and GitHub Milestones to track the work associated with plans for each quarter. Pronounced \"eerie\" and often styled with the emoji \u21a9","title":"Home"},{"location":"#iree","text":"IREE ( I ntermediate R epresentation E xecution E nvironment 1 ) is an MLIR -based end-to-end compiler and runtime that lowers Machine Learning (ML) models to a unified IR that scales up to meet the needs of the datacenter and down to satisfy the constraints and special considerations of mobile and edge deployments.","title":"IREE"},{"location":"#key-features","text":"Ahead-of-time compilation of scheduling and execution logic together Support for dynamic shapes, flow control, streaming, and other advanced model features Optimized for many CPU and GPU architectures Low overhead, pipelined execution for efficient power and resource usage Binary size as low as 30KB on embedded systems Debugging and profiling support","title":"Key features"},{"location":"#support-matrix","text":"IREE supports importing from a variety of ML frameworks: TensorFlow TensorFlow Lite JAX PyTorch (planned) ONNX (hoped for) The IREE compiler tools run on Linux, Windows, and macOS and can generate efficient code for a variety of runtime platforms: Linux Windows Android macOS (planned) iOS (planned) Bare metal WebAssembly (planned) and architectures: ARM x86 RISC-V Support for hardware accelerators and APIs is also included: Vulkan CUDA Metal (planned) WebGPU (planned)","title":"Support matrix"},{"location":"#project-architecture","text":"IREE adopts a holistic approach towards ML model compilation: the IR produced contains both the scheduling logic, required to communicate data dependencies to low-level parallel pipelined hardware/API like Vulkan , and the execution logic, encoding dense computation on the hardware in the form of hardware/API-specific binaries like SPIR-V .","title":"Project architecture"},{"location":"#workflow-overview","text":"Specific examples outlining IREE's workflow can be found in the User Getting Started Guide . Using IREE involves the following general steps: Import your model Develop your program using one of the supported frameworks , then run your model using one of IREE's import tools. Select your deployment configuration Identify your target platform, accelerator(s), and other constraints. Compile your model Compile through IREE, picking compilation targets based on your deployment configuration. Run your model Use IREE's runtime components to execute your compiled model.","title":"Workflow overview"},{"location":"#importing-models-from-ml-frameworks","text":"IREE supports importing models from a growing list of ML frameworks and model formats: TensorFlow TensorFlow Lite JAX","title":"Importing models from ML frameworks"},{"location":"#selecting-deployment-configurations","text":"IREE provides a flexible set of tools for various deployment scenarios. Fully featured environments can use IREE for dynamic model deployments taking advantage of multi-threaded hardware, while embedded systems can bypass IREE's runtime entirely or interface with custom accelerators. What platforms are you targeting? Desktop? Mobile? An embedded system? What hardware should the bulk of your model run on? CPU? GPU? How fixed is your model itself? Can the weights be changed? Do you want to support loading different model architectures dynamically? IREE supports the full set of these configurations using the same underlying technology.","title":"Selecting deployment configurations"},{"location":"#compiling-models","text":"Model compilation is performed ahead-of-time on a host machine for any combination of targets . The compilation process converts from layers and operators used by high level frameworks down into optimized native code and associated scheduling logic. For example, compiling for GPU execution using Vulkan generates SPIR-V kernels and Vulkan API calls. For CPU execution , native code with static or dynamic linkage and the associated function calls are generated.","title":"Compiling models"},{"location":"#running-models","text":"IREE offers a low level C API, as well as several specialized sets of bindings for running IREE models using other languages: C API Python TensorFlow Lite","title":"Running models"},{"location":"#communication-channels","text":"GitHub issues : Feature requests, bugs, and other work tracking IREE Discord server : Daily development discussions with the core team and collaborators iree-discuss email list : Announcements, general and low-priority discussion","title":"Communication channels"},{"location":"#roadmap","text":"IREE is in the early stages of development and is not yet ready for broad adoption. Check out the long-term design roadmap to get a sense of where we're headed. We plan on a quarterly basis using OKRs . Review our latest objectives to see what we're up to. We use GitHub Projects to track progress on IREE components and specific efforts and GitHub Milestones to track the work associated with plans for each quarter. Pronounced \"eerie\" and often styled with the emoji \u21a9","title":"Roadmap"},{"location":"bindings/","text":"Bindings \u00b6 IREE offers specialized sets of bindings for running compiled programs from various languages or with specific APIs: Runtime C API Compiler and runtime Python bindings Runtime TensorFlow Lite bindings","title":"Bindings"},{"location":"bindings/#bindings","text":"IREE offers specialized sets of bindings for running compiled programs from various languages or with specific APIs: Runtime C API Compiler and runtime Python bindings Runtime TensorFlow Lite bindings","title":"Bindings"},{"location":"bindings/c-api/","text":"C API bindings \u00b6 IREE provides a low level C API for its runtime 1 , which can be used directly or through higher level APIs and language bindings built on top of it. API header files are organized by runtime component: Component header file Overview iree/base/api.h Core API, type definitions, ownership policies, utilities iree/vm/api.h VM APIs: loading modules, I/O, calling functions iree/hal/api.h HAL APIs: device management, synchronization, accessing hardware features The samples/ directory demonstrates several ways to use IREE's C API. Prerequisites \u00b6 To use IREE's C API, you will need to build the runtime from source . The iree-template-cpp community project also shows how to integrate IREE into an external project using CMake 2 . Concepts \u00b6 By default, IREE uses its own tiny Virtual Machine (VM) at runtime to interpret program instructions on the host system. VM instructions may also be lowered further to LLVM IR, C, or other representations for static or resource constrained deployment. The VM supports generic operations like loads, stores, arithmetic, function calls, and control flow. It builds streams of more complex program logic and dense math into command buffers that are dispatched to hardware backends through the Hardware Abstraction Layer (HAL) interface. Most interaction with IREE's C API involves either the VM or the HAL. IREE VM \u00b6 VM instances can serve multiple isolated execution contexts VM contexts are effectively sandboxes for loading modules and running programs VM modules provide extra functionality to execution contexts , such as access to hardware accelerators through the HAL. Compiled user programs are also modules. IREE HAL \u00b6 HAL drivers are used to enumerate and create HAL devices HAL devices interface with hardware, such as by allocating device memory, preparing executables, recording and dispatching command buffers, and synchronizing with the host HAL buffers and buffer views represent storage and shaped/typed views into that storage (aka \"tensors\") Using the C API \u00b6 Setup \u00b6 Include headers: #include \"iree/base/api.h\" #include \"iree/hal/api.h\" #include \"iree/vm/api.h\" // The VM bytecode and HAL modules will typically be included, along // with those for the specific HAL drivers your application uses. // Functionality extensions can be used via custom modules. #include \"iree/modules/hal/module.h\" #include \"iree/hal/drivers/local_task/registration/driver_module.h\" #include \"iree/vm/bytecode_module.h\" Check the API version and register components: // Statically linking the runtime should never have version conflicts, // however dynamically linking against a runtime shared object must // always verify that the version is as expected. iree_api_version_t actual_version ; IREE_CHECK_OK ( iree_api_version_check ( IREE_API_VERSION_LATEST , & actual_version )); // Device drivers are managed through registries. // Applications may use multiple registries to more finely control driver // lifetimes and visibility. IREE_CHECK_OK ( iree_hal_local_task_driver_module_register ( iree_hal_driver_registry_default ())); Tip The IREE_CHECK_OK() macro calls assert() if an error occurs. Applications should propagate errors and handle or report them as desired. Configure stateful objects \u00b6 Create a VM instance along with a HAL driver and device: // Applications should try to reuse instances so resource usage across contexts // is handled and extraneous device interaction is avoided. iree_vm_instance_t * instance = NULL ; IREE_CHECK_OK ( iree_vm_instance_create ( iree_allocator_system (), & instance )); // Modules with custom types must be statically registered before use. IREE_CHECK_OK ( iree_hal_module_register_all_types ( instance )); // We use the CPU \"local-task\" driver in this example, but could use a different // driver like the GPU \"vulkan\" driver. The driver(s) used should match with // the target(s) specified during compilation. iree_hal_driver_t * driver = NULL ; IREE_CHECK_OK ( iree_hal_driver_registry_try_create ( iree_hal_driver_registry_default (), iree_string_view_literal ( \"local-task\" ), iree_allocator_system (), & driver )); // Drivers may support multiple devices, such as when a machine has multiple // GPUs. You may either enumerate devices and select based on their properties, // or just use the default device. iree_hal_device_t * device = NULL ; IREE_CHECK_OK ( iree_hal_driver_create_default_device ( driver , iree_allocator_system (), & device )); // Create a HAL module initialized to use the newly created device. // We'll load this module into a VM context later. iree_vm_module_t * hal_module = NULL ; IREE_CHECK_OK ( iree_hal_module_create ( instance , device , IREE_HAL_MODULE_FLAG_NONE , iree_allocator_system (), & hal_module )); // The reference to the driver can be released now. iree_hal_driver_release ( driver ); Tip The default iree_allocator_system() is backed by malloc and free , but custom allocators may also be used. Load a vmfb bytecode module containing program data: // (Application-specific loading into memory, such as loading from a file) iree_vm_module_t * bytecode_module = NULL ; IREE_CHECK_OK ( iree_vm_bytecode_module_create ( instance , iree_const_byte_span_t { module_data , module_size }, /*flatbuffer_allocator=*/ iree_allocator_null (), /*allocator=*/ iree_allocator_system (), & bytecode_module )); Note Many IREE samples use c_embed_data to embed vmfb files as C code to avoid file I/O and ease portability. Applications should use what makes sense for their platforms and deployment configurations. Create a VM context and load modules into it: iree_vm_context_t * context = NULL ; iree_vm_module_t * modules [ 2 ] = { hal_module , bytecode_module }; IREE_CHECK_OK ( iree_vm_context_create_with_modules ( instance , IREE_VM_CONTEXT_FLAG_NONE , IREE_ARRAYSIZE ( modules ), modules , iree_allocator_system (), & context )); // References to the modules can be released now. iree_vm_module_release ( hal_module ); iree_vm_module_release ( bytecode_module ); Look up the function(s) to call: iree_vm_function_t main_function ; IREE_CHECK_OK ( iree_vm_context_resolve_function ( context , iree_string_view_literal ( \"module.main_function\" ), & main_function )); Invoke functions \u00b6 // (Application-specific I/O buffer setup, making data available to the device) IREE_CHECK_OK ( iree_vm_invoke ( context , main_function , /*policy=*/ NULL , inputs , outputs , iree_allocator_system ())); // (Application-specific output buffer retrieval and reading back from the device) Cleanup resources \u00b6 iree_hal_device_release ( device ); iree_vm_context_release ( context ); iree_vm_instance_release ( instance ); We are exploring adding a C API for IREE's compiler, see this GitHub issue \u21a9 We plan on deploying via vcpkg in the future too, see this GitHub project \u21a9","title":"C API"},{"location":"bindings/c-api/#c-api-bindings","text":"IREE provides a low level C API for its runtime 1 , which can be used directly or through higher level APIs and language bindings built on top of it. API header files are organized by runtime component: Component header file Overview iree/base/api.h Core API, type definitions, ownership policies, utilities iree/vm/api.h VM APIs: loading modules, I/O, calling functions iree/hal/api.h HAL APIs: device management, synchronization, accessing hardware features The samples/ directory demonstrates several ways to use IREE's C API.","title":"C API bindings"},{"location":"bindings/c-api/#prerequisites","text":"To use IREE's C API, you will need to build the runtime from source . The iree-template-cpp community project also shows how to integrate IREE into an external project using CMake 2 .","title":"Prerequisites"},{"location":"bindings/c-api/#concepts","text":"By default, IREE uses its own tiny Virtual Machine (VM) at runtime to interpret program instructions on the host system. VM instructions may also be lowered further to LLVM IR, C, or other representations for static or resource constrained deployment. The VM supports generic operations like loads, stores, arithmetic, function calls, and control flow. It builds streams of more complex program logic and dense math into command buffers that are dispatched to hardware backends through the Hardware Abstraction Layer (HAL) interface. Most interaction with IREE's C API involves either the VM or the HAL.","title":"Concepts"},{"location":"bindings/c-api/#iree-vm","text":"VM instances can serve multiple isolated execution contexts VM contexts are effectively sandboxes for loading modules and running programs VM modules provide extra functionality to execution contexts , such as access to hardware accelerators through the HAL. Compiled user programs are also modules.","title":"IREE VM"},{"location":"bindings/c-api/#iree-hal","text":"HAL drivers are used to enumerate and create HAL devices HAL devices interface with hardware, such as by allocating device memory, preparing executables, recording and dispatching command buffers, and synchronizing with the host HAL buffers and buffer views represent storage and shaped/typed views into that storage (aka \"tensors\")","title":"IREE HAL"},{"location":"bindings/c-api/#using-the-c-api","text":"","title":"Using the C API"},{"location":"bindings/c-api/#setup","text":"Include headers: #include \"iree/base/api.h\" #include \"iree/hal/api.h\" #include \"iree/vm/api.h\" // The VM bytecode and HAL modules will typically be included, along // with those for the specific HAL drivers your application uses. // Functionality extensions can be used via custom modules. #include \"iree/modules/hal/module.h\" #include \"iree/hal/drivers/local_task/registration/driver_module.h\" #include \"iree/vm/bytecode_module.h\" Check the API version and register components: // Statically linking the runtime should never have version conflicts, // however dynamically linking against a runtime shared object must // always verify that the version is as expected. iree_api_version_t actual_version ; IREE_CHECK_OK ( iree_api_version_check ( IREE_API_VERSION_LATEST , & actual_version )); // Device drivers are managed through registries. // Applications may use multiple registries to more finely control driver // lifetimes and visibility. IREE_CHECK_OK ( iree_hal_local_task_driver_module_register ( iree_hal_driver_registry_default ())); Tip The IREE_CHECK_OK() macro calls assert() if an error occurs. Applications should propagate errors and handle or report them as desired.","title":"Setup"},{"location":"bindings/c-api/#configure-stateful-objects","text":"Create a VM instance along with a HAL driver and device: // Applications should try to reuse instances so resource usage across contexts // is handled and extraneous device interaction is avoided. iree_vm_instance_t * instance = NULL ; IREE_CHECK_OK ( iree_vm_instance_create ( iree_allocator_system (), & instance )); // Modules with custom types must be statically registered before use. IREE_CHECK_OK ( iree_hal_module_register_all_types ( instance )); // We use the CPU \"local-task\" driver in this example, but could use a different // driver like the GPU \"vulkan\" driver. The driver(s) used should match with // the target(s) specified during compilation. iree_hal_driver_t * driver = NULL ; IREE_CHECK_OK ( iree_hal_driver_registry_try_create ( iree_hal_driver_registry_default (), iree_string_view_literal ( \"local-task\" ), iree_allocator_system (), & driver )); // Drivers may support multiple devices, such as when a machine has multiple // GPUs. You may either enumerate devices and select based on their properties, // or just use the default device. iree_hal_device_t * device = NULL ; IREE_CHECK_OK ( iree_hal_driver_create_default_device ( driver , iree_allocator_system (), & device )); // Create a HAL module initialized to use the newly created device. // We'll load this module into a VM context later. iree_vm_module_t * hal_module = NULL ; IREE_CHECK_OK ( iree_hal_module_create ( instance , device , IREE_HAL_MODULE_FLAG_NONE , iree_allocator_system (), & hal_module )); // The reference to the driver can be released now. iree_hal_driver_release ( driver ); Tip The default iree_allocator_system() is backed by malloc and free , but custom allocators may also be used. Load a vmfb bytecode module containing program data: // (Application-specific loading into memory, such as loading from a file) iree_vm_module_t * bytecode_module = NULL ; IREE_CHECK_OK ( iree_vm_bytecode_module_create ( instance , iree_const_byte_span_t { module_data , module_size }, /*flatbuffer_allocator=*/ iree_allocator_null (), /*allocator=*/ iree_allocator_system (), & bytecode_module )); Note Many IREE samples use c_embed_data to embed vmfb files as C code to avoid file I/O and ease portability. Applications should use what makes sense for their platforms and deployment configurations. Create a VM context and load modules into it: iree_vm_context_t * context = NULL ; iree_vm_module_t * modules [ 2 ] = { hal_module , bytecode_module }; IREE_CHECK_OK ( iree_vm_context_create_with_modules ( instance , IREE_VM_CONTEXT_FLAG_NONE , IREE_ARRAYSIZE ( modules ), modules , iree_allocator_system (), & context )); // References to the modules can be released now. iree_vm_module_release ( hal_module ); iree_vm_module_release ( bytecode_module ); Look up the function(s) to call: iree_vm_function_t main_function ; IREE_CHECK_OK ( iree_vm_context_resolve_function ( context , iree_string_view_literal ( \"module.main_function\" ), & main_function ));","title":"Configure stateful objects"},{"location":"bindings/c-api/#invoke-functions","text":"// (Application-specific I/O buffer setup, making data available to the device) IREE_CHECK_OK ( iree_vm_invoke ( context , main_function , /*policy=*/ NULL , inputs , outputs , iree_allocator_system ())); // (Application-specific output buffer retrieval and reading back from the device)","title":"Invoke functions"},{"location":"bindings/c-api/#cleanup-resources","text":"iree_hal_device_release ( device ); iree_vm_context_release ( context ); iree_vm_instance_release ( instance ); We are exploring adding a C API for IREE's compiler, see this GitHub issue \u21a9 We plan on deploying via vcpkg in the future too, see this GitHub project \u21a9","title":"Cleanup resources"},{"location":"bindings/python/","text":"Python bindings \u00b6 Info API reference pages for IREE's runtime and compiler Python APIs are hosted on readthedocs . Overview \u00b6 IREE offers Python bindings split into several packages, covering different components: PIP package name Description iree-compiler IREE's generic compiler tools and helpers iree-runtime IREE's runtime, including CPU and GPU backends iree-tools-tf Tools for importing from TensorFlow iree-tools-tflite Tools for importing from TensorFlow Lite iree-tools-xla Tools for importing from XLA iree-jax Tools for importing from JAX Collectively, these packages allow for importing from frontends, compiling towards various targets, and executing compiled code on IREE's backends. Warning The TensorFlow, TensorFlow Lite, and XLA packages are currently only available on Linux and macOS. They are not available on Windows yet (see this issue ). Prerequisites \u00b6 To use IREE's Python bindings, you will first need to install Python 3 and pip , as needed. Tip We recommend using virtual environments to manage python packages, such as through venv ( about , tutorial ): Linux and MacOS Windows python -m venv .venv source .venv/bin/activate python -m venv . venv . venv \\ Scripts \\ activate . bat When done, run deactivate . Next, install packages: python -m pip install --upgrade pip python -m pip install numpy absl-py Installing IREE packages \u00b6 Prebuilt packages \u00b6 Stable release packages are published to PyPI . Minimal All packages To install just the core IREE packages: python -m pip install \\ iree-compiler \\ iree-runtime To install IREE packages with tools for all frontends: python -m pip install \\ iree-compiler \\ iree-runtime \\ iree-tools-tf \\ iree-tools-tflite \\ iree-tools-xla Tip Nightly packages are also published on GitHub releases . To use these, run pip install with this extra option: --find-links https://github.com/iree-org/iree/releases Building from source \u00b6 See Building Python bindings page for instructions for building from source. Using the Python bindings \u00b6 API reference pages for IREE's runtime and compiler Python APIs are hosted on readthedocs . Check out the samples in IREE's samples/colab/ directory and the iree-samples repository for examples using the Python APIs.","title":"Python"},{"location":"bindings/python/#python-bindings","text":"Info API reference pages for IREE's runtime and compiler Python APIs are hosted on readthedocs .","title":"Python bindings"},{"location":"bindings/python/#overview","text":"IREE offers Python bindings split into several packages, covering different components: PIP package name Description iree-compiler IREE's generic compiler tools and helpers iree-runtime IREE's runtime, including CPU and GPU backends iree-tools-tf Tools for importing from TensorFlow iree-tools-tflite Tools for importing from TensorFlow Lite iree-tools-xla Tools for importing from XLA iree-jax Tools for importing from JAX Collectively, these packages allow for importing from frontends, compiling towards various targets, and executing compiled code on IREE's backends. Warning The TensorFlow, TensorFlow Lite, and XLA packages are currently only available on Linux and macOS. They are not available on Windows yet (see this issue ).","title":"Overview"},{"location":"bindings/python/#prerequisites","text":"To use IREE's Python bindings, you will first need to install Python 3 and pip , as needed. Tip We recommend using virtual environments to manage python packages, such as through venv ( about , tutorial ): Linux and MacOS Windows python -m venv .venv source .venv/bin/activate python -m venv . venv . venv \\ Scripts \\ activate . bat When done, run deactivate . Next, install packages: python -m pip install --upgrade pip python -m pip install numpy absl-py","title":"Prerequisites"},{"location":"bindings/python/#installing-iree-packages","text":"","title":"Installing IREE packages"},{"location":"bindings/python/#prebuilt-packages","text":"Stable release packages are published to PyPI . Minimal All packages To install just the core IREE packages: python -m pip install \\ iree-compiler \\ iree-runtime To install IREE packages with tools for all frontends: python -m pip install \\ iree-compiler \\ iree-runtime \\ iree-tools-tf \\ iree-tools-tflite \\ iree-tools-xla Tip Nightly packages are also published on GitHub releases . To use these, run pip install with this extra option: --find-links https://github.com/iree-org/iree/releases","title":"Prebuilt packages"},{"location":"bindings/python/#building-from-source","text":"See Building Python bindings page for instructions for building from source.","title":"Building from source"},{"location":"bindings/python/#using-the-python-bindings","text":"API reference pages for IREE's runtime and compiler Python APIs are hosted on readthedocs . Check out the samples in IREE's samples/colab/ directory and the iree-samples repository for examples using the Python APIs.","title":"Using the Python bindings"},{"location":"bindings/tensorflow-lite/","text":"TensorFlow Lite bindings \u00b6 Todo Issue#5462 : write this documentation","title":"TensorFlow Lite"},{"location":"bindings/tensorflow-lite/#tensorflow-lite-bindings","text":"Todo Issue#5462 : write this documentation","title":"TensorFlow Lite bindings"},{"location":"blog/","text":"Blog \u00b6 Latest posts from the IREE team \u00b6 2021-10-15: CUDA backend 2021-10-13: Work in progress on Matrix Multiplication on CPU 2021-07-19: TFLite Support via TOSA","title":"Blog"},{"location":"blog/#blog","text":"","title":"Blog"},{"location":"blog/#latest-posts-from-the-iree-team","text":"2021-10-15: CUDA backend 2021-10-13: Work in progress on Matrix Multiplication on CPU 2021-07-19: TFLite Support via TOSA","title":"Latest posts from the IREE team"},{"location":"blog/2021-07-19-tflite-tosa/","text":"Monday, July 19, 2021 By Rob Suderman and Jenni Kilduff TFLite Support via TOSA \u00b6 IREE can now execute TensorFlow Lite (TFLite) models through the use of TOSA , an open standard of common tensor operations, and a part of MLIR core. TOSA\u2019s high-level representation of tensor operations provides a common front-end for ingesting models from different frameworks. In this case we ingest a TFLite FlatBuffer and compile it to TOSA IR, which IREE takes as an input format to compile to its various backends. Using TFLite as a frontend for IREE provides an alternative ingestion method for already existing models that could benefit from IREE\u2019s design. This enables models already designed for on-device inference to have an alternative path for execution without requiring any additional porting, while benefiting from IREE\u2019s improvements in buffer management, work dispatch system, and compact binary format. With continued improvements to IREE/MLIR\u2019s compilation performance, more optimized versions can be compiled and distributed to target devices without an update to the clientside environment. Today, we have validated floating point support for a variety of models, including mobilenet (v1, v2, and v3) and mobilebert . More work is in progress to support fully quantized models, and TFLite\u2019s hybrid quantization, along with dynamic shape support. Examples \u00b6 TFLite with IREE is available in Python and Java. We have a colab notebook that shows how to use IREE\u2019s python bindings and TFLite compiler tools to compile a pre-trained TFLite model from a FlatBuffer and run using IREE. We also have an Android Java app that was forked from an existing TFLite demo app, swapping out the TFLite library for our own AAR. More information on IREE\u2019s TFLite frontend is available here .","title":"TFLite Support via TOSA"},{"location":"blog/2021-07-19-tflite-tosa/#tflite-support-via-tosa","text":"IREE can now execute TensorFlow Lite (TFLite) models through the use of TOSA , an open standard of common tensor operations, and a part of MLIR core. TOSA\u2019s high-level representation of tensor operations provides a common front-end for ingesting models from different frameworks. In this case we ingest a TFLite FlatBuffer and compile it to TOSA IR, which IREE takes as an input format to compile to its various backends. Using TFLite as a frontend for IREE provides an alternative ingestion method for already existing models that could benefit from IREE\u2019s design. This enables models already designed for on-device inference to have an alternative path for execution without requiring any additional porting, while benefiting from IREE\u2019s improvements in buffer management, work dispatch system, and compact binary format. With continued improvements to IREE/MLIR\u2019s compilation performance, more optimized versions can be compiled and distributed to target devices without an update to the clientside environment. Today, we have validated floating point support for a variety of models, including mobilenet (v1, v2, and v3) and mobilebert . More work is in progress to support fully quantized models, and TFLite\u2019s hybrid quantization, along with dynamic shape support.","title":"TFLite Support via TOSA"},{"location":"blog/2021-07-19-tflite-tosa/#examples","text":"TFLite with IREE is available in Python and Java. We have a colab notebook that shows how to use IREE\u2019s python bindings and TFLite compiler tools to compile a pre-trained TFLite model from a FlatBuffer and run using IREE. We also have an Android Java app that was forked from an existing TFLite demo app, swapping out the TFLite library for our own AAR. More information on IREE\u2019s TFLite frontend is available here .","title":"Examples"},{"location":"blog/2021-10-13-mmt4d/","text":"Wednesday, October 13, 2021 By Ahmed Taei, Benoit Jacob Work in progress on Matrix Multiplication on CPU \u00b6 Introduction \u00b6 Matrix multiplication (matmul) is an important operation in ML workloads that poses specific challenges to code generation. For example, matmul makes repeated accesses to the same data, which makes locality of reference a top concern. Moreover, modern CPUs instruction set architectures (ISAs) offer specialized SIMD instructions that the matmul implementation needs to use to achieve optimal performance, and these instructions expect data to be in a particular layout. This article is about an in-development MLIR operation, linalg.mmt4d , offering a compilation path for linalg.matmul that is designed from the ground up for these efficiency considerations. We are still in the early implementation phase of this linalg.mmt4d plan, but we feel confident that we know where we are going because what we are really doing here is importing into the compiler what we have learned working on optimized matrix multiplication libraries, particularly Ruy . We know what loop schedule and kernel we want the compiler to generate \u2014 essentially the same as we wrote in Ruy, give or take additional optimizations such as fusions and constant folding that become possible now that we are doing this within a compiler. This allows us to focus on how we get the compiler to generate that schedule and kernel with purely algebraic transformations that compose and enable further compiler optimizations. At the basis of this work is the extensible op system of the Linalg dialect in the MLIR compiler toolkit. In this case, a general purpose, mixed precision mmt4d op is defined via a high level description directly in the compiler and is then available to both users of the compiler (as a linalg.mmt4d op) or for direct emission via Python based IR construction (i.e. for direct integration into high level frameworks without rebuilding the compiler). The ability to define such new special forms cheaply, and without any systemic framework level cost, is part of the extensibility and composition story that we expect will become increasingly important in development and deployment scenarios in the future, and in this case, it let us spring board off of high quality code generation which was already well integrated and composed well with other features of the compiler. Existing Matrix Multplication Code Generation \u00b6 Let us start by discussing IREE\u2019s existing matmul code generation and highlight the issues that mmt4d aims to overcome. The existing approach operates in-place on the source matrices. When we discuss \"tiling\" in this paragraph, we refer exclusively to the traversal \u2014 how these source matrices are traversed by the matmul loop. There is no \"tiled layout\" here, which will be the key difference with mmt4d below. The destination matrix is tiled into workgroups (CPU threads) tiles, then each workgroup tile is tiled to fit some level of CPU cache, and finally each tile is further tiled to fit target architecture registers (e.g. 8x8). That multi-level tiling means that the code works like the following loop nest: def tiled_matmul ( A , B , C , tile_m , tile_n , tile_k , tile_m_v , tile_n_v , tile_k_v ): m = A . shape [ 0 ] k = A . shape [ 1 ] n = B . shape [ 1 ] for m1 in range ( 0 , m , tile_m ): for n1 in range ( 0 , n , tile_n ): for k1 in range ( 0 , k , tile_k ): # First level of tiling views... lhs_tile = A [ m1 : m1 + tile_m , k1 : k1 + tile_k ] rhs_tile = B [ k1 : k1 + tile_k , n1 : n1 + tile_n ] dst_tile = C [ m1 : m1 + tile_m , n1 : n1 + tile_n ] for mv in range ( 0 , tile_m , tile_m_v ): for nv in range ( 0 , tile_n , tile_n_v ): for kv in range ( 0 , tile_k , tile_k_v ): # Register tiling views... lhs_tile_v = lhs_tile [ mv : mv + tile_m_v , kv : kv + tile_k_v ] rhs_tile_v = rhs_tile [ kv : kv + tile_k_v , nv : nv + tile_n_v ] # kernel. dst_tile [ mv : mv + tile_m_v , nv : nv + tile_n_v ] += np . matmul ( lhs_tile_v , rhs_tile_v ) return C The two main problems with this approach are: Overhead to meet SIMD ISA layout requirements : In practice, the kernel needs to use specific SIMD instructions to perform the arithmetic. They expect small tiles of the matrices to be loaded in registers, in a specific layout. If the matrix data wasn't already stored in memory in such a tiled layout, then the kernel has to perform such a data rearrangement on the fly, incurring substantial overhead. For NxN matrix multiplication, the kernel performs O(N 3 ) work on O(N 2 ) data, so doing that rearrangement there means O(N 3 ) overhead where O(N 2 ) should have sufficed, as this could have been done as a pre-processing step on O(N 2 ) data. Inefficent memory traversal: For efficiency reasons, we always need tile_m_v>1 and tile_n_v>1 . That is because the higher these values, the fewer memory-load instructions are needed overall; and this is also dictated by the SIMD instructions that we want to use. But that means that the kernel is accessing simultaneously multiple rows or columns of the left-hand and right-hand side matrices. And in this existing approach, they are stored in linear layout, not in a tiled layout, so these accesses are not contiguous in memory. This is detrimental to memory access performance, meaning the CPU caches , in multiple ways. One is that these multiple non-contiguous accesses may alias each other in the L1 cache because of low associativity . Matrix Multiplication Operation With 4D Tiled Operands \u00b6 For the reasons above, an efficient matmul implementation must reorder data into a tiled layout matching the target SIMD ISA and making the memory access patterns as contiguous as possible. IREE/MLIR defaults to bufferizing all tensors into a \"row-major\" order, meaning that the last-enumerated dimension is the one that is contiguous in memory. As we prefer not to write custom bufferization code, we can't specify an alternative layout for a tensor. Fortunately, it is possible to represent a 2D tiled layout as a 4D layout. For example, tensor<2x2x2x2xf32> can represent a 4x4 matrix made of 2x2 tiles, each of which is 2x2. The row-major layout on tensor<2x2x2x2xf32> makes each 2x2 tile contiguous and row-major, and arranges the 2x2 tiles themselves into a row-major 2x2 layout in the overall 4x4 matrix. Such a row-major-tiled layout is exactly what we need for the left-hand side of a matrix multiplication, because matrix multiplication traverses the left-hand side matrix row by row. But for the right-hand side matrix, we want a column-major-tiled layout. To solve this problem, we decide to implement not matrix multiplication, but matrix-multiplication-by-transposed-right-hand-side which is where the t in the linalg.mmt4d came from. Now such an op is happy with both the left and right-hand sides being row-major-tiled. The following example illustrates that. In these diagrams, each matrix element is rendered its memory offset. To compute the 2x2 block in the destination matrix, we will have to load two yellow blocks from LHS, RHS matrices respectively compute their matmul results (i.e. call the kernel), then the two blue blocks, and so on. As we can see, each tile loads data that is not contiguous. It would be better if we rearranged the elements in the following layout: Now tiles are stored contiguously in memory and the kernel can simply load them from memory into the registers that will be directly consumed by the SIMD instructions performing the multiplications. Moreover, the kernel is now loading from just two contiguous data streams, a simple memory access pattern which is sure to be efficient (regarding caches, etc) on any reasonable target hardware. We introduce a linalg.mmt4d operation that performs such a matrix multiplication on matrices in a tiled layout represented as 4D tensors. That leaves the question of how to represent, within the linalg dialect, the conversions between ordinary matrices represented as 2D tensors, and these tiled matrices represented as 4D tensors. Moreover, these conversions should be tileable and decompose well. Thankfully, the transformation from 2D to 4D can be written as a reshape followed by a transpose as in the following digram: So we can think of the outermost two dimensions of the 4D representations as the tile position in the overall matrix, and the innermost two as the element position within one tile. Hopefully the following Python pseudocode makes it more concrete: def pack_2d_4d ( operand , parallel_size , reduction_size ): i1 = operand . shape [ 0 ] // parallel_size # M1 i2 = parallel_size # M0 j1 = operand . shape [ 1 ] // reduction_size # K1 j2 = reduction_size # K0 operand_4d = np . reshape ( operand , [ i1 , i2 , j1 , j2 ]) return np . transpose ( operand_4d , [ 0 , 2 , 1 , 3 ]) # [M1, K1, M0, K0] Now the mmt4d operation will follow a structure as the multi level tiling, for simplicity we considered the case here where no L1 tiling is required only first level of distribution to workgroups: def mmt4d ( A , B , C , M0 , N0 , K0 ): M = A . shape [ 0 ] N = B . shape [ 1 ] Bt = np . transpose ( B , [ 1 , 0 ]) A4d = pack_2d_4d ( A , M0 , K0 ) Bt4d = pack_2d_4d ( Bt , N0 , K0 ) M1 = A4d . shape [ 0 ] N1 = Bt4d . shape [ 0 ] K1 = A4d . shape [ 1 ] for m1 in range ( 0 , M1 ): for n1 in range ( 0 , N1 ): for k1 in range ( 0 , K1 ): # Tile views that are contiguous in memory. lhs_tile = np . reshape ( A4d [ m1 , k1 , :, :], [ M0 , K0 ]) rhs_tile = np . reshape ( Bt4d [ n1 , k1 , :, :], [ N0 , K0 ]) # Inner kernel. C [ m1 , n1 , :, :] += np . matmul ( lhs_tile , np . transpose ( rhs_tile , [ 1 , 0 ])) # 4d -> 2D C2d = unpack_4d_2d ( C ) return C2d The resulting 4D tiled matrix still needs be rearranged back to the original layout as 2D tensor: def unpack_4d_2d ( operand ): i1 = operand . shape [ 0 ] # M1 j1 = operand . shape [ 1 ] # N1 i2 = operand . shape [ 2 ] # M0 j2 = operand . shape [ 3 ] # N0 operand_transposed = operand . transpose ([ 0 , 2 , 1 , 3 ]) # [M1, M0, N1, N0] return operand_transposed . reshape ([ i1 * i2 , j1 * j2 ]) # [M, N] Performance Results \u00b6 We benchmarked various float32 matmul problems of different sizes and the result showed that mmt4d is faster than the existing matmul implementation for bigger matrices as we can see the in the following chart: The SIMD instruction being used here is the simplest kind, a vector*scalar multiplication, and the storage orders of the matrices allow the existing implementation to directly load the vectors from the source matrices without any rearrangement overhead. So this case is particularly friendly to the existing code, which is why the mmt4d code is only faster for bigger matrices. To understand why mmt4d is faster in that case, we collected statistics of L1 cache misses: This shows that in this case, the better cache-friendliness of mmt4d, thanks to its simple contiguous memory access pattern, accounts for its higher performance. As we proceed with increasingly sophisticated SIMD targets, starting with the dot-product instructions found in current mobile devices for the int8 case and going to become generalized to all data types all the way to float32 over the next few years with upcoming ARM SIMD instructions, the advantage of mmt4d will widen for all sizes, not just the larger ones. Part of why we feel confident about the eventual performance that our approach will achieve is that, as mentioned in the introduction, we are rebuilding within the compiler an existing library 's schedule and kernel, and we have benchmark results about it. Conclusion \u00b6 We introduced a 4d tiled representation for 2d matrix-matrix multiplication with a decomposable algebric transformations that requires only reshape and transpose of input operands, we discussed and empirically showed how that solves major drawbacks in row-major linear matmul by providing a flexible way to match different ISA layout along with better cache locality achieving near peak performance. As was mentioned in the introduction, this work in under active development and the next immediate steps are to prove the rest of the hypothesis by: Handling dynamic sizes and padding to the next multiple of the target tile size. Implementing the integer case ( int32 += int8 * int8 ). Implementing the dispatch to different SIMD ISA variants at runtime. Implementing cache-friendly traversal for larger matmuls and multi-threading by interfacing with IREE's runtime dispatch. Improving the generated code by fusing the 4d tiled layout with the producers and consumers of the linalg.mmt4d .","title":"Work in progress on Matrix Multiplication on CPU"},{"location":"blog/2021-10-13-mmt4d/#work-in-progress-on-matrix-multiplication-on-cpu","text":"","title":"Work in progress on Matrix Multiplication on CPU"},{"location":"blog/2021-10-13-mmt4d/#introduction","text":"Matrix multiplication (matmul) is an important operation in ML workloads that poses specific challenges to code generation. For example, matmul makes repeated accesses to the same data, which makes locality of reference a top concern. Moreover, modern CPUs instruction set architectures (ISAs) offer specialized SIMD instructions that the matmul implementation needs to use to achieve optimal performance, and these instructions expect data to be in a particular layout. This article is about an in-development MLIR operation, linalg.mmt4d , offering a compilation path for linalg.matmul that is designed from the ground up for these efficiency considerations. We are still in the early implementation phase of this linalg.mmt4d plan, but we feel confident that we know where we are going because what we are really doing here is importing into the compiler what we have learned working on optimized matrix multiplication libraries, particularly Ruy . We know what loop schedule and kernel we want the compiler to generate \u2014 essentially the same as we wrote in Ruy, give or take additional optimizations such as fusions and constant folding that become possible now that we are doing this within a compiler. This allows us to focus on how we get the compiler to generate that schedule and kernel with purely algebraic transformations that compose and enable further compiler optimizations. At the basis of this work is the extensible op system of the Linalg dialect in the MLIR compiler toolkit. In this case, a general purpose, mixed precision mmt4d op is defined via a high level description directly in the compiler and is then available to both users of the compiler (as a linalg.mmt4d op) or for direct emission via Python based IR construction (i.e. for direct integration into high level frameworks without rebuilding the compiler). The ability to define such new special forms cheaply, and without any systemic framework level cost, is part of the extensibility and composition story that we expect will become increasingly important in development and deployment scenarios in the future, and in this case, it let us spring board off of high quality code generation which was already well integrated and composed well with other features of the compiler.","title":"Introduction"},{"location":"blog/2021-10-13-mmt4d/#existing-matrix-multplication-code-generation","text":"Let us start by discussing IREE\u2019s existing matmul code generation and highlight the issues that mmt4d aims to overcome. The existing approach operates in-place on the source matrices. When we discuss \"tiling\" in this paragraph, we refer exclusively to the traversal \u2014 how these source matrices are traversed by the matmul loop. There is no \"tiled layout\" here, which will be the key difference with mmt4d below. The destination matrix is tiled into workgroups (CPU threads) tiles, then each workgroup tile is tiled to fit some level of CPU cache, and finally each tile is further tiled to fit target architecture registers (e.g. 8x8). That multi-level tiling means that the code works like the following loop nest: def tiled_matmul ( A , B , C , tile_m , tile_n , tile_k , tile_m_v , tile_n_v , tile_k_v ): m = A . shape [ 0 ] k = A . shape [ 1 ] n = B . shape [ 1 ] for m1 in range ( 0 , m , tile_m ): for n1 in range ( 0 , n , tile_n ): for k1 in range ( 0 , k , tile_k ): # First level of tiling views... lhs_tile = A [ m1 : m1 + tile_m , k1 : k1 + tile_k ] rhs_tile = B [ k1 : k1 + tile_k , n1 : n1 + tile_n ] dst_tile = C [ m1 : m1 + tile_m , n1 : n1 + tile_n ] for mv in range ( 0 , tile_m , tile_m_v ): for nv in range ( 0 , tile_n , tile_n_v ): for kv in range ( 0 , tile_k , tile_k_v ): # Register tiling views... lhs_tile_v = lhs_tile [ mv : mv + tile_m_v , kv : kv + tile_k_v ] rhs_tile_v = rhs_tile [ kv : kv + tile_k_v , nv : nv + tile_n_v ] # kernel. dst_tile [ mv : mv + tile_m_v , nv : nv + tile_n_v ] += np . matmul ( lhs_tile_v , rhs_tile_v ) return C The two main problems with this approach are: Overhead to meet SIMD ISA layout requirements : In practice, the kernel needs to use specific SIMD instructions to perform the arithmetic. They expect small tiles of the matrices to be loaded in registers, in a specific layout. If the matrix data wasn't already stored in memory in such a tiled layout, then the kernel has to perform such a data rearrangement on the fly, incurring substantial overhead. For NxN matrix multiplication, the kernel performs O(N 3 ) work on O(N 2 ) data, so doing that rearrangement there means O(N 3 ) overhead where O(N 2 ) should have sufficed, as this could have been done as a pre-processing step on O(N 2 ) data. Inefficent memory traversal: For efficiency reasons, we always need tile_m_v>1 and tile_n_v>1 . That is because the higher these values, the fewer memory-load instructions are needed overall; and this is also dictated by the SIMD instructions that we want to use. But that means that the kernel is accessing simultaneously multiple rows or columns of the left-hand and right-hand side matrices. And in this existing approach, they are stored in linear layout, not in a tiled layout, so these accesses are not contiguous in memory. This is detrimental to memory access performance, meaning the CPU caches , in multiple ways. One is that these multiple non-contiguous accesses may alias each other in the L1 cache because of low associativity .","title":"Existing Matrix Multplication Code Generation"},{"location":"blog/2021-10-13-mmt4d/#matrix-multiplication-operation-with-4d-tiled-operands","text":"For the reasons above, an efficient matmul implementation must reorder data into a tiled layout matching the target SIMD ISA and making the memory access patterns as contiguous as possible. IREE/MLIR defaults to bufferizing all tensors into a \"row-major\" order, meaning that the last-enumerated dimension is the one that is contiguous in memory. As we prefer not to write custom bufferization code, we can't specify an alternative layout for a tensor. Fortunately, it is possible to represent a 2D tiled layout as a 4D layout. For example, tensor<2x2x2x2xf32> can represent a 4x4 matrix made of 2x2 tiles, each of which is 2x2. The row-major layout on tensor<2x2x2x2xf32> makes each 2x2 tile contiguous and row-major, and arranges the 2x2 tiles themselves into a row-major 2x2 layout in the overall 4x4 matrix. Such a row-major-tiled layout is exactly what we need for the left-hand side of a matrix multiplication, because matrix multiplication traverses the left-hand side matrix row by row. But for the right-hand side matrix, we want a column-major-tiled layout. To solve this problem, we decide to implement not matrix multiplication, but matrix-multiplication-by-transposed-right-hand-side which is where the t in the linalg.mmt4d came from. Now such an op is happy with both the left and right-hand sides being row-major-tiled. The following example illustrates that. In these diagrams, each matrix element is rendered its memory offset. To compute the 2x2 block in the destination matrix, we will have to load two yellow blocks from LHS, RHS matrices respectively compute their matmul results (i.e. call the kernel), then the two blue blocks, and so on. As we can see, each tile loads data that is not contiguous. It would be better if we rearranged the elements in the following layout: Now tiles are stored contiguously in memory and the kernel can simply load them from memory into the registers that will be directly consumed by the SIMD instructions performing the multiplications. Moreover, the kernel is now loading from just two contiguous data streams, a simple memory access pattern which is sure to be efficient (regarding caches, etc) on any reasonable target hardware. We introduce a linalg.mmt4d operation that performs such a matrix multiplication on matrices in a tiled layout represented as 4D tensors. That leaves the question of how to represent, within the linalg dialect, the conversions between ordinary matrices represented as 2D tensors, and these tiled matrices represented as 4D tensors. Moreover, these conversions should be tileable and decompose well. Thankfully, the transformation from 2D to 4D can be written as a reshape followed by a transpose as in the following digram: So we can think of the outermost two dimensions of the 4D representations as the tile position in the overall matrix, and the innermost two as the element position within one tile. Hopefully the following Python pseudocode makes it more concrete: def pack_2d_4d ( operand , parallel_size , reduction_size ): i1 = operand . shape [ 0 ] // parallel_size # M1 i2 = parallel_size # M0 j1 = operand . shape [ 1 ] // reduction_size # K1 j2 = reduction_size # K0 operand_4d = np . reshape ( operand , [ i1 , i2 , j1 , j2 ]) return np . transpose ( operand_4d , [ 0 , 2 , 1 , 3 ]) # [M1, K1, M0, K0] Now the mmt4d operation will follow a structure as the multi level tiling, for simplicity we considered the case here where no L1 tiling is required only first level of distribution to workgroups: def mmt4d ( A , B , C , M0 , N0 , K0 ): M = A . shape [ 0 ] N = B . shape [ 1 ] Bt = np . transpose ( B , [ 1 , 0 ]) A4d = pack_2d_4d ( A , M0 , K0 ) Bt4d = pack_2d_4d ( Bt , N0 , K0 ) M1 = A4d . shape [ 0 ] N1 = Bt4d . shape [ 0 ] K1 = A4d . shape [ 1 ] for m1 in range ( 0 , M1 ): for n1 in range ( 0 , N1 ): for k1 in range ( 0 , K1 ): # Tile views that are contiguous in memory. lhs_tile = np . reshape ( A4d [ m1 , k1 , :, :], [ M0 , K0 ]) rhs_tile = np . reshape ( Bt4d [ n1 , k1 , :, :], [ N0 , K0 ]) # Inner kernel. C [ m1 , n1 , :, :] += np . matmul ( lhs_tile , np . transpose ( rhs_tile , [ 1 , 0 ])) # 4d -> 2D C2d = unpack_4d_2d ( C ) return C2d The resulting 4D tiled matrix still needs be rearranged back to the original layout as 2D tensor: def unpack_4d_2d ( operand ): i1 = operand . shape [ 0 ] # M1 j1 = operand . shape [ 1 ] # N1 i2 = operand . shape [ 2 ] # M0 j2 = operand . shape [ 3 ] # N0 operand_transposed = operand . transpose ([ 0 , 2 , 1 , 3 ]) # [M1, M0, N1, N0] return operand_transposed . reshape ([ i1 * i2 , j1 * j2 ]) # [M, N]","title":"Matrix Multiplication Operation With 4D Tiled Operands"},{"location":"blog/2021-10-13-mmt4d/#performance-results","text":"We benchmarked various float32 matmul problems of different sizes and the result showed that mmt4d is faster than the existing matmul implementation for bigger matrices as we can see the in the following chart: The SIMD instruction being used here is the simplest kind, a vector*scalar multiplication, and the storage orders of the matrices allow the existing implementation to directly load the vectors from the source matrices without any rearrangement overhead. So this case is particularly friendly to the existing code, which is why the mmt4d code is only faster for bigger matrices. To understand why mmt4d is faster in that case, we collected statistics of L1 cache misses: This shows that in this case, the better cache-friendliness of mmt4d, thanks to its simple contiguous memory access pattern, accounts for its higher performance. As we proceed with increasingly sophisticated SIMD targets, starting with the dot-product instructions found in current mobile devices for the int8 case and going to become generalized to all data types all the way to float32 over the next few years with upcoming ARM SIMD instructions, the advantage of mmt4d will widen for all sizes, not just the larger ones. Part of why we feel confident about the eventual performance that our approach will achieve is that, as mentioned in the introduction, we are rebuilding within the compiler an existing library 's schedule and kernel, and we have benchmark results about it.","title":"Performance Results"},{"location":"blog/2021-10-13-mmt4d/#conclusion","text":"We introduced a 4d tiled representation for 2d matrix-matrix multiplication with a decomposable algebric transformations that requires only reshape and transpose of input operands, we discussed and empirically showed how that solves major drawbacks in row-major linear matmul by providing a flexible way to match different ISA layout along with better cache locality achieving near peak performance. As was mentioned in the introduction, this work in under active development and the next immediate steps are to prove the rest of the hypothesis by: Handling dynamic sizes and padding to the next multiple of the target tile size. Implementing the integer case ( int32 += int8 * int8 ). Implementing the dispatch to different SIMD ISA variants at runtime. Implementing cache-friendly traversal for larger matmuls and multi-threading by interfacing with IREE's runtime dispatch. Improving the generated code by fusing the 4d tiled layout with the producers and consumers of the linalg.mmt4d .","title":"Conclusion"},{"location":"blog/2021-10-15-cuda-backend/","text":"Friday, October 15, 2021 By Thomas Raoux CUDA Backend in IREE \u00b6 IREE is being designed with re-targetability as a core goal: it should be possible to use IREE to target a broad spectrum of power regimes, from embedded systems to distributed clusters; and it should be possible to extend IREE to target new back-ends without having to reinvent the wheel each time. To explore this, we recently branched out from our initial focus on low-latency mobile deployments with a goal of using IREE to target data center workloads on Nvidia CUDA. This post describes how we quickly brought up a CUDA back-end for IREE and used it to train BERT , then shares some metrics and next steps. Bring up \u00b6 HAL support \u00b6 IREE has a HAL API that abstract all the targets behind a common interface. The first step to supporting a CUDA target was to map the HAL API onto CUDA. We use the CUDA driver API to reduce dependencies and be closer to the hardware. The HAL API is based on other GPU APIs like Vulkan and Metal, so it was a natural fit for CUDA. The HAL API exposes memory allocations, basic fill and memset commands, kernel dispatch, and general command buffer handling. The original implementation uses the CUDA graph API as a graph maps naturally to command buffers. There is also an implementation using CUDA streams for comparison. HAL exposes an API that can be tested independently, even if we are not able to create CUDA kernels yet we can test a large portion of the CUDA driver using CTS tests . Those can be run to make sure a system has the required CUDA support. Compiler support \u00b6 CUDA has an open source backend in LLVM generating PTX that we are leveraging. Therefore IREE can create NVVM (CUDA LLVM variant) and use LLVM's backend to generate PTX. The CUDA driver will do the \"last mile compilation\" at runtime to convert PTX into the GPU's native ISA. IREE compiler pipeline starts from linalg with tensor operands. A large part of the compiler is independent of the target. The linalg on tensor representation of the graph is broken up into dispatch regions that are processed by NVVM Codegen. A simple implementation of the compiler is to run bufferization and convert linalg to standard followed by conversion to NVVM/LLVM. Most of those transformation can re-use upstream MLIR transformations and share it with any other backend targeting LLVM IR. Leveraging MLIR conversion to LLVM will allow us to quickly go from a simple \"hello world\" to supporting full models. IREE code generation is based on MLIR infrastructure so each step can easily be tested independently using the MLIR lit framework. FlatBuffer definition \u00b6 Kernels are encoded in a FlatBuffer containing the PTX code as well as the workgroup size to use for the dispatch. This allows serialization of the kernels in the IR, it is then de-serialized by the HAL layer. table CUDAExecutableDef { // A map of entry point ordinals to string names as used in the shader // library. entry_points:[string]; // Block sizes for each entry point. block_sizes:[CUDABlockSizeDef]; // PTX string of the module. ptx_image:string; } Hello world \u00b6 Together those 3 steps are enough to provide most of the functionality and we can now successfully compile full models. The steps to reproduce running a simple op end to end through CUDA backend are described here . Performance \u00b6 Now that we have enabled functionality we need to look at the performance. Once again we can leverage existing MLIR transformations to speed up the developement work. Tiling and distribution \u00b6 The first obvious step to get efficient code on CUDA is to make sure we distribute the work on enough blocks and threads to fill up the GPU. At the time of bring up not all ops were being tiled and distributed in the common IREE layer. During dispatch region creation we apply tile and fuse which will distribute the work into a set of workgroups that are mapped to CUDA blocks. At the beginning of the code generation we look at the dispatch region and decide on the tile size for a workgroup. For CUDA we also decide the number of threads per block. We will then have a pass tiling the ops in the dispatch region a second time to distribute the work onto threads within the block. At this stage the IR looks like the following: %8 = \"gpu.thread_id\"() {dimension = \"x\"} : () -> index %9 = affine.apply affine_map<()[s0] -> (s0 * 4)>()[%8] %10 = memref.subview %in0[%9] [4] [1] : memref<128xf32, affine_map<(d0)[s0] -> (d0 + s0)>> to memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>> %11 = memref.subview %in1[%9] [4] [1] : memref<128xf32, affine_map<(d0)[s0] -> (d0 + s0)>> to memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>> %12 = memref.subview %out[%9] [4] [1] : memref<128xf32, affine_map<(d0)[s0] -> (d0 + s0)>> to memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>> linalg.generic { indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [\"parallel\"]} ins(%10, %11 : memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>>, memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>>) outs(%12 : memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>>) { ^bb0(%arg1: f32, %arg2: f32, %arg3: f32): // no predecessors %13 = addf %arg1, %arg2 : f32 linalg.yield %13 : f32 } Vectorization \u00b6 Even though GPUs execute most operations as scalar, memory operations are optimized to access 128 bits of data per thread. Therefore it is critical to vectorize load/store operations. After tiling to a size we vectorize the IR to get vector read/write mapping to load4/store4. This significantly improves the memory access pattern of the code generated. This convert the previous IR to: %8 = \"gpu.thread_id\"() {dimension = \"x\"} : () -> index %9 = affine.apply affine_map<()[s0] -> (s0 * 4)>()[%8] %10 = memref.subview %in0[%9] [4] [1] : memref<128xf32, affine_map<(d0)[s0] -> (d0 + s0)>> to memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>> %11 = memref.subview %in1[%9] [4] [1] : memref<128xf32, affine_map<(d0)[s0] -> (d0 + s0)>> to memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>> %12 = memref.subview %out[%9] [4] [1] : memref<128xf32, affine_map<(d0)[s0] -> (d0 + s0)>> to memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>> %13 = vector.transfer_read %10[%c0], %cst {in_bounds = [true]} : memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>>, vector<4xf32> %14 = vector.transfer_read %11[%c0], %cst {in_bounds = [true]} : memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>>, vector<4xf32> %15 = addf %13, %14 : vector<4xf32> vector.transfer_write %15, %12[%c0] {in_bounds = [true]} : vector<4xf32>, memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>> Shared memory optimization \u00b6 Nvidia GPUs have a fast shared memory that needs to be leveraged to optimize cases where we may be memory bound and have the potential to re-use memory reads. For operations like GEMM using shared memory gives us a significant speed up. We leverage memory promotion, vector distribution and software pipelining transformations from MLIR to generate efficient copies from global to shared memory that can be interleaved with the compute work. Optimization pipeline \u00b6 Those different transformations compose to this flow: The full dump step by step of a linalg.matmul operation can be found here . Results and next steps \u00b6 GEMM \u00b6 We compare the performance of a single GEMM operation to highly optimized library cuBLAS using mmperf framework . The graph can be re-produced based on instructions on mmperf Future work \u00b6 Nod.ai has contributed an experimental HAL module for ROCM that allows us to re-use the compiler parts to support ROCM, more support is going to be added in the future. Several performance improvements are still under progress, including optimizing the runtime allocator to reduce the host-side overhead and tuning tile sizes based profiling. Several models are running and we will publish more detailed benchmark results in the near future.","title":"CUDA backend"},{"location":"blog/2021-10-15-cuda-backend/#cuda-backend-in-iree","text":"IREE is being designed with re-targetability as a core goal: it should be possible to use IREE to target a broad spectrum of power regimes, from embedded systems to distributed clusters; and it should be possible to extend IREE to target new back-ends without having to reinvent the wheel each time. To explore this, we recently branched out from our initial focus on low-latency mobile deployments with a goal of using IREE to target data center workloads on Nvidia CUDA. This post describes how we quickly brought up a CUDA back-end for IREE and used it to train BERT , then shares some metrics and next steps.","title":"CUDA Backend in IREE"},{"location":"blog/2021-10-15-cuda-backend/#bring-up","text":"","title":"Bring up"},{"location":"blog/2021-10-15-cuda-backend/#hal-support","text":"IREE has a HAL API that abstract all the targets behind a common interface. The first step to supporting a CUDA target was to map the HAL API onto CUDA. We use the CUDA driver API to reduce dependencies and be closer to the hardware. The HAL API is based on other GPU APIs like Vulkan and Metal, so it was a natural fit for CUDA. The HAL API exposes memory allocations, basic fill and memset commands, kernel dispatch, and general command buffer handling. The original implementation uses the CUDA graph API as a graph maps naturally to command buffers. There is also an implementation using CUDA streams for comparison. HAL exposes an API that can be tested independently, even if we are not able to create CUDA kernels yet we can test a large portion of the CUDA driver using CTS tests . Those can be run to make sure a system has the required CUDA support.","title":"HAL support"},{"location":"blog/2021-10-15-cuda-backend/#compiler-support","text":"CUDA has an open source backend in LLVM generating PTX that we are leveraging. Therefore IREE can create NVVM (CUDA LLVM variant) and use LLVM's backend to generate PTX. The CUDA driver will do the \"last mile compilation\" at runtime to convert PTX into the GPU's native ISA. IREE compiler pipeline starts from linalg with tensor operands. A large part of the compiler is independent of the target. The linalg on tensor representation of the graph is broken up into dispatch regions that are processed by NVVM Codegen. A simple implementation of the compiler is to run bufferization and convert linalg to standard followed by conversion to NVVM/LLVM. Most of those transformation can re-use upstream MLIR transformations and share it with any other backend targeting LLVM IR. Leveraging MLIR conversion to LLVM will allow us to quickly go from a simple \"hello world\" to supporting full models. IREE code generation is based on MLIR infrastructure so each step can easily be tested independently using the MLIR lit framework.","title":"Compiler support"},{"location":"blog/2021-10-15-cuda-backend/#flatbuffer-definition","text":"Kernels are encoded in a FlatBuffer containing the PTX code as well as the workgroup size to use for the dispatch. This allows serialization of the kernels in the IR, it is then de-serialized by the HAL layer. table CUDAExecutableDef { // A map of entry point ordinals to string names as used in the shader // library. entry_points:[string]; // Block sizes for each entry point. block_sizes:[CUDABlockSizeDef]; // PTX string of the module. ptx_image:string; }","title":"FlatBuffer definition"},{"location":"blog/2021-10-15-cuda-backend/#hello-world","text":"Together those 3 steps are enough to provide most of the functionality and we can now successfully compile full models. The steps to reproduce running a simple op end to end through CUDA backend are described here .","title":"Hello world"},{"location":"blog/2021-10-15-cuda-backend/#performance","text":"Now that we have enabled functionality we need to look at the performance. Once again we can leverage existing MLIR transformations to speed up the developement work.","title":"Performance"},{"location":"blog/2021-10-15-cuda-backend/#tiling-and-distribution","text":"The first obvious step to get efficient code on CUDA is to make sure we distribute the work on enough blocks and threads to fill up the GPU. At the time of bring up not all ops were being tiled and distributed in the common IREE layer. During dispatch region creation we apply tile and fuse which will distribute the work into a set of workgroups that are mapped to CUDA blocks. At the beginning of the code generation we look at the dispatch region and decide on the tile size for a workgroup. For CUDA we also decide the number of threads per block. We will then have a pass tiling the ops in the dispatch region a second time to distribute the work onto threads within the block. At this stage the IR looks like the following: %8 = \"gpu.thread_id\"() {dimension = \"x\"} : () -> index %9 = affine.apply affine_map<()[s0] -> (s0 * 4)>()[%8] %10 = memref.subview %in0[%9] [4] [1] : memref<128xf32, affine_map<(d0)[s0] -> (d0 + s0)>> to memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>> %11 = memref.subview %in1[%9] [4] [1] : memref<128xf32, affine_map<(d0)[s0] -> (d0 + s0)>> to memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>> %12 = memref.subview %out[%9] [4] [1] : memref<128xf32, affine_map<(d0)[s0] -> (d0 + s0)>> to memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>> linalg.generic { indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [\"parallel\"]} ins(%10, %11 : memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>>, memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>>) outs(%12 : memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>>) { ^bb0(%arg1: f32, %arg2: f32, %arg3: f32): // no predecessors %13 = addf %arg1, %arg2 : f32 linalg.yield %13 : f32 }","title":"Tiling and distribution"},{"location":"blog/2021-10-15-cuda-backend/#vectorization","text":"Even though GPUs execute most operations as scalar, memory operations are optimized to access 128 bits of data per thread. Therefore it is critical to vectorize load/store operations. After tiling to a size we vectorize the IR to get vector read/write mapping to load4/store4. This significantly improves the memory access pattern of the code generated. This convert the previous IR to: %8 = \"gpu.thread_id\"() {dimension = \"x\"} : () -> index %9 = affine.apply affine_map<()[s0] -> (s0 * 4)>()[%8] %10 = memref.subview %in0[%9] [4] [1] : memref<128xf32, affine_map<(d0)[s0] -> (d0 + s0)>> to memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>> %11 = memref.subview %in1[%9] [4] [1] : memref<128xf32, affine_map<(d0)[s0] -> (d0 + s0)>> to memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>> %12 = memref.subview %out[%9] [4] [1] : memref<128xf32, affine_map<(d0)[s0] -> (d0 + s0)>> to memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>> %13 = vector.transfer_read %10[%c0], %cst {in_bounds = [true]} : memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>>, vector<4xf32> %14 = vector.transfer_read %11[%c0], %cst {in_bounds = [true]} : memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>>, vector<4xf32> %15 = addf %13, %14 : vector<4xf32> vector.transfer_write %15, %12[%c0] {in_bounds = [true]} : vector<4xf32>, memref<4xf32, affine_map<(d0)[s0] -> (d0 + s0)>>","title":"Vectorization"},{"location":"blog/2021-10-15-cuda-backend/#shared-memory-optimization","text":"Nvidia GPUs have a fast shared memory that needs to be leveraged to optimize cases where we may be memory bound and have the potential to re-use memory reads. For operations like GEMM using shared memory gives us a significant speed up. We leverage memory promotion, vector distribution and software pipelining transformations from MLIR to generate efficient copies from global to shared memory that can be interleaved with the compute work.","title":"Shared memory optimization"},{"location":"blog/2021-10-15-cuda-backend/#optimization-pipeline","text":"Those different transformations compose to this flow: The full dump step by step of a linalg.matmul operation can be found here .","title":"Optimization pipeline"},{"location":"blog/2021-10-15-cuda-backend/#results-and-next-steps","text":"","title":"Results and next steps"},{"location":"blog/2021-10-15-cuda-backend/#gemm","text":"We compare the performance of a single GEMM operation to highly optimized library cuBLAS using mmperf framework . The graph can be re-produced based on instructions on mmperf","title":"GEMM"},{"location":"blog/2021-10-15-cuda-backend/#future-work","text":"Nod.ai has contributed an experimental HAL module for ROCM that allows us to re-use the compiler parts to support ROCM, more support is going to be added in the future. Several performance improvements are still under progress, including optimizing the runtime allocator to reduce the host-side overhead and tuning tile sizes based profiling. Several models are running and we will publish more detailed benchmark results in the near future.","title":"Future work"},{"location":"building-from-source/","text":"Building IREE from source \u00b6 While IREE does offer binary distributions for its compiler tools and Python bindings , building from source is still useful when using IREE's runtime or when making changes to the compiler or import tools themselves. Reference pages \u00b6 Getting started Bindings and importers Android cross-compilation RISC-V cross-compilation","title":"Building IREE from source"},{"location":"building-from-source/#building-iree-from-source","text":"While IREE does offer binary distributions for its compiler tools and Python bindings , building from source is still useful when using IREE's runtime or when making changes to the compiler or import tools themselves.","title":"Building IREE from source"},{"location":"building-from-source/#reference-pages","text":"Getting started Bindings and importers Android cross-compilation RISC-V cross-compilation","title":"Reference pages"},{"location":"building-from-source/android/","text":"Android cross-compilation \u00b6 Running on a platform like Android involves cross-compiling from a host platform (e.g. Linux) to a target platform (a specific Android version and system architecture): IREE's compiler is built on the host and is used there to generate modules for the target IREE's runtime is built on the host for the target. The runtime is then either pushed to the target to run natively or is bundled into an Android APK Prerequisites \u00b6 Host environment setup \u00b6 You should already be able to build IREE from source on your host platform. Please make sure you have followed the getting started steps. Install Android NDK and ADB \u00b6 The Android Native Developer Kit (NDK) is needed to use native C/C++ code on Android. You can download it here , or, if you have installed Android Studio , you can follow this guide instead. Note Make sure the ANDROID_NDK environment variable is set after installing the NDK. ADB (the Android Debug Bridge) is also needed to communicate with Android devices from the command line. Install it following the official user guide . Configure and build \u00b6 Host configuration \u00b6 Build and install on your host machine: cmake -GNinja -B ../iree-build/ \\ -DCMAKE_INSTALL_PREFIX = ../iree-build/install \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ . cmake --build ../iree-build/ --target install Target configuration \u00b6 Build the runtime using the Android NDK toolchain: Linux and MacOS Windows cmake -GNinja -B ../iree-build-android/ \\ -DCMAKE_TOOLCHAIN_FILE = \" ${ ANDROID_NDK ? } /build/cmake/android.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = \" $PWD /../iree-build/install\" \\ -DANDROID_ABI = \"arm64-v8a\" \\ -DANDROID_PLATFORM = \"android-29\" \\ -DIREE_BUILD_COMPILER = OFF \\ . cmake --build ../iree-build-android/ cmake -GNinja -B ../iree-build-android/ \\ -DCMAKE_TOOLCHAIN_FILE = \"%ANDROID_NDK%/build/cmake/android.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = \"%CD%/../iree-build/install\" \\ -DANDROID_ABI = \"arm64-v8a\" \\ -DANDROID_PLATFORM = \"android-29\" \\ -DIREE_BUILD_COMPILER = OFF \\ . cmake --build ../iree-build-android/ Note See the Android NDK CMake guide and Android Studio CMake guide for details on configuring CMake for Android. The specific ANDROID_ABI and ANDROID_PLATFORM used should match your target device. Running Android tests \u00b6 Make sure you enable developer options and USB debugging on your Android device and can see your it when you run adb devices , then run all built tests through CTest : ctest --test-dir ../iree-build-android/ --output-on-failure This will automatically upload build artifacts to the connected Android device, run the tests, then report the status back to your host machine. Running tools directly \u00b6 Invoke the host compiler tools to produce a bytecode module FlatBuffer: ../iree-build/install/bin/iree-compile \\ --iree-hal-target-backends = vmvx \\ samples/models/simple_abs.mlir \\ -o /tmp/simple_abs_vmvx.vmfb Push the Android runtime tools to the device, along with any FlatBuffer files: adb push ../iree-build-android/tools/iree-run-module /data/local/tmp/ adb shell chmod +x /data/local/tmp/iree-run-module adb push /tmp/simple_abs_vmvx.vmfb /data/local/tmp/ Run the tool: adb shell /data/local/tmp/iree-run-module --device = local-task \\ --module_file = /data/local/tmp/simple_abs_vmvx.vmfb \\ --entry_function = abs \\ --function_input = \"f32=-5\"","title":"Android cross-compilation"},{"location":"building-from-source/android/#android-cross-compilation","text":"Running on a platform like Android involves cross-compiling from a host platform (e.g. Linux) to a target platform (a specific Android version and system architecture): IREE's compiler is built on the host and is used there to generate modules for the target IREE's runtime is built on the host for the target. The runtime is then either pushed to the target to run natively or is bundled into an Android APK","title":"Android cross-compilation"},{"location":"building-from-source/android/#prerequisites","text":"","title":"Prerequisites"},{"location":"building-from-source/android/#host-environment-setup","text":"You should already be able to build IREE from source on your host platform. Please make sure you have followed the getting started steps.","title":"Host environment setup"},{"location":"building-from-source/android/#install-android-ndk-and-adb","text":"The Android Native Developer Kit (NDK) is needed to use native C/C++ code on Android. You can download it here , or, if you have installed Android Studio , you can follow this guide instead. Note Make sure the ANDROID_NDK environment variable is set after installing the NDK. ADB (the Android Debug Bridge) is also needed to communicate with Android devices from the command line. Install it following the official user guide .","title":"Install Android NDK and ADB"},{"location":"building-from-source/android/#configure-and-build","text":"","title":"Configure and build"},{"location":"building-from-source/android/#host-configuration","text":"Build and install on your host machine: cmake -GNinja -B ../iree-build/ \\ -DCMAKE_INSTALL_PREFIX = ../iree-build/install \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ . cmake --build ../iree-build/ --target install","title":"Host configuration"},{"location":"building-from-source/android/#target-configuration","text":"Build the runtime using the Android NDK toolchain: Linux and MacOS Windows cmake -GNinja -B ../iree-build-android/ \\ -DCMAKE_TOOLCHAIN_FILE = \" ${ ANDROID_NDK ? } /build/cmake/android.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = \" $PWD /../iree-build/install\" \\ -DANDROID_ABI = \"arm64-v8a\" \\ -DANDROID_PLATFORM = \"android-29\" \\ -DIREE_BUILD_COMPILER = OFF \\ . cmake --build ../iree-build-android/ cmake -GNinja -B ../iree-build-android/ \\ -DCMAKE_TOOLCHAIN_FILE = \"%ANDROID_NDK%/build/cmake/android.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = \"%CD%/../iree-build/install\" \\ -DANDROID_ABI = \"arm64-v8a\" \\ -DANDROID_PLATFORM = \"android-29\" \\ -DIREE_BUILD_COMPILER = OFF \\ . cmake --build ../iree-build-android/ Note See the Android NDK CMake guide and Android Studio CMake guide for details on configuring CMake for Android. The specific ANDROID_ABI and ANDROID_PLATFORM used should match your target device.","title":"Target configuration"},{"location":"building-from-source/android/#running-android-tests","text":"Make sure you enable developer options and USB debugging on your Android device and can see your it when you run adb devices , then run all built tests through CTest : ctest --test-dir ../iree-build-android/ --output-on-failure This will automatically upload build artifacts to the connected Android device, run the tests, then report the status back to your host machine.","title":"Running Android tests"},{"location":"building-from-source/android/#running-tools-directly","text":"Invoke the host compiler tools to produce a bytecode module FlatBuffer: ../iree-build/install/bin/iree-compile \\ --iree-hal-target-backends = vmvx \\ samples/models/simple_abs.mlir \\ -o /tmp/simple_abs_vmvx.vmfb Push the Android runtime tools to the device, along with any FlatBuffer files: adb push ../iree-build-android/tools/iree-run-module /data/local/tmp/ adb shell chmod +x /data/local/tmp/iree-run-module adb push /tmp/simple_abs_vmvx.vmfb /data/local/tmp/ Run the tool: adb shell /data/local/tmp/iree-run-module --device = local-task \\ --module_file = /data/local/tmp/simple_abs_vmvx.vmfb \\ --entry_function = abs \\ --function_input = \"f32=-5\"","title":"Running tools directly"},{"location":"building-from-source/getting-started/","text":"Getting started \u00b6 Prerequisites \u00b6 You will need to install CMake , the Ninja CMake generator, and the clang or MSVC C/C++ compilers: Note You are welcome to try different CMake generators and compilers, but IREE devs and CIs exclusively use these and other configurations are \"best effort\". Additionally, compilation on macOS is \"best effort\" as well, though we generally expect it to work due to its similarity with Linux. Patches to improve support for these are always welcome. Linux and macOS Windows Install a compiler/linker (typically \"clang\" and \"lld\" package) Install CMake (typically \"cmake\" package) Install Ninja (typically \"ninja-build\" package) On a relatively recent Debian/Ubuntu: sudo apt install cmake ninja-build clang lld Install MSVC from Visual Studio or \"Tools for Visual Studio\" on the official downloads page Install CMake from the official downloads page Install Ninja either from the official site Note You will need to initialize MSVC by running vcvarsall.bat to use it from the command line. See the official documentation for details. Clone and build \u00b6 Use Git to clone the IREE repository and initialize its submodules: git clone https://github.com/iree-org/iree.git cd iree git submodule update --init Configure then build all targets using CMake: Configure CMake: Linux and MacOS Windows # Recommended for simple development using clang and lld: cmake -GNinja -B ../iree-build/ -S . \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ -DIREE_ENABLE_ASSERTIONS = ON \\ -DCMAKE_C_COMPILER = clang \\ -DCMAKE_CXX_COMPILER = clang++ \\ -DIREE_ENABLE_LLD = ON # Alternately, with system compiler and your choice of CMake generator: # cmake -B ../iree-build/ -S . # Additional quality of life CMake flags: # Enable ccache: # -DIREE_ENABLE_CCACHE=ON cmake -GNinja -B ../iree-build/ -S . \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ -DIREE_ENABLE_ASSERTIONS = ON Build: cmake --build ../iree-build/ Tip We recommend using the RelWithDebInfo build type by default for a good balance of debugging information and performance. The Debug , Release , and MinSizeRel build types are useful in more specific scenarios. In particular, note that several useful LLVM debugging features are only available in Debug builds. See the official CMake documentation for general details. What's next? \u00b6 Running tests \u00b6 Build test dependencies: cmake --build ../iree-build --target iree-test-deps Run all built tests through CTest : ctest --test-dir ../iree-build/ --output-on-failure Take a look around \u00b6 Check out the contents of the 'tools' build directory: ls ../iree-build/tools/ ../iree-build/tools/iree-compile --help","title":"Getting started"},{"location":"building-from-source/getting-started/#getting-started","text":"","title":"Getting started"},{"location":"building-from-source/getting-started/#prerequisites","text":"You will need to install CMake , the Ninja CMake generator, and the clang or MSVC C/C++ compilers: Note You are welcome to try different CMake generators and compilers, but IREE devs and CIs exclusively use these and other configurations are \"best effort\". Additionally, compilation on macOS is \"best effort\" as well, though we generally expect it to work due to its similarity with Linux. Patches to improve support for these are always welcome. Linux and macOS Windows Install a compiler/linker (typically \"clang\" and \"lld\" package) Install CMake (typically \"cmake\" package) Install Ninja (typically \"ninja-build\" package) On a relatively recent Debian/Ubuntu: sudo apt install cmake ninja-build clang lld Install MSVC from Visual Studio or \"Tools for Visual Studio\" on the official downloads page Install CMake from the official downloads page Install Ninja either from the official site Note You will need to initialize MSVC by running vcvarsall.bat to use it from the command line. See the official documentation for details.","title":"Prerequisites"},{"location":"building-from-source/getting-started/#clone-and-build","text":"Use Git to clone the IREE repository and initialize its submodules: git clone https://github.com/iree-org/iree.git cd iree git submodule update --init Configure then build all targets using CMake: Configure CMake: Linux and MacOS Windows # Recommended for simple development using clang and lld: cmake -GNinja -B ../iree-build/ -S . \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ -DIREE_ENABLE_ASSERTIONS = ON \\ -DCMAKE_C_COMPILER = clang \\ -DCMAKE_CXX_COMPILER = clang++ \\ -DIREE_ENABLE_LLD = ON # Alternately, with system compiler and your choice of CMake generator: # cmake -B ../iree-build/ -S . # Additional quality of life CMake flags: # Enable ccache: # -DIREE_ENABLE_CCACHE=ON cmake -GNinja -B ../iree-build/ -S . \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ -DIREE_ENABLE_ASSERTIONS = ON Build: cmake --build ../iree-build/ Tip We recommend using the RelWithDebInfo build type by default for a good balance of debugging information and performance. The Debug , Release , and MinSizeRel build types are useful in more specific scenarios. In particular, note that several useful LLVM debugging features are only available in Debug builds. See the official CMake documentation for general details.","title":"Clone and build"},{"location":"building-from-source/getting-started/#whats-next","text":"","title":"What's next?"},{"location":"building-from-source/getting-started/#running-tests","text":"Build test dependencies: cmake --build ../iree-build --target iree-test-deps Run all built tests through CTest : ctest --test-dir ../iree-build/ --output-on-failure","title":"Running tests"},{"location":"building-from-source/getting-started/#take-a-look-around","text":"Check out the contents of the 'tools' build directory: ls ../iree-build/tools/ ../iree-build/tools/iree-compile --help","title":"Take a look around"},{"location":"building-from-source/python-bindings-and-importers/","text":"Python bindings and importers \u00b6 Attention These components are more complicated to build from source than the rest of the project. If your usage does not require making source changes, prefer to install from the official binary distributions instead. This page covers how to build IREE's Python-based bindings and import tools from source. These components are built using CMake as well as other dependencies and each section extends the basic build steps in the getting started page. Building Python bindings \u00b6 This section describes how to build and interactively use built-from-source Python bindings for the following packages: Python Import Description import iree.compiler IREE's generic compiler tools and helpers import iree.runtime IREE's runtime, including CPU and GPU backends Also see instructions for installing pre-built binaries . Pre-requisites: A relatively recent Python3 installation >=3.7 (we aim to support non-eol Python versions ). Installation of python dependencies as specified in runtime/bindings/python/iree/runtime/build_requirements.txt . CMake Variables: IREE_BUILD_PYTHON_BINDINGS : BOOL Enables building of Python bindings under runtime/bindings/python in the repository. Defaults to OFF . Python3_EXECUTABLE : PATH Full path to the Python3 executable to build against. If not specified, CMake will auto-detect this, which often yields incorrect results on systems with multiple Python versions. Explicitly setting this is recommended. Note that mixed case of the option. Environment setup \u00b6 We recommend using virtual environments to manage python packages, such as through venv , which may need to be installed via your system package manager ( about , tutorial ): Linux and MacOS Windows # Make sure your 'python' is what you expect. Note that on multi-python # systems, this may have a version suffix, and on many Linuxes and MacOS where # python2 and python3 co-exist, you may also want to use `python3`. which python python --version # Create a persistent virtual environment (first time only). python -m venv iree.venv # Activate the virtual environment (per shell). # Now the `python` command will resolve to your virtual environment # (even on systems where you typically use `python3`). source iree.venv/bin/activate # Upgrade PIP. On Linux, many packages cannot be installed for older # PIP versions. See: https://github.com/pypa/manylinux python -m pip install --upgrade pip # Install IREE build pre-requisites. python -m pip install -r ./runtime/bindings/python/iree/runtime/build_requirements.txt # Create a persistent virtual environment (first time only). python -m venv . venv # Activate the virtual environment (per shell). # Now the `python` command will resolve to your virtual environment # (even on systems where you typically use `python3`). . venv \\ Scripts \\ activate . bat # Upgrade PIP. python -m pip install - -upgrade pip # Install IREE build pre-requisites. python -m pip install -r runtime \\ bindings \\ python \\ iree \\ runtime \\ build_requirements . txt When you are done with the venv, you can close it by closing your shell or running deactivate . Building with CMake \u00b6 From the iree-build directory: Linux and MacOS Windows cmake \\ -GNinja \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ -DIREE_BUILD_PYTHON_BINDINGS = ON \\ -DPython3_EXECUTABLE = \" $( which python ) \" \\ . cmake --build . # Add ./bindings/python and compiler-api/python_package to PYTHONPATH and # use the API. source .env && export PYTHONPATH python -c \"import iree.compiler\" python -c \"import iree.runtime\" cmake -GNinja -DCMAKE_BUILD_TYPE = RelWithDebInfo -DIREE_BUILD_PYTHON_BINDINGS = ON . cmake - -build . # Add bindings\\python and compiler-api\\python_package to PYTHONPATH and use # the API. set PYTHONPATH = \"$pwd\\compiler-api\\python_package;$pwd\\bindings\\python;%PYTHONPATH%\" python -c \"import iree.compiler\" python -c \"import iree.runtime\" Tests can now be run individually via python or via ctest. Building TensorFlow frontend bindings \u00b6 This section describes how to build compiler tools and Python bindings for importing models from various TensorFlow-ecosystem frontends, including TensorFlow, XLA (used for JAX), and TFLite. It extends the instructions in Building Python Bindings above with additional steps that are TensorFlow specific. There are various ways to achieve these ends, but this section describes the canonical method that the core developers recommend. Upon completing these steps, you will have access to additional Python packages: Python Import Description import iree.compiler.tools.tf Tools for importing from TensorFlow import iree.compiler.tools.tflite Tools for importing from TensorFlow Lite import iree.compiler.tools.xla Tools for importing from XLA These tools packages are needed in order for the frontend specific, high-level APIs under import iree.compiler.tf , import iree.compiler.tflite , import iree.compiler.xla , and import iree.jax to be fully functional. Warning This section is under construction. Refer to the source documentation for the latest building from source instructions. Note Due to the difficulties using Bazel and compiling TensorFlow, only compilation on Linux with clang is supported. Other OS's and compilers are \"best effort\" with patches to improve support welcome.","title":"Python bindings and importers"},{"location":"building-from-source/python-bindings-and-importers/#python-bindings-and-importers","text":"Attention These components are more complicated to build from source than the rest of the project. If your usage does not require making source changes, prefer to install from the official binary distributions instead. This page covers how to build IREE's Python-based bindings and import tools from source. These components are built using CMake as well as other dependencies and each section extends the basic build steps in the getting started page.","title":"Python bindings and importers"},{"location":"building-from-source/python-bindings-and-importers/#building-python-bindings","text":"This section describes how to build and interactively use built-from-source Python bindings for the following packages: Python Import Description import iree.compiler IREE's generic compiler tools and helpers import iree.runtime IREE's runtime, including CPU and GPU backends Also see instructions for installing pre-built binaries . Pre-requisites: A relatively recent Python3 installation >=3.7 (we aim to support non-eol Python versions ). Installation of python dependencies as specified in runtime/bindings/python/iree/runtime/build_requirements.txt . CMake Variables: IREE_BUILD_PYTHON_BINDINGS : BOOL Enables building of Python bindings under runtime/bindings/python in the repository. Defaults to OFF . Python3_EXECUTABLE : PATH Full path to the Python3 executable to build against. If not specified, CMake will auto-detect this, which often yields incorrect results on systems with multiple Python versions. Explicitly setting this is recommended. Note that mixed case of the option.","title":"Building Python bindings"},{"location":"building-from-source/python-bindings-and-importers/#environment-setup","text":"We recommend using virtual environments to manage python packages, such as through venv , which may need to be installed via your system package manager ( about , tutorial ): Linux and MacOS Windows # Make sure your 'python' is what you expect. Note that on multi-python # systems, this may have a version suffix, and on many Linuxes and MacOS where # python2 and python3 co-exist, you may also want to use `python3`. which python python --version # Create a persistent virtual environment (first time only). python -m venv iree.venv # Activate the virtual environment (per shell). # Now the `python` command will resolve to your virtual environment # (even on systems where you typically use `python3`). source iree.venv/bin/activate # Upgrade PIP. On Linux, many packages cannot be installed for older # PIP versions. See: https://github.com/pypa/manylinux python -m pip install --upgrade pip # Install IREE build pre-requisites. python -m pip install -r ./runtime/bindings/python/iree/runtime/build_requirements.txt # Create a persistent virtual environment (first time only). python -m venv . venv # Activate the virtual environment (per shell). # Now the `python` command will resolve to your virtual environment # (even on systems where you typically use `python3`). . venv \\ Scripts \\ activate . bat # Upgrade PIP. python -m pip install - -upgrade pip # Install IREE build pre-requisites. python -m pip install -r runtime \\ bindings \\ python \\ iree \\ runtime \\ build_requirements . txt When you are done with the venv, you can close it by closing your shell or running deactivate .","title":"Environment setup"},{"location":"building-from-source/python-bindings-and-importers/#building-with-cmake","text":"From the iree-build directory: Linux and MacOS Windows cmake \\ -GNinja \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ -DIREE_BUILD_PYTHON_BINDINGS = ON \\ -DPython3_EXECUTABLE = \" $( which python ) \" \\ . cmake --build . # Add ./bindings/python and compiler-api/python_package to PYTHONPATH and # use the API. source .env && export PYTHONPATH python -c \"import iree.compiler\" python -c \"import iree.runtime\" cmake -GNinja -DCMAKE_BUILD_TYPE = RelWithDebInfo -DIREE_BUILD_PYTHON_BINDINGS = ON . cmake - -build . # Add bindings\\python and compiler-api\\python_package to PYTHONPATH and use # the API. set PYTHONPATH = \"$pwd\\compiler-api\\python_package;$pwd\\bindings\\python;%PYTHONPATH%\" python -c \"import iree.compiler\" python -c \"import iree.runtime\" Tests can now be run individually via python or via ctest.","title":"Building with CMake"},{"location":"building-from-source/python-bindings-and-importers/#building-tensorflow-frontend-bindings","text":"This section describes how to build compiler tools and Python bindings for importing models from various TensorFlow-ecosystem frontends, including TensorFlow, XLA (used for JAX), and TFLite. It extends the instructions in Building Python Bindings above with additional steps that are TensorFlow specific. There are various ways to achieve these ends, but this section describes the canonical method that the core developers recommend. Upon completing these steps, you will have access to additional Python packages: Python Import Description import iree.compiler.tools.tf Tools for importing from TensorFlow import iree.compiler.tools.tflite Tools for importing from TensorFlow Lite import iree.compiler.tools.xla Tools for importing from XLA These tools packages are needed in order for the frontend specific, high-level APIs under import iree.compiler.tf , import iree.compiler.tflite , import iree.compiler.xla , and import iree.jax to be fully functional. Warning This section is under construction. Refer to the source documentation for the latest building from source instructions. Note Due to the difficulties using Bazel and compiling TensorFlow, only compilation on Linux with clang is supported. Other OS's and compilers are \"best effort\" with patches to improve support welcome.","title":"Building TensorFlow frontend bindings"},{"location":"building-from-source/riscv/","text":"RISC-V cross-compilation \u00b6 Running on a platform like RISC-V involves cross-compiling from a host platform (e.g. Linux) to a target platform (a specific RISC-V CPU architecture and operating system): IREE's compiler is built on the host and is used there to generate modules for the target IREE's runtime is built on the host for the target. The runtime is then pushed to the target to run natively. Prerequisites \u00b6 Host environment setup \u00b6 You should already be able to build IREE from source on your host platform. Please make sure you have followed the getting started steps. Install RISC-V cross-compile toolchain and emulator \u00b6 You'll need a RISC-V LLVM compilation toolchain and a RISC-V enabled QEMU emulator. See instructions in the following links Clang getting started RISC-V GNU toolchain QEMU RISC-V Linux QEMU Note The RISCV_TOOLCHAIN_ROOT environment variable needs to be set to the root directory of the installed GNU toolchain when building the RISC-V compiler target and the runtime library. Install prebuilt RISC-V tools (RISC-V 64-bit Linux toolchain) \u00b6 Execute the following script to download the prebuilt RISC-V toolchain and QEMU from the IREE root directory: ./build_tools/riscv/riscv_bootstrap.sh Support vector extension \u00b6 For RISC-V vector extensions support, see additional instructions Configure and build \u00b6 Host configuration \u00b6 Build and install on your host machine: cmake -GNinja -B ../iree-build/ \\ -DCMAKE_C_COMPILER = clang \\ -DCMAKE_CXX_COMPILER = clang++ \\ -DCMAKE_INSTALL_PREFIX = ../iree-build/install \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ . cmake --build ../iree-build/ --target install Target configuration \u00b6 The following instruction shows how to build for a RISC-V 64-bit Linux machine. For other RISC-V targets, please refer to riscv.toolchain.cmake as a reference of how to set up the cmake configuration. RISC-V 64-bit Linux target \u00b6 cmake -GNinja -B ../iree-build-riscv/ \\ -DCMAKE_TOOLCHAIN_FILE = \"./build_tools/cmake/riscv.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = $( realpath ../iree-build/install ) \\ -DRISCV_CPU = rv64 \\ -DIREE_BUILD_COMPILER = OFF \\ -DRISCV_TOOLCHAIN_ROOT = ${ RISCV_TOOLCHAIN_ROOT } \\ . cmake --build ../iree-build-riscv/ Running IREE bytecode modules on the RISC-V system \u00b6 Note The following instructions are meant for the RISC-V 64-bit Linux target. For the bare-metal target, please refer to simple_embedding to see how to build a ML workload for a bare-metal machine. Set the path to qemu-riscv64 Linux emulator binary in the QEMU_BIN environment variable. If it is installed with riscv_bootstrap.sh , the path is default at ${HOME}/riscv/qemu/linux/RISCV/bin/qemu-riscv64. export QEMU_BIN = <path to qemu-riscv64 binary> Invoke the host compiler tools to produce a bytecode module FlatBuffer: ../iree-build/install/bin/iree-compile \\ --iree-hal-target-backends = vmvx \\ samples/models/simple_abs.mlir \\ -o /tmp/simple_abs_vmvx.vmfb Run the RISC-V emulation: ${ QEMU_BIN } \\ -cpu rv64 \\ -L ${ RISCV_TOOLCHAIN_ROOT } /sysroot/ \\ ../iree-build-riscv/tools/iree-run-module \\ --device = local-task \\ --module_file = /tmp/simple_abs_vmvx.vmfb \\ --entry_function = abs \\ --function_input = f32 = -5 Optional configuration \u00b6 RISC-V Vector extensions allows SIMD code to run more efficiently. To enable the vector extension for the compiler toolchain and the emulator, build the tools from the following sources: RISC-V toolchain is built from https://github.com/llvm/llvm-project (main branch). Currently, the LLVM compiler is built on GNU toolchain, including libgcc, GNU linker, and C libraries. You need to build GNU toolchain first. Clone GNU toolchain from: https://github.com/riscv/riscv-gnu-toolchain (master branch). Switch the \"riscv-binutils\" submodule to git://sourceware.org/git/binutils-gdb.git (master branch) manually. RISC-V QEMU is built from https://github.com/sifive/qemu/tree/v5.2.0-rvv-rvb-zfh . The SIMD code can be generated following the IREE CPU flow with the additional command-line flags tools/iree-compile \\ --iree-hal-target-backends = llvm-cpu \\ --iree-llvm-target-triple = riscv64 \\ --iree-llvm-target-cpu = generic-rv64 \\ --iree-llvm-target-abi = lp64d \\ --iree-llvm-target-cpu-features = \"+m,+a,+f,+d,+v\" \\ --riscv-v-vector-bits-min = 512 --riscv-v-fixed-length-vector-lmul-max = 8 \\ iree_input.mlir -o mobilenet_cpu.vmfb Then run on the RISC-V QEMU: ${ QEMU_BIN } \\ -cpu rv64,x-v = true,x-k = true,vlen = 512 ,elen = 64 ,vext_spec = v1.0 \\ -L ${ RISCV_TOOLCHAIN_ROOT } /sysroot/ \\ ../iree-build-riscv/tools/iree-run-module \\ --device = local-task \\ --module_file = mobilenet_cpu.vmfb \\ --entry_function = predict \\ --function_input = \"1x224x224x3xf32=0\"","title":"RISC-V cross-compilation"},{"location":"building-from-source/riscv/#risc-v-cross-compilation","text":"Running on a platform like RISC-V involves cross-compiling from a host platform (e.g. Linux) to a target platform (a specific RISC-V CPU architecture and operating system): IREE's compiler is built on the host and is used there to generate modules for the target IREE's runtime is built on the host for the target. The runtime is then pushed to the target to run natively.","title":"RISC-V cross-compilation"},{"location":"building-from-source/riscv/#prerequisites","text":"","title":"Prerequisites"},{"location":"building-from-source/riscv/#host-environment-setup","text":"You should already be able to build IREE from source on your host platform. Please make sure you have followed the getting started steps.","title":"Host environment setup"},{"location":"building-from-source/riscv/#install-risc-v-cross-compile-toolchain-and-emulator","text":"You'll need a RISC-V LLVM compilation toolchain and a RISC-V enabled QEMU emulator. See instructions in the following links Clang getting started RISC-V GNU toolchain QEMU RISC-V Linux QEMU Note The RISCV_TOOLCHAIN_ROOT environment variable needs to be set to the root directory of the installed GNU toolchain when building the RISC-V compiler target and the runtime library.","title":"Install RISC-V cross-compile toolchain and emulator"},{"location":"building-from-source/riscv/#install-prebuilt-risc-v-tools-risc-v-64-bit-linux-toolchain","text":"Execute the following script to download the prebuilt RISC-V toolchain and QEMU from the IREE root directory: ./build_tools/riscv/riscv_bootstrap.sh","title":"Install prebuilt RISC-V tools (RISC-V 64-bit Linux toolchain)"},{"location":"building-from-source/riscv/#support-vector-extension","text":"For RISC-V vector extensions support, see additional instructions","title":"Support vector extension"},{"location":"building-from-source/riscv/#configure-and-build","text":"","title":"Configure and build"},{"location":"building-from-source/riscv/#host-configuration","text":"Build and install on your host machine: cmake -GNinja -B ../iree-build/ \\ -DCMAKE_C_COMPILER = clang \\ -DCMAKE_CXX_COMPILER = clang++ \\ -DCMAKE_INSTALL_PREFIX = ../iree-build/install \\ -DCMAKE_BUILD_TYPE = RelWithDebInfo \\ . cmake --build ../iree-build/ --target install","title":"Host configuration"},{"location":"building-from-source/riscv/#target-configuration","text":"The following instruction shows how to build for a RISC-V 64-bit Linux machine. For other RISC-V targets, please refer to riscv.toolchain.cmake as a reference of how to set up the cmake configuration.","title":"Target configuration"},{"location":"building-from-source/riscv/#risc-v-64-bit-linux-target","text":"cmake -GNinja -B ../iree-build-riscv/ \\ -DCMAKE_TOOLCHAIN_FILE = \"./build_tools/cmake/riscv.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = $( realpath ../iree-build/install ) \\ -DRISCV_CPU = rv64 \\ -DIREE_BUILD_COMPILER = OFF \\ -DRISCV_TOOLCHAIN_ROOT = ${ RISCV_TOOLCHAIN_ROOT } \\ . cmake --build ../iree-build-riscv/","title":"RISC-V 64-bit Linux target"},{"location":"building-from-source/riscv/#running-iree-bytecode-modules-on-the-risc-v-system","text":"Note The following instructions are meant for the RISC-V 64-bit Linux target. For the bare-metal target, please refer to simple_embedding to see how to build a ML workload for a bare-metal machine. Set the path to qemu-riscv64 Linux emulator binary in the QEMU_BIN environment variable. If it is installed with riscv_bootstrap.sh , the path is default at ${HOME}/riscv/qemu/linux/RISCV/bin/qemu-riscv64. export QEMU_BIN = <path to qemu-riscv64 binary> Invoke the host compiler tools to produce a bytecode module FlatBuffer: ../iree-build/install/bin/iree-compile \\ --iree-hal-target-backends = vmvx \\ samples/models/simple_abs.mlir \\ -o /tmp/simple_abs_vmvx.vmfb Run the RISC-V emulation: ${ QEMU_BIN } \\ -cpu rv64 \\ -L ${ RISCV_TOOLCHAIN_ROOT } /sysroot/ \\ ../iree-build-riscv/tools/iree-run-module \\ --device = local-task \\ --module_file = /tmp/simple_abs_vmvx.vmfb \\ --entry_function = abs \\ --function_input = f32 = -5","title":"Running IREE bytecode modules on the RISC-V system"},{"location":"building-from-source/riscv/#optional-configuration","text":"RISC-V Vector extensions allows SIMD code to run more efficiently. To enable the vector extension for the compiler toolchain and the emulator, build the tools from the following sources: RISC-V toolchain is built from https://github.com/llvm/llvm-project (main branch). Currently, the LLVM compiler is built on GNU toolchain, including libgcc, GNU linker, and C libraries. You need to build GNU toolchain first. Clone GNU toolchain from: https://github.com/riscv/riscv-gnu-toolchain (master branch). Switch the \"riscv-binutils\" submodule to git://sourceware.org/git/binutils-gdb.git (master branch) manually. RISC-V QEMU is built from https://github.com/sifive/qemu/tree/v5.2.0-rvv-rvb-zfh . The SIMD code can be generated following the IREE CPU flow with the additional command-line flags tools/iree-compile \\ --iree-hal-target-backends = llvm-cpu \\ --iree-llvm-target-triple = riscv64 \\ --iree-llvm-target-cpu = generic-rv64 \\ --iree-llvm-target-abi = lp64d \\ --iree-llvm-target-cpu-features = \"+m,+a,+f,+d,+v\" \\ --riscv-v-vector-bits-min = 512 --riscv-v-fixed-length-vector-lmul-max = 8 \\ iree_input.mlir -o mobilenet_cpu.vmfb Then run on the RISC-V QEMU: ${ QEMU_BIN } \\ -cpu rv64,x-v = true,x-k = true,vlen = 512 ,elen = 64 ,vext_spec = v1.0 \\ -L ${ RISCV_TOOLCHAIN_ROOT } /sysroot/ \\ ../iree-build-riscv/tools/iree-run-module \\ --device = local-task \\ --module_file = mobilenet_cpu.vmfb \\ --entry_function = predict \\ --function_input = \"1x224x224x3xf32=0\"","title":"Optional configuration"},{"location":"community/","text":"Community projects \u00b6 The IREE Bare-Metal Arm Sample demonstrates how build IREE with the Arm GNU Toolchain for bare-metal Arm targets using the open-source firmware libraries CMSIS and libopencm3 . The IREE C++ Template demonstrates how to integrate IREE into a third-party project with CMake. The project demonstrates the usage of runtime support. The IREE LLVM Sandbox contains experimental work by the IREE team closely related to LLVM and MLIR, usually with the aim of contributing back to those upstream projects in some form.","title":"Community projects"},{"location":"community/#community-projects","text":"The IREE Bare-Metal Arm Sample demonstrates how build IREE with the Arm GNU Toolchain for bare-metal Arm targets using the open-source firmware libraries CMSIS and libopencm3 . The IREE C++ Template demonstrates how to integrate IREE into a third-party project with CMake. The project demonstrates the usage of runtime support. The IREE LLVM Sandbox contains experimental work by the IREE team closely related to LLVM and MLIR, usually with the aim of contributing back to those upstream projects in some form.","title":"Community projects"},{"location":"deployment-configurations/","text":"Deployment configurations \u00b6 IREE provides a flexible set of tools for various deployment scenarios. Fully featured environments can use IREE to load programs on demand and to take advantage of multi-threaded hardware, while embedded systems can bypass IREE's runtime entirely or interface with custom accelerators. Stable configurations \u00b6 CPU for general purpose CPU deployment CPU - Bare-Metal with minimal platform dependencies GPU - Vulkan GPU - CUDA/ROCm These are just the most stable configurations IREE supports. Feel free to reach out on any of IREE's communication channels if you have questions about a specific platform, hardware accelerator, or set of system features.","title":"Deployment configurations"},{"location":"deployment-configurations/#deployment-configurations","text":"IREE provides a flexible set of tools for various deployment scenarios. Fully featured environments can use IREE to load programs on demand and to take advantage of multi-threaded hardware, while embedded systems can bypass IREE's runtime entirely or interface with custom accelerators.","title":"Deployment configurations"},{"location":"deployment-configurations/#stable-configurations","text":"CPU for general purpose CPU deployment CPU - Bare-Metal with minimal platform dependencies GPU - Vulkan GPU - CUDA/ROCm These are just the most stable configurations IREE supports. Feel free to reach out on any of IREE's communication channels if you have questions about a specific platform, hardware accelerator, or set of system features.","title":"Stable configurations"},{"location":"deployment-configurations/bare-metal/","text":"Run on a Bare-Metal Platform \u00b6 IREE supports CPU model execution on bare-metal platforms. That is, platforms without operating system support, for which executables are built using machine-specific linker scripts and/or board support packages (BSPs). Bare-metal deployment typically uses IREE's LLVM compiler target much like the CPU configuration , but using a limited subset of IREE's CPU HAL driver code at runtime to load and execute compiled programs. Prerequisites \u00b6 Out-of-tree bare-metal platform tools and source code for the system should be ready, such as Compilation toolchain Platform linker script Firmware libraries Please follow the instructions to retrieve the IREE compiler. Compile the model for bare-metal \u00b6 The model can be compiled with the following command (assuming the path to iree-compile is in your system's PATH ): iree-compile \\ --iree-stream-partitioning-favor = min-peak-memory \\ --iree-hal-target-backends = llvm-cpu \\ --iree-llvm-target-triple = x86_64-pc-linux-elf \\ --iree-llvm-debug-symbols = false \\ samples/models/simple_abs.mlir \\ -o /tmp/simple_abs_cpu.vmfb In which -iree-stream-partitioning-favor=min-peak-memory : Optimize for minimum peak memory usage at the cost of concurrency - include when targeting single-threaded execution to reduce memory consumption. iree-hal-target-backends=llvm-cpu : Compile using the LLVM CPU target iree-llvm-target-triple : Use the <arch>-pc-linux-elf LLVM target triple so the artifact has a fixed ABI to be rendered by the elf_module library iree-llvm-debug-symbols=false : To reduce the artifact size See generate.sh for example command-line instructions of some common architectures You can replace the MLIR file with the other MLIR model files, following the instructions Compiling the bare-metal model for static-library support \u00b6 See the static_library demo sample for an example and instructions on running a model with IREE's static_library_loader . By default, the demo targets the host machine when compiling. To produce a bare-metal compatible model, run iree-compile as in the previous example and add the additional -iree-llvm-static-library-output-path= flag to specify the static library destination. This will produce a .h\\.o file to link directly into the target application. Build bare-metal runtime from the source \u00b6 A few CMake options and macros should be set to build a subset of IREE runtime libraries compatible with the bare-metal platform. We assume there's no multi-thread control nor system library support in the bare-metal system. The model execution is in a single-thread synchronous fashion. Set CMake options \u00b6 set(IREE_BUILD_COMPILER OFF) : Build IREE runtime only set(CMAKE_SYSTEM_NAME Generic) : Tell CMake to skip targeting a specific operating system set(IREE_BINDINGS_TFLITE OFF) : Disable the TFLite binding support set(IREE_ENABLE_THREADING OFF) : Disable multi-thread library support set(IREE_HAL_DRIVER_DEFAULTS OFF) : Disable HAL drivers by default, then enable the synchronous HAL drivers with set(IREE_HAL_DRIVER_LOCAL_SYNC ON) set(IREE_HAL_EXECUTABLE_LOADER_DEFAULTS OFF) : Disable HAL executable loaders by default, then enable the CPU codegen and VMVX loaders with set(IREE_HAL_EXECUTABLE_LOADER_EMBEDDED_ELF ON) and set(IREE_HAL_EXECUTABLE_LOADER_VMVX_MODULE ON) set(IREE_BUILD_TESTS OFF) : Disable tests until IREE supports running them on bare-metal platforms set(IREE_BUILD_SAMPLES ON) : Build simple_embedding example Todo Clean the list up after #6353 is fixed. Also, set the toolchain-specific cmake file to match the tool path, target architecture, target abi, linker script, system library path, etc. Define IREE macros \u00b6 -DIREE_PLATFORM_GENERIC : Let IREE to build the runtime library without targeting a specific platform. -DIREE_SYNCHRONIZATION_DISABLE_UNSAFE=1 : Disable thread synchronization support. Must only be used if there's a single thread. -DIREE_FILE_IO_ENABLE=0 : Disable file I/O. -DIREE_TIME_NOW_FN : A function to return the system time. For the bare-metal system, it can be set as -DIREE_TIME_NOW_FN=\\\"\\{ return 0;\\}\\\" as there's no asynchronous wait handling. -DIREE_WAIT_UNTIL_FN : A function to wait until the given time in nanoseconds. Must match the signature bool(uint64_t nanos) and return false if the wait failed. Examples of how to setup the CMakeLists.txt and .cmake file: IREE RISC-V toolchain cmake IREE Bare-Metal Arm Sample IREE Bare-Metal RV32 Sample Bare-metal execution example \u00b6 See simple_embedding for generic platform to see how to use the IREE runtime library to build/run the IREE model for the bare-metal target.","title":"CPU - Bare-Metal"},{"location":"deployment-configurations/bare-metal/#run-on-a-bare-metal-platform","text":"IREE supports CPU model execution on bare-metal platforms. That is, platforms without operating system support, for which executables are built using machine-specific linker scripts and/or board support packages (BSPs). Bare-metal deployment typically uses IREE's LLVM compiler target much like the CPU configuration , but using a limited subset of IREE's CPU HAL driver code at runtime to load and execute compiled programs.","title":"Run on a Bare-Metal Platform"},{"location":"deployment-configurations/bare-metal/#prerequisites","text":"Out-of-tree bare-metal platform tools and source code for the system should be ready, such as Compilation toolchain Platform linker script Firmware libraries Please follow the instructions to retrieve the IREE compiler.","title":"Prerequisites"},{"location":"deployment-configurations/bare-metal/#compile-the-model-for-bare-metal","text":"The model can be compiled with the following command (assuming the path to iree-compile is in your system's PATH ): iree-compile \\ --iree-stream-partitioning-favor = min-peak-memory \\ --iree-hal-target-backends = llvm-cpu \\ --iree-llvm-target-triple = x86_64-pc-linux-elf \\ --iree-llvm-debug-symbols = false \\ samples/models/simple_abs.mlir \\ -o /tmp/simple_abs_cpu.vmfb In which -iree-stream-partitioning-favor=min-peak-memory : Optimize for minimum peak memory usage at the cost of concurrency - include when targeting single-threaded execution to reduce memory consumption. iree-hal-target-backends=llvm-cpu : Compile using the LLVM CPU target iree-llvm-target-triple : Use the <arch>-pc-linux-elf LLVM target triple so the artifact has a fixed ABI to be rendered by the elf_module library iree-llvm-debug-symbols=false : To reduce the artifact size See generate.sh for example command-line instructions of some common architectures You can replace the MLIR file with the other MLIR model files, following the instructions","title":"Compile the model for bare-metal"},{"location":"deployment-configurations/bare-metal/#compiling-the-bare-metal-model-for-static-library-support","text":"See the static_library demo sample for an example and instructions on running a model with IREE's static_library_loader . By default, the demo targets the host machine when compiling. To produce a bare-metal compatible model, run iree-compile as in the previous example and add the additional -iree-llvm-static-library-output-path= flag to specify the static library destination. This will produce a .h\\.o file to link directly into the target application.","title":"Compiling the bare-metal model for static-library support"},{"location":"deployment-configurations/bare-metal/#build-bare-metal-runtime-from-the-source","text":"A few CMake options and macros should be set to build a subset of IREE runtime libraries compatible with the bare-metal platform. We assume there's no multi-thread control nor system library support in the bare-metal system. The model execution is in a single-thread synchronous fashion.","title":"Build bare-metal runtime from the source"},{"location":"deployment-configurations/bare-metal/#set-cmake-options","text":"set(IREE_BUILD_COMPILER OFF) : Build IREE runtime only set(CMAKE_SYSTEM_NAME Generic) : Tell CMake to skip targeting a specific operating system set(IREE_BINDINGS_TFLITE OFF) : Disable the TFLite binding support set(IREE_ENABLE_THREADING OFF) : Disable multi-thread library support set(IREE_HAL_DRIVER_DEFAULTS OFF) : Disable HAL drivers by default, then enable the synchronous HAL drivers with set(IREE_HAL_DRIVER_LOCAL_SYNC ON) set(IREE_HAL_EXECUTABLE_LOADER_DEFAULTS OFF) : Disable HAL executable loaders by default, then enable the CPU codegen and VMVX loaders with set(IREE_HAL_EXECUTABLE_LOADER_EMBEDDED_ELF ON) and set(IREE_HAL_EXECUTABLE_LOADER_VMVX_MODULE ON) set(IREE_BUILD_TESTS OFF) : Disable tests until IREE supports running them on bare-metal platforms set(IREE_BUILD_SAMPLES ON) : Build simple_embedding example Todo Clean the list up after #6353 is fixed. Also, set the toolchain-specific cmake file to match the tool path, target architecture, target abi, linker script, system library path, etc.","title":"Set CMake options"},{"location":"deployment-configurations/bare-metal/#define-iree-macros","text":"-DIREE_PLATFORM_GENERIC : Let IREE to build the runtime library without targeting a specific platform. -DIREE_SYNCHRONIZATION_DISABLE_UNSAFE=1 : Disable thread synchronization support. Must only be used if there's a single thread. -DIREE_FILE_IO_ENABLE=0 : Disable file I/O. -DIREE_TIME_NOW_FN : A function to return the system time. For the bare-metal system, it can be set as -DIREE_TIME_NOW_FN=\\\"\\{ return 0;\\}\\\" as there's no asynchronous wait handling. -DIREE_WAIT_UNTIL_FN : A function to wait until the given time in nanoseconds. Must match the signature bool(uint64_t nanos) and return false if the wait failed. Examples of how to setup the CMakeLists.txt and .cmake file: IREE RISC-V toolchain cmake IREE Bare-Metal Arm Sample IREE Bare-Metal RV32 Sample","title":"Define IREE macros"},{"location":"deployment-configurations/bare-metal/#bare-metal-execution-example","text":"See simple_embedding for generic platform to see how to use the IREE runtime library to build/run the IREE model for the bare-metal target.","title":"Bare-metal execution example"},{"location":"deployment-configurations/cpu/","text":"CPU Deployment \u00b6 IREE supports efficient program execution on CPU devices by using LLVM to compile all dense computations in each program into highly optimized CPU native instruction streams, which are embedded in one of IREE's deployable formats. To compile a program for CPU execution, pick one of IREE's supported executable formats: Executable Format Description embedded ELF portable, high performance dynamic library system library platform-specific dynamic library (.so, .dll, etc.) VMVX reference target At runtime, CPU executables can be loaded using one of IREE's CPU HAL drivers: local-task : asynchronous, multithreaded driver built on IREE's \"task\" system local-sync : synchronous, single-threaded driver that executes work inline Todo Add IREE's CPU support matrix: what architectures are supported; what architectures are well optimized; etc. Get compiler and runtime \u00b6 Get compiler for CPU native instructions \u00b6 Download as Python package \u00b6 Python packages for various IREE functionalities are regularly published to PyPI . See the Python Bindings page for more details. The core iree-compiler package includes the LLVM-based CPU compiler: python -m pip install iree-compiler Tip iree-compile is installed to your python module installation path. If you pip install with the user mode, it is under ${HOME}/.local/bin , or %APPDATA%Python on Windows. You may want to include the path in your system's PATH environment variable. export PATH = ${ HOME } /.local/bin: ${ PATH } Build compiler from source \u00b6 Please make sure you have followed the Getting started page to build IREE for your host platform and the Android cross-compilation page if you are cross compiling for Android. The LLVM (CPU) compiler backend is compiled in by default on all platforms. Ensure that the IREE_TARGET_BACKEND_LLVM_CPU CMake option is ON when configuring for the host. Tip iree-compile is under iree-build/tools/ directory. You may want to include this path in your system's PATH environment variable. Compile and run the model \u00b6 With the compiler and runtime for local CPU execution, we can now compile a model and run it. Compile the model \u00b6 The IREE compiler transforms a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by the IREE compiler first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then, Compile using the command-line \u00b6 Run the following command (assuming the path to iree-compile is in your system's PATH ): iree-compile \\ --iree-hal-target-backends = llvm-cpu \\ iree_input.mlir -o mobilenet_cpu.vmfb Todo Choose the suitable target triple for the current CPU where iree_input.mlir is the model's initial MLIR representation generated by IREE's TensorFlow importer. Get IREE runtime with local CPU HAL driver \u00b6 You will need to get an IREE runtime that supports the local CPU HAL driver, along with the appropriate executable loaders for your application. Build runtime from source \u00b6 Please make sure you have followed the Getting started page to build IREE for your host platform and the Android cross-compilation page if you are cross compiling for Android. The local CPU HAL drivers are compiled in by default on all platforms. Ensure that the IREE_HAL_DRIVER_LOCAL_TASK and IREE_HAL_EXECUTABLE_LOADER_EMBEDDED_ELF (or other executable loader) CMake options are ON when configuring for the target. Run the model \u00b6 Run using the command-line \u00b6 In the build directory, run the following command: tools/iree-run-module \\ --device = local-task \\ --module_file = mobilenet_cpu.vmfb \\ --entry_function = predict \\ --function_input = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values.","title":"CPU"},{"location":"deployment-configurations/cpu/#cpu-deployment","text":"IREE supports efficient program execution on CPU devices by using LLVM to compile all dense computations in each program into highly optimized CPU native instruction streams, which are embedded in one of IREE's deployable formats. To compile a program for CPU execution, pick one of IREE's supported executable formats: Executable Format Description embedded ELF portable, high performance dynamic library system library platform-specific dynamic library (.so, .dll, etc.) VMVX reference target At runtime, CPU executables can be loaded using one of IREE's CPU HAL drivers: local-task : asynchronous, multithreaded driver built on IREE's \"task\" system local-sync : synchronous, single-threaded driver that executes work inline Todo Add IREE's CPU support matrix: what architectures are supported; what architectures are well optimized; etc.","title":"CPU Deployment"},{"location":"deployment-configurations/cpu/#get-compiler-and-runtime","text":"","title":"Get compiler and runtime"},{"location":"deployment-configurations/cpu/#get-compiler-for-cpu-native-instructions","text":"","title":"Get compiler for CPU native instructions"},{"location":"deployment-configurations/cpu/#download-as-python-package","text":"Python packages for various IREE functionalities are regularly published to PyPI . See the Python Bindings page for more details. The core iree-compiler package includes the LLVM-based CPU compiler: python -m pip install iree-compiler Tip iree-compile is installed to your python module installation path. If you pip install with the user mode, it is under ${HOME}/.local/bin , or %APPDATA%Python on Windows. You may want to include the path in your system's PATH environment variable. export PATH = ${ HOME } /.local/bin: ${ PATH }","title":"Download as Python package"},{"location":"deployment-configurations/cpu/#build-compiler-from-source","text":"Please make sure you have followed the Getting started page to build IREE for your host platform and the Android cross-compilation page if you are cross compiling for Android. The LLVM (CPU) compiler backend is compiled in by default on all platforms. Ensure that the IREE_TARGET_BACKEND_LLVM_CPU CMake option is ON when configuring for the host. Tip iree-compile is under iree-build/tools/ directory. You may want to include this path in your system's PATH environment variable.","title":"Build compiler from source"},{"location":"deployment-configurations/cpu/#compile-and-run-the-model","text":"With the compiler and runtime for local CPU execution, we can now compile a model and run it.","title":"Compile and run the model"},{"location":"deployment-configurations/cpu/#compile-the-model","text":"The IREE compiler transforms a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by the IREE compiler first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then,","title":"Compile the model"},{"location":"deployment-configurations/cpu/#compile-using-the-command-line","text":"Run the following command (assuming the path to iree-compile is in your system's PATH ): iree-compile \\ --iree-hal-target-backends = llvm-cpu \\ iree_input.mlir -o mobilenet_cpu.vmfb Todo Choose the suitable target triple for the current CPU where iree_input.mlir is the model's initial MLIR representation generated by IREE's TensorFlow importer.","title":"Compile using the command-line"},{"location":"deployment-configurations/cpu/#get-iree-runtime-with-local-cpu-hal-driver","text":"You will need to get an IREE runtime that supports the local CPU HAL driver, along with the appropriate executable loaders for your application.","title":"Get IREE runtime with local CPU HAL driver"},{"location":"deployment-configurations/cpu/#build-runtime-from-source","text":"Please make sure you have followed the Getting started page to build IREE for your host platform and the Android cross-compilation page if you are cross compiling for Android. The local CPU HAL drivers are compiled in by default on all platforms. Ensure that the IREE_HAL_DRIVER_LOCAL_TASK and IREE_HAL_EXECUTABLE_LOADER_EMBEDDED_ELF (or other executable loader) CMake options are ON when configuring for the target.","title":"Build runtime from source"},{"location":"deployment-configurations/cpu/#run-the-model","text":"","title":"Run the model"},{"location":"deployment-configurations/cpu/#run-using-the-command-line","text":"In the build directory, run the following command: tools/iree-run-module \\ --device = local-task \\ --module_file = mobilenet_cpu.vmfb \\ --entry_function = predict \\ --function_input = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values.","title":"Run using the command-line"},{"location":"deployment-configurations/gpu-cuda-rocm/","text":"CUDA and ROCm GPU HAL Driver \u00b6 IREE can accelerate model execution on NVIDIA GPUs using CUDA and on AMD GPUs using ROCm. Due to the similarity of CUDA and ROCm APIs and infrastructure, the CUDA and ROCm backends share much of their implementation in IREE: The IREE compiler uses a similar GPU code generation pipeline for each, but generates PTX for CUDA and hsaco for ROCm The IREE runtime HAL driver for ROCm mirrors the one for CUDA, except for command buffers implementations - where CUDA has \"direct\", \"stream\", and \"graph\" command buffers, and ROCm has only \"direct\" command buffers Prerequisites \u00b6 In order to use CUDA or ROCm to drive the GPU, you need to have a functional CUDA or ROCm environment. It can be verified by the following steps: Nvidia/CUDA AMD/ROCm Run the following command in a shell: nvidia-smi | grep CUDA If nvidia-smi does not exist, you will need to install the latest CUDA Toolkit SDK . Run the following command in a shell: rocm-smi | grep rocm If rocm-smi does not exist, you will need to install the latest ROCm Toolkit SDK . Get runtime and compiler \u00b6 Get IREE runtime \u00b6 Next you will need to get an IREE runtime that includes the CUDA (for Nvidia hardware) or ROCm (for AMD hardware) HAL driver. Build runtime from source \u00b6 Please make sure you have followed the Getting started page to build IREE from source, then enable the CUDA HAL driver with the IREE_HAL_DRIVER_CUDA option or the experimental ROCm HAL driver with the IREE_EXTERNAL_HAL_DRIVERS=rocm option. Download compiler as Python package \u00b6 Nvidia/CUDA AMD/ROCm Python packages for various IREE functionalities are regularly published to PyPI . See the Python Bindings page for more details. The core iree-compiler package includes the CUDA compiler: python -m pip install iree-compiler Tip iree-compile is installed to your python module installation path. If you pip install with the user mode, it is under ${HOME}/.local/bin , or %APPDATA%Python on Windows. You may want to include the path in your system's PATH environment variable. export PATH = ${ HOME } /.local/bin: ${ PATH } Currently ROCm is NOT supported for the Python interface. Build compiler from source \u00b6 Please make sure you have followed the Getting started page to build the IREE compiler, then enable the CUDA compiler target with the IREE_TARGET_BACKEND_CUDA option or the ROCm compiler target with the IREE_TARGET_BACKEND_ROCM option. Compile and run the model \u00b6 With the compiler and runtime ready, we can now compile a model and run it on the GPU. Compile the model \u00b6 IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by main IREE compilers first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then, Compile using the command-line \u00b6 Let iree_input.mlir be the model's initial MLIR representation generated by IREE's TensorFlow importer. We can now compile them for each GPU by running the following command: Nvidia/CUDA AMD/ROCm Assuming the path to iree-compiler is in the system's PATH environment variable: iree-compile \\ --iree-hal-target-backends = cuda \\ --iree-hal-cuda-llvm-target-arch = <...> \\ --iree-hal-cuda-disable-loop-nounroll-wa \\ iree_input.mlir -o mobilenet-cuda.vmfb Note that a cuda target architecture( iree-hal-cuda-llvm-target-arch ) of the form sm_<arch_number> is needed to compile towards each GPU architecture. If no architecture is specified then we will default to sm_35 . Here are a table of commonly used architectures: CUDA GPU Target Architecture Nvidia K80 sm_35 Nvidia P100 sm_60 Nvidia V100 sm_70 Nvidia A100 sm_80 In the build directory: tools/iree-compile \\ --iree-hal-target-backends = rocm \\ --iree-rocm-target-chip = <...> \\ --iree-rocm-link-bc = true \\ --iree-rocm-bc-dir = <...> \\ iree_input.mlir -o mobilenet-rocm.vmfb Note ROCm Bitcode Dir( iree-rocm-bc-dir ) path is required. If the system you are compiling IREE in has ROCm installed, then the default value of /opt/rocm/amdgcn/bitcode will usually suffice. If you intend on building ROCm compiler in a non-ROCm capable system, please set iree-rocm-bc-dir to the absolute path where you might have saved the amdgcn bitcode. Note that a ROCm target chip( iree-rocm-target-chip ) of the form gfx<arch_number> is needed to compile towards each GPU architecture. If no architecture is specified then we will default to gfx908 Here are a table of commonly used architecture AMD GPU Target Chip AMD MI25 gfx900 AMD MI50 gfx906 AMD MI60 gfx906 AMD MI100 gfx908 Run the model \u00b6 Run using the command-line \u00b6 In the build directory, run the following command: Nvidia/CUDA AMD/ROCm tools/iree-run-module \\ --device = cuda \\ --module_file = mobilenet-cuda.vmfb \\ --entry_function = predict \\ --function_input = \"1x224x224x3xf32=0\" tools/iree-run-module \\ --device = rocm \\ --module_file = mobilenet-rocm.vmfb \\ --entry_function = predict \\ --function_input = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values.","title":"GPU - CUDA/ROCm"},{"location":"deployment-configurations/gpu-cuda-rocm/#cuda-and-rocm-gpu-hal-driver","text":"IREE can accelerate model execution on NVIDIA GPUs using CUDA and on AMD GPUs using ROCm. Due to the similarity of CUDA and ROCm APIs and infrastructure, the CUDA and ROCm backends share much of their implementation in IREE: The IREE compiler uses a similar GPU code generation pipeline for each, but generates PTX for CUDA and hsaco for ROCm The IREE runtime HAL driver for ROCm mirrors the one for CUDA, except for command buffers implementations - where CUDA has \"direct\", \"stream\", and \"graph\" command buffers, and ROCm has only \"direct\" command buffers","title":"CUDA and ROCm GPU HAL Driver"},{"location":"deployment-configurations/gpu-cuda-rocm/#prerequisites","text":"In order to use CUDA or ROCm to drive the GPU, you need to have a functional CUDA or ROCm environment. It can be verified by the following steps: Nvidia/CUDA AMD/ROCm Run the following command in a shell: nvidia-smi | grep CUDA If nvidia-smi does not exist, you will need to install the latest CUDA Toolkit SDK . Run the following command in a shell: rocm-smi | grep rocm If rocm-smi does not exist, you will need to install the latest ROCm Toolkit SDK .","title":"Prerequisites"},{"location":"deployment-configurations/gpu-cuda-rocm/#get-runtime-and-compiler","text":"","title":"Get runtime and compiler"},{"location":"deployment-configurations/gpu-cuda-rocm/#get-iree-runtime","text":"Next you will need to get an IREE runtime that includes the CUDA (for Nvidia hardware) or ROCm (for AMD hardware) HAL driver.","title":"Get IREE runtime"},{"location":"deployment-configurations/gpu-cuda-rocm/#build-runtime-from-source","text":"Please make sure you have followed the Getting started page to build IREE from source, then enable the CUDA HAL driver with the IREE_HAL_DRIVER_CUDA option or the experimental ROCm HAL driver with the IREE_EXTERNAL_HAL_DRIVERS=rocm option.","title":"Build runtime from source"},{"location":"deployment-configurations/gpu-cuda-rocm/#download-compiler-as-python-package","text":"Nvidia/CUDA AMD/ROCm Python packages for various IREE functionalities are regularly published to PyPI . See the Python Bindings page for more details. The core iree-compiler package includes the CUDA compiler: python -m pip install iree-compiler Tip iree-compile is installed to your python module installation path. If you pip install with the user mode, it is under ${HOME}/.local/bin , or %APPDATA%Python on Windows. You may want to include the path in your system's PATH environment variable. export PATH = ${ HOME } /.local/bin: ${ PATH } Currently ROCm is NOT supported for the Python interface.","title":"Download compiler as Python package"},{"location":"deployment-configurations/gpu-cuda-rocm/#build-compiler-from-source","text":"Please make sure you have followed the Getting started page to build the IREE compiler, then enable the CUDA compiler target with the IREE_TARGET_BACKEND_CUDA option or the ROCm compiler target with the IREE_TARGET_BACKEND_ROCM option.","title":"Build compiler from source"},{"location":"deployment-configurations/gpu-cuda-rocm/#compile-and-run-the-model","text":"With the compiler and runtime ready, we can now compile a model and run it on the GPU.","title":"Compile and run the model"},{"location":"deployment-configurations/gpu-cuda-rocm/#compile-the-model","text":"IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by main IREE compilers first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then,","title":"Compile the model"},{"location":"deployment-configurations/gpu-cuda-rocm/#compile-using-the-command-line","text":"Let iree_input.mlir be the model's initial MLIR representation generated by IREE's TensorFlow importer. We can now compile them for each GPU by running the following command: Nvidia/CUDA AMD/ROCm Assuming the path to iree-compiler is in the system's PATH environment variable: iree-compile \\ --iree-hal-target-backends = cuda \\ --iree-hal-cuda-llvm-target-arch = <...> \\ --iree-hal-cuda-disable-loop-nounroll-wa \\ iree_input.mlir -o mobilenet-cuda.vmfb Note that a cuda target architecture( iree-hal-cuda-llvm-target-arch ) of the form sm_<arch_number> is needed to compile towards each GPU architecture. If no architecture is specified then we will default to sm_35 . Here are a table of commonly used architectures: CUDA GPU Target Architecture Nvidia K80 sm_35 Nvidia P100 sm_60 Nvidia V100 sm_70 Nvidia A100 sm_80 In the build directory: tools/iree-compile \\ --iree-hal-target-backends = rocm \\ --iree-rocm-target-chip = <...> \\ --iree-rocm-link-bc = true \\ --iree-rocm-bc-dir = <...> \\ iree_input.mlir -o mobilenet-rocm.vmfb Note ROCm Bitcode Dir( iree-rocm-bc-dir ) path is required. If the system you are compiling IREE in has ROCm installed, then the default value of /opt/rocm/amdgcn/bitcode will usually suffice. If you intend on building ROCm compiler in a non-ROCm capable system, please set iree-rocm-bc-dir to the absolute path where you might have saved the amdgcn bitcode. Note that a ROCm target chip( iree-rocm-target-chip ) of the form gfx<arch_number> is needed to compile towards each GPU architecture. If no architecture is specified then we will default to gfx908 Here are a table of commonly used architecture AMD GPU Target Chip AMD MI25 gfx900 AMD MI50 gfx906 AMD MI60 gfx906 AMD MI100 gfx908","title":"Compile using the command-line"},{"location":"deployment-configurations/gpu-cuda-rocm/#run-the-model","text":"","title":"Run the model"},{"location":"deployment-configurations/gpu-cuda-rocm/#run-using-the-command-line","text":"In the build directory, run the following command: Nvidia/CUDA AMD/ROCm tools/iree-run-module \\ --device = cuda \\ --module_file = mobilenet-cuda.vmfb \\ --entry_function = predict \\ --function_input = \"1x224x224x3xf32=0\" tools/iree-run-module \\ --device = rocm \\ --module_file = mobilenet-rocm.vmfb \\ --entry_function = predict \\ --function_input = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values.","title":"Run using the command-line"},{"location":"deployment-configurations/gpu-vulkan/","text":"Vulkan GPU HAL Driver \u00b6 IREE can accelerate model execution on GPUs via Vulkan , a low-overhead graphics and compute API. Vulkan is cross-platform: it is available on many operating systems, including Android, Linux, and Windows. Vulkan is also cross-vendor: it is supported by most GPU vendors, including AMD, ARM, Intel, NVIDIA, and Qualcomm. Support matrix \u00b6 As IREE and the compiler ecosystem it operates within matures, more target specific optimizations will be implemented. At this stage, expect reasonable performance across all GPUs and for improvements to be made over time for specific vendors and architectures. GPU Vendor Category Performance Focus Architecture ARM Mali GPU Mobile Good Valhall Qualcomm Adreno GPU Mobile Reasonable 640+ AMD GPU Desktop/server Reasonable - NVIDIA GPU Desktop/server Reasonable - Prerequisites \u00b6 In order to use Vulkan to drive the GPU, you need to have a functional Vulkan environment. IREE requires Vulkan 1.1 on Android and 1.2 elsewhere. It can be verified by the following steps: Android Linux Windows Android mandates Vulkan 1.1 support since Android 10. You just need to make sure the device's Android version is 10 or higher. Run the following command in a shell: vulkaninfo | grep apiVersion If vulkaninfo does not exist, you will need to install the latest Vulkan SDK . For Ubuntu 18.04/20.04, installing via LunarG's package repository is recommended, as it places Vulkan libraries and tools under system paths so it's easy to discover. If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU. Run the following command in a shell: vulkaninfo | grep apiVersion If vulkaninfo does not exist, you will need to install the latest Vulkan SDK . If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU. Get runtime and compiler \u00b6 Get IREE runtime with Vulkan HAL driver \u00b6 Next you will need to get an IREE runtime that supports the Vulkan HAL driver so it can execute the model on GPU via Vulkan. Build runtime from source \u00b6 Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The Vulkan HAL driver is compiled in by default on non-Apple platforms. Ensure that the IREE_HAL_DRIVER_VULKAN CMake option is ON when configuring for the target. Get compiler for SPIR-V exchange format \u00b6 Vulkan expects the program running on GPU to be expressed by the SPIR-V binary exchange format, which the model must be compiled into. Download as Python package \u00b6 Python packages for various IREE functionalities are regularly published to PyPI . See the Python Bindings page for more details. The core iree-compiler package includes the SPIR-V compiler: python -m pip install iree-compiler Tip iree-compile is installed to your python module installation path. If you pip install with the user mode, it is under ${HOME}/.local/bin , or %APPDATA%Python on Windows. You may want to include the path in your system's PATH environment variable. export PATH = ${ HOME } /.local/bin: ${ PATH } Build compiler from source \u00b6 Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The SPIR-V compiler backend is compiled in by default on all platforms. Ensure that the IREE_TARGET_BACKEND_VULKAN_SPIRV CMake option is ON when configuring for the host. Compile and run the model \u00b6 With the compiler for SPIR-V and runtime for Vulkan, we can now compile a model and run it on the GPU. Compile the model \u00b6 IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by main IREE compilers first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then, Compile using the command-line \u00b6 Run the following command (assuming the path to iree-compile is in your system's PATH ): iree-compile \\ --iree-hal-target-backends = vulkan-spirv \\ --iree-vulkan-target-triple = <...> \\ iree_input.mlir -o mobilenet-vulkan.vmfb where iree_input.mlir is the model's initial MLIR representation generated by IREE's TensorFlow importer. Note that a target triple of the form <vendor/arch>-<product>-<os> is needed to compile towards each GPU architecture. If no triple is specified then a safe but more limited default will be used. We don't support the full spectrum here 1 ; the following table summarizes the currently recognized ones: GPU Vendor Target Triple ARM Mali GPU valhall-g78-android11 Qualcomm Adreno GPU adreno-unknown-android11 AMD GPU e.g., rdna1-5700xt-linux NVIDIA GPU e..g, ampere-rtx3080-windows SwiftShader CPU cpu-swiftshader-unknown Run the model \u00b6 Run using the command-line \u00b6 In the build directory, run the following command: tools/iree-run-module \\ --device = vulkan \\ --module_file = mobilenet-vulkan.vmfb \\ --entry_function = predict \\ --function_input = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values. It's also impossible to capture all details of a Vulkan implementation with a target triple, given the allowed variances on extensions, properties, limits, etc. So the target triple is just an approximation for usage. \u21a9","title":"GPU - Vulkan"},{"location":"deployment-configurations/gpu-vulkan/#vulkan-gpu-hal-driver","text":"IREE can accelerate model execution on GPUs via Vulkan , a low-overhead graphics and compute API. Vulkan is cross-platform: it is available on many operating systems, including Android, Linux, and Windows. Vulkan is also cross-vendor: it is supported by most GPU vendors, including AMD, ARM, Intel, NVIDIA, and Qualcomm.","title":"Vulkan GPU HAL Driver"},{"location":"deployment-configurations/gpu-vulkan/#support-matrix","text":"As IREE and the compiler ecosystem it operates within matures, more target specific optimizations will be implemented. At this stage, expect reasonable performance across all GPUs and for improvements to be made over time for specific vendors and architectures. GPU Vendor Category Performance Focus Architecture ARM Mali GPU Mobile Good Valhall Qualcomm Adreno GPU Mobile Reasonable 640+ AMD GPU Desktop/server Reasonable - NVIDIA GPU Desktop/server Reasonable -","title":"Support matrix"},{"location":"deployment-configurations/gpu-vulkan/#prerequisites","text":"In order to use Vulkan to drive the GPU, you need to have a functional Vulkan environment. IREE requires Vulkan 1.1 on Android and 1.2 elsewhere. It can be verified by the following steps: Android Linux Windows Android mandates Vulkan 1.1 support since Android 10. You just need to make sure the device's Android version is 10 or higher. Run the following command in a shell: vulkaninfo | grep apiVersion If vulkaninfo does not exist, you will need to install the latest Vulkan SDK . For Ubuntu 18.04/20.04, installing via LunarG's package repository is recommended, as it places Vulkan libraries and tools under system paths so it's easy to discover. If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU. Run the following command in a shell: vulkaninfo | grep apiVersion If vulkaninfo does not exist, you will need to install the latest Vulkan SDK . If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU.","title":"Prerequisites"},{"location":"deployment-configurations/gpu-vulkan/#get-runtime-and-compiler","text":"","title":"Get runtime and compiler"},{"location":"deployment-configurations/gpu-vulkan/#get-iree-runtime-with-vulkan-hal-driver","text":"Next you will need to get an IREE runtime that supports the Vulkan HAL driver so it can execute the model on GPU via Vulkan.","title":"Get IREE runtime with Vulkan HAL driver"},{"location":"deployment-configurations/gpu-vulkan/#build-runtime-from-source","text":"Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The Vulkan HAL driver is compiled in by default on non-Apple platforms. Ensure that the IREE_HAL_DRIVER_VULKAN CMake option is ON when configuring for the target.","title":"Build runtime from source"},{"location":"deployment-configurations/gpu-vulkan/#get-compiler-for-spir-v-exchange-format","text":"Vulkan expects the program running on GPU to be expressed by the SPIR-V binary exchange format, which the model must be compiled into.","title":"Get compiler for SPIR-V exchange format"},{"location":"deployment-configurations/gpu-vulkan/#download-as-python-package","text":"Python packages for various IREE functionalities are regularly published to PyPI . See the Python Bindings page for more details. The core iree-compiler package includes the SPIR-V compiler: python -m pip install iree-compiler Tip iree-compile is installed to your python module installation path. If you pip install with the user mode, it is under ${HOME}/.local/bin , or %APPDATA%Python on Windows. You may want to include the path in your system's PATH environment variable. export PATH = ${ HOME } /.local/bin: ${ PATH }","title":"Download as Python package"},{"location":"deployment-configurations/gpu-vulkan/#build-compiler-from-source","text":"Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The SPIR-V compiler backend is compiled in by default on all platforms. Ensure that the IREE_TARGET_BACKEND_VULKAN_SPIRV CMake option is ON when configuring for the host.","title":"Build compiler from source"},{"location":"deployment-configurations/gpu-vulkan/#compile-and-run-the-model","text":"With the compiler for SPIR-V and runtime for Vulkan, we can now compile a model and run it on the GPU.","title":"Compile and run the model"},{"location":"deployment-configurations/gpu-vulkan/#compile-the-model","text":"IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by main IREE compilers first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then,","title":"Compile the model"},{"location":"deployment-configurations/gpu-vulkan/#compile-using-the-command-line","text":"Run the following command (assuming the path to iree-compile is in your system's PATH ): iree-compile \\ --iree-hal-target-backends = vulkan-spirv \\ --iree-vulkan-target-triple = <...> \\ iree_input.mlir -o mobilenet-vulkan.vmfb where iree_input.mlir is the model's initial MLIR representation generated by IREE's TensorFlow importer. Note that a target triple of the form <vendor/arch>-<product>-<os> is needed to compile towards each GPU architecture. If no triple is specified then a safe but more limited default will be used. We don't support the full spectrum here 1 ; the following table summarizes the currently recognized ones: GPU Vendor Target Triple ARM Mali GPU valhall-g78-android11 Qualcomm Adreno GPU adreno-unknown-android11 AMD GPU e.g., rdna1-5700xt-linux NVIDIA GPU e..g, ampere-rtx3080-windows SwiftShader CPU cpu-swiftshader-unknown","title":"Compile using the command-line"},{"location":"deployment-configurations/gpu-vulkan/#run-the-model","text":"","title":"Run the model"},{"location":"deployment-configurations/gpu-vulkan/#run-using-the-command-line","text":"In the build directory, run the following command: tools/iree-run-module \\ --device = vulkan \\ --module_file = mobilenet-vulkan.vmfb \\ --entry_function = predict \\ --function_input = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values. It's also impossible to capture all details of a Vulkan implementation with a target triple, given the allowed variances on extensions, properties, limits, etc. So the target triple is just an approximation for usage. \u21a9","title":"Run using the command-line"},{"location":"extensions/","text":"Extension mechanisms \u00b6 Note Much of this describes provisions for extension within IREE but until the core of the system has settled little work will be done to fully flesh-out and document them in detail. A large majority of things that would make someone want to extend IREE can instead be accomplished much easier and performantly using native MLIR dialects that are then processed by the IREE compiler. Guidelines \u00b6 IREE has a compiler and runtime separation, a multi-layered architecture, and split between execution of \"host code\" that schedules compute-heavy work and SPMD \"device code\" that performs the bulk of compute operations. Each axis has a different set of extension mechanisms that can be used independently or combined. Extension philosophy \u00b6 Organized below are some of the mechanisms IREE provides for extending the core compiler and runtime and when they should(n't) be used. The goal of these progressively lower-level extension mechanisms is to make it easier for users to fall into the pit of success : Quote \" a well-designed system makes it easy to do the right things and annoying (but not impossible) to do the wrong things. \" - Jeff Atwood The amount of engineering complexity for initial bring-up and maintenance increases with each subsequently lower-level approach and it is best to start from the top and exit as fast as possible: this is a choose-your-own-adventure where you're trying to escape the dungeon with both the loot and your limbs . Avoid the temptation of immediately dropping down to making external C calls at runtime because that's how it's been done before as it's easier, more robust, and more performant to use the system as it is intended to be used. When to extend \u00b6 The primary goal when extending any framework should first be to avoid extending it at all. There is no mechanism that is free - whether in terms of engineering effort to develop and maintain over time, include in compiler deployments, or include in runtime deployments. As a system scales in deployment configurations the available mechanisms for extension increase but so too does the chaos introduced by extensions that do not also scale with that design. Users are the only ones who can determine the tradeoffs they are willing to accept: for example, the mechanism to extend device code with a custom runtime call to a C function does not work on GPUs and gets significantly more complicated on CPUs as sandboxes/enclaves are used - but if the user scenario is for local process CPU-only execution that may not matter. Where to extend (inputs/compiler/runtime) \u00b6 Consider in normal software development when one would choose to write more code (possibly packaging it into a reusable library) vs. changing the programming language or compiler they are using to compile their code vs. changing the operating systems their code runs on. The further one gets from the problem they are trying to solve the more work, coordination, and maintenance is involved and though there are reasons to make changes across the stack they should be done only when a simpler solution would not suffice. An author will retain more control over their logic the closer they sit to the inputs to the compiler. IREE provides several mechanisms that try to keep control with the author and robust to changes in IREE or MLIR internals and it is strongly encouraged that those looking to extend take those routes first. Contributions that help everyone are very welcome but do have a higher cost and it's often much easier to design and justify upstream changes with working examples in forks or at higher levels of the stack. Where to extend (host/device) \u00b6 From a performance perspective the rule is to colocate code with the data it is acting on: tensor data, for example, should almost exclusively be manipulated by device code as tensors live on device. Attempting to use tensor data with host code will result in synchronization points and host/device transfers that can decimate performance. This can lead to seemingly paradoxical situations where swapping out compiler-generated code for a human-authored \"fast path\" can be slower than even the most naive compiler results. An important thing to keep in mind with compilers is that it is exceedingly difficult to produce code by hand that is consistently more performant across a broad range of deployments and the first temptation should always be to improve the compiler - extending it via other mechanisms when not required by the task is often just premature optimization. 1. Target IREE input dialects \u00b6 TL;DR Convert your custom ops into standard MLIR dialects. +------------+ +--------+ +---------------+ | Your input | -+-> | iree | -+-> | IREE compiler | +------------+ | +--------+ | +---------------+ | +--------+ | +-> | linalg | -+ | +--------+ | | .... | The easiest, cleanest, and most robust path to extend IREE is to make use of what MLIR is designed for: composing dialects and converting between them. IREE supports several input dialects such as tosa , mhlo , linalg , and the standard arith , math , tensor , and scf dialects. Any source IR that can be turned into that mix of dialects (directly or transitively) will work with the whole IREE pipeline for all deployment configurations and targets. If possible to express the computation in this form it will always be the best route to getting small deployments without the need to modify or include any additional code at runtime and run on all device types and execution modes. This mechanism can also be layered with any of the subsequent lower-level ones: if some part of the operation runs on the host and some part on device then decomposing it such that it contains as many standard ops for flow control as possible and linear algebra/custom ops for the dense math will reduce the engineering effort required on both sides and lead to an easier to maintain solution even if lower-level extension is required. A large majority of classic ML \"custom ops\" can be accomplished with this approach. Pros \u00b6 No IREE compiler or runtime code changes required. Can use standard IREE packaged releases and tools. No versioning issues at runtime. IREE's host/device partitioning can partition your code. Fusion and other compiler techniques (CSE/DCE/inlining/etc) work on your code. All target backends (CPU/GPU/accelerators/enclaves/etc) work. Cons \u00b6 Input dialects cannot natively represent all possible programs (such as file IO and other syscalls). Performance-sensitive host code (b-trees and other in-memory databases) will run through the slower VM paths if not authored as dense compute. When to use \u00b6 Targeting multiple MLIR toolchains of which IREE is just one (as little to no IREE-specific code is required). Operation represents host code in addition to device code. All code is known statically or symbolically at compile-time (instead of independently versioned libraries at runtime). Complex high-performance code not representable as linear algebra. External runtime interactions (file/network/user IO). Use custom modules. Implementation \u00b6 To make use of this approach one just needs to follow the standard MLIR dialect conversion behavior: add a dialect with ops, add a conversion pass, and run that pass before providing the resulting IR to the IREE compiler. See Creating a Dialect . Think of this like authoring C++ sources with templates that you compile into your application: Clang (and LLVM beyond) don't know about your library details and instead just process it as it would any other code. You can take the same source and pass it to GCC and it'll be robust to underlying changes in the system. 2. Extend host code with custom modules \u00b6 TL;DR Import MLIR functions in the compiler and custom modules at runtime. // Main user module compiled by IREE: module @model { func.func @predict() { %4 = call @my_custom_module::@some_func(%3) : (tensor<?xf32>) -> i32 ... } } // External module that will be available at runtime: module @my_custom_module { func.func @some_func(%input: tensor<?xf32>) -> i32 // note empty for extern } IREE provides dynamic linking at runtime via its VM interfaces. For code that runs on the host and requires syscalls or calling out to existing libraries - such as file IO, text processing, and JPEG decoding - this is an easy way to interop without paying attention to the more complex details of device code. An IREE module compiled using custom modules is portable and dynamically deployable so long as the custom module is registered at runtime. This approach conceptually matches what normal native binaries do in an OS: imports are declared and at runtime they are resolved based on the available exports of modules in the system. Just as with normal systems engineering design of the API between modules is up to the user and depending on rigor can have several pitfalls but these problems and their solutions are not IREE specific and anyone who has designed a shared library interface can apply the same rules here in IREE around versioning, performance, etc. One does not add 2 integers via a syscall and the same holds here: custom modules and the functions within should perform a large amount of work to hide overheads involved in the cross-module calls and users must be aware that the compiler cannot optimize across the call boundaries. Pros \u00b6 No IREE compiler code changes required. Produced artifacts are portable across IREE deployment configurations. Full system access is allowed - the VM just calls external functions. Runtime modules can be implemented (via shims) in other languages/runtimes. Cons \u00b6 (Today) no asynchronous scheduling or pipelining. Future improvements will enable pipelining of host calls. Custom modules must be registered at runtime by the user. The VM custom module ABI goo must be authored by the user (such as with JNI or pybind to move between java/python and C). All custom module code must be compiled and deployed regardless of how much any modules use. The granularity of modules and their versioning is up to the user. Custom module code cannot be optimized by the IREE compiler to avoid host/device readbacks and unnecessary data type conversion. When to use \u00b6 Interactions with large libraries or system calls. Performance-sensitive host code that cannot easily be represented as device code (like UTF-8 string transformation using libicu). (Today) Pipelining or asynchronous execution are required. Extensively using tensor resources. Implementation \u00b6 The current implementation of this requires conversion goo in the compiler to properly setup the VM interface. Future improvements will make the compiler portion automatic such that in the common case no additional compiler code is required beyond inserting the calls into the external modules which can even happen before passing the input into the IREE compiler. The runtime portion requires that the code be exported to the VM system by way of an iree_vm_module_t interface. A low-level native interface exists with minimal overhead and is used for example by the IREE HAL itself . There is also a C++ wrapper that is significantly easier to work with however it needs some performance improvements. A full end-to-end example can be found under samples/custom_modules/ , though it should not currently be considered representative of best practices. 3. Extend target-specific device conversion patterns \u00b6 TL;DR Add patterns to iree/Compiler/Codegen/ to emit target code. The easiest and most robust path for specializations of device code is to emit such code mixed with the IREE compiler generated code at the highest possible level of abstraction within the target pipeline. For example, if the code can be represented with the vector dialect then inserting conversion patterns between linalg and vector enables the emitted code to be specialized further based on user configuration and optimized with the full set of available passes that run in the pipeline. For each level lower one goes the more flexibility they gain such as being able to emit inline assembly blocks that do anything while trading off generality and multi-targeting applicability. How much the tradeoff matters is based on the behavior of the extension. If a pattern changing a transcendental function to an approximation can operate at the vector level then all IREE deployment targets can benefit from the pattern and as new targets are made available they will automatically receive the benefits. In contrast, a pattern at the vector level that turns generic vector operations into architecture-specific LLVM intrinsics by its nature only pertains to a single target family and can be done at a lower level. As a rule of thumb if a particular pattern is going to need ~N implementations for ~N targets that are all mostly the same it's better to try to move that higher in the stack. At this point the complexity of extending things is still fairly constrained: a C++ pass or pattern is verified with normal lit tests and can be upstreamed easily either into MLIR or IREE (a large number of IREE patterns are upstreamed, benefiting all users of MLIR). Cross-compilation and versioning are not a factor and the IREE artifacts can be considered durable at a coarse level (outside of major target architectural changes). Note that depending on the target there are various mechanisms for representing code in MLIR, up to including inline assembly snippets in IR via llvm.inline_asm . Pros \u00b6 Not limited to what is possible to represent in any particular MLIR dialect. Rich target configuration available; multiple passes can contribute info. Produced executable binaries are hermetic and no runtime changes are required. Specialization can happen in MLIR dialects like linalg or vector as well as target-specific representations like SPIR-V and LLVM IR. The compiler can perform deep optimizations across both the generated code and the provided code (hoisting/loop invariant code motion/cse/etc). Cons \u00b6 Requires implementing the patterns as code in the IREE compiler or via TBD interfaces. When to use \u00b6 Code that must be emitted during target lowering - such as something optimizing for a particular CPU architecture. Hot code mixed with generated code at a fine granularity (within the innermost loop). External existing hand-authored libraries. Either statically or dynamically link instead. Implementation \u00b6 There are several ways to author patterns and passes in MLIR. As examples: A majority of patterns are authored in C++ using PatternRewriter . PDL is an MLIR-based way to express rewrite operations with strong typing, compile-time verification, and easily-readable and less-verbose IR. linalg uses a python-based DSL for defining some of its extended ops. There are many examples within both MLIR and IREE, one specifically being the polynomial approximation expansion patterns . 4. Include external target-specific device code \u00b6 TL;DR Statically link external object files into IREE executables. For large bodies of existing device code or library calls that are available for static linkage the work involved to reimplement them at higher levels of the stack can be cost prohibitive even if it leads to better results. In these cases just as with a normal toolchain one would just want to declare an external function, call it, and add the object file to the linker command line. In IREE the same can be performed by way of taking compatible bitcode or native object files and linking them in with the generated code. An MLIR pattern would declare and emit the call and the target-specific IREE linker would pull in the objects. As the linking behavior varies per target (for example, some targets like SPIR-V don't have traditional linkers) how this is performed is up to the IREE target backends. The complexity involved in producing the object files to link will also vary per-backend and the complexity of the deployment: cross-compiling for multiple architectures or compilation modes (ASAN, etc) will require unique copies of the object files matching that precise configuration. At this point generality is largely out as is the ability to cleanly upstream such files. It should be apparent how a few dozen lines of C++ or PDL that avoids the need for any of this complexity is more appealing. In extremely specific cases of a single platform/architecture/version for a single program deployed via a specific artifact composition it's not so bad but IREE is designed such that extreme specificity is an optional mode of the more general solution. This does not mean this mechanism is not useful in some situations and only that it should be a last-resort when one of the easier to manage solutions is not viable - not a shortcut to avoid writing some C++ patterns. Pros \u00b6 Works with hand-authored code in compatible object files from any toolchain. No IREE runtime changes required. All deployment modes still work, including multi-targeting. No versioning concerns as custom code is included in artifacts. Cons \u00b6 Users must provide per-target precompiled object files on disk. IREE compiler changes are still needed for generating the external calls. Though LTO may be able to optimize across the calls it is not guaranteed. When to use \u00b6 Existing math libraries or architecture-specific functions that cannot be ported into a more MLIR-friendly form. Mixing in hand-authored code written in C/rust/etc with generated code from MLIR. External code can be represented as either linalg , vector , or LLVM IR. Use target-specific conversion patterns instead. External code size is large and unlikely to benefit from link-time optimizations (such as something like libjpeg). Dynamically link instead. Implementation \u00b6 As the linking behavior varies per target backend there is no general solution at this level: if targeting the CPU then the system native linker or lld need to be provided the object files, while SPIR-V will need to merge the SPIR-V binaries directly, and Metal shader libraries will need to be constructed with the Apple-specific metallib tooling. Producing these files and performing the linking is outside the scope of IREE. If the files can be acquired then compiler changes will be required to emit calls to them and invoke the linker with the the files. On the CPU an alternative is to use the static library output mode where IREE produces an object file and then the user invokes the linker themselves; this still requires the compiler changes to emit the calls but avoids needing to teach the compiler how to link the files. 5. Dynamically link target-specific device code (CPU only) \u00b6 TL;DR Dynamically link external C functions at runtime from device code. It is pitch black. You are likely to be eaten by a grue. This is the lowest-level integration in the system and is designed to act as an escape hatch and - as with any emergency escape hatch - it's not designed for ergonomics. Users should try first to come in through the door and attempting to use this mechanism should trigger alarms about the approach being attempted. IREE's execution model for device code and native machine binary deployment mechanisms are designed with several constraints in order to make all of the above approaches possible and performant. Calling arbitrary C functions from deep within the system can introduce subtle (and not-so-subtle) bugs that are extremely difficult to track down and versioning between the compiler emitting the calls and the runtime providing the implementations can cause skew unless held carefully. Consider the methods added here like syscalls in that they must be extremely focused and if they are ever likely to change (including being removed) then care will be needed just as with versioning or redirecting a syscall. Designing good stable interfaces is hard and a classic pit of failure. Some things to note: Device code executes in a tiled fashion and single dispatches may invoke the same function many times from many threads concurrently to perform the larger work. Tiles may execute in any order and on any thread; performing fine-grained locking within the tile can lead to deadlocks. Device code is stateless in order to allow for access restrictions and caching across multiple loaded models - any library state required must be externally managed via process globals. Device code may be running out-of-process (sandbox/enclave) and the library functions must be available where the dispatches run and not where they are launched (such as being linked into the sandbox binary, if separate from the main process binary). The stack must be used to pass arguments/results to external calls via a single pointer and there is no libffi-like functionality for magically calling arbitrary C functions. Users must provide the shims they need. Thread-local storage is unavailable in the called code (it may be usable, but it is not guaranteed it'll work on all platforms and leaks are likely). No heap allocator is provided and the use of libc malloc is unsupported. Most of the constraints here come from the SPMD parallelism model, platform-agnostic deployment format, and overall data-oriented design of IREE. Code operating in this fashion has a certain shape and that is usually not the same as big legacy single-threaded CPU-focused BLAS libraries that perform their own caching, internal thread and state management, and other shenanigans. IREE is not designed to wrap such things and if any of these notes are issues it is more an indicator that the approach needs adjustment than anything else. Trying to bypass or workaround the constraints is possible - after all IREE is an open source project and any user is welcome to fork it - but unsupported by the core IREE team. Pros \u00b6 Function resolution at runtime is orthogonal to compiler target specification. Machine code can be shared between the application and IREE artifacts. Cons \u00b6 IREE compiler and runtime must both be modified. Deeper integration with the IREE codegen compiler infrastructure required. ABI versioning complexity between compiler and runtime. Runtimes must ship the imports for the lifetime of any artifact compiled to use them. Humans are bad at predicting the future. Using the same artifact in different binaries at runtime requires changes to each binary - including those that may not be owned by the person producing the artifact. Weak imports and conditional usage can help but still leads to bloat. When to use \u00b6 Calling into opaque closed-source BLAS-like microkernel libraries. Any other cases covered above can be used, especially microkernels that can be represented in MLIR or as statically linked libraries. Implementation \u00b6 The compiler is changed to produce calls to imports via a dynamic import table provided to each dispatch function. The import table is declared in the executable library for use at runtime. Runtime applications register an import provider to resolve named symbols in the import table to C functions that marshal arguments and results. The compiler-side needs some additional work but an example is included here: Issue 7504 . The runtime-side is complete and resolution is performed by a user-supplied iree_hal_executable_import_provider_t .","title":"Extension mechanisms"},{"location":"extensions/#extension-mechanisms","text":"Note Much of this describes provisions for extension within IREE but until the core of the system has settled little work will be done to fully flesh-out and document them in detail. A large majority of things that would make someone want to extend IREE can instead be accomplished much easier and performantly using native MLIR dialects that are then processed by the IREE compiler.","title":"Extension mechanisms"},{"location":"extensions/#guidelines","text":"IREE has a compiler and runtime separation, a multi-layered architecture, and split between execution of \"host code\" that schedules compute-heavy work and SPMD \"device code\" that performs the bulk of compute operations. Each axis has a different set of extension mechanisms that can be used independently or combined.","title":"Guidelines"},{"location":"extensions/#extension-philosophy","text":"Organized below are some of the mechanisms IREE provides for extending the core compiler and runtime and when they should(n't) be used. The goal of these progressively lower-level extension mechanisms is to make it easier for users to fall into the pit of success : Quote \" a well-designed system makes it easy to do the right things and annoying (but not impossible) to do the wrong things. \" - Jeff Atwood The amount of engineering complexity for initial bring-up and maintenance increases with each subsequently lower-level approach and it is best to start from the top and exit as fast as possible: this is a choose-your-own-adventure where you're trying to escape the dungeon with both the loot and your limbs . Avoid the temptation of immediately dropping down to making external C calls at runtime because that's how it's been done before as it's easier, more robust, and more performant to use the system as it is intended to be used.","title":"Extension philosophy"},{"location":"extensions/#when-to-extend","text":"The primary goal when extending any framework should first be to avoid extending it at all. There is no mechanism that is free - whether in terms of engineering effort to develop and maintain over time, include in compiler deployments, or include in runtime deployments. As a system scales in deployment configurations the available mechanisms for extension increase but so too does the chaos introduced by extensions that do not also scale with that design. Users are the only ones who can determine the tradeoffs they are willing to accept: for example, the mechanism to extend device code with a custom runtime call to a C function does not work on GPUs and gets significantly more complicated on CPUs as sandboxes/enclaves are used - but if the user scenario is for local process CPU-only execution that may not matter.","title":"When to extend"},{"location":"extensions/#where-to-extend-inputscompilerruntime","text":"Consider in normal software development when one would choose to write more code (possibly packaging it into a reusable library) vs. changing the programming language or compiler they are using to compile their code vs. changing the operating systems their code runs on. The further one gets from the problem they are trying to solve the more work, coordination, and maintenance is involved and though there are reasons to make changes across the stack they should be done only when a simpler solution would not suffice. An author will retain more control over their logic the closer they sit to the inputs to the compiler. IREE provides several mechanisms that try to keep control with the author and robust to changes in IREE or MLIR internals and it is strongly encouraged that those looking to extend take those routes first. Contributions that help everyone are very welcome but do have a higher cost and it's often much easier to design and justify upstream changes with working examples in forks or at higher levels of the stack.","title":"Where to extend (inputs/compiler/runtime)"},{"location":"extensions/#where-to-extend-hostdevice","text":"From a performance perspective the rule is to colocate code with the data it is acting on: tensor data, for example, should almost exclusively be manipulated by device code as tensors live on device. Attempting to use tensor data with host code will result in synchronization points and host/device transfers that can decimate performance. This can lead to seemingly paradoxical situations where swapping out compiler-generated code for a human-authored \"fast path\" can be slower than even the most naive compiler results. An important thing to keep in mind with compilers is that it is exceedingly difficult to produce code by hand that is consistently more performant across a broad range of deployments and the first temptation should always be to improve the compiler - extending it via other mechanisms when not required by the task is often just premature optimization.","title":"Where to extend (host/device)"},{"location":"extensions/#1-target-iree-input-dialects","text":"TL;DR Convert your custom ops into standard MLIR dialects. +------------+ +--------+ +---------------+ | Your input | -+-> | iree | -+-> | IREE compiler | +------------+ | +--------+ | +---------------+ | +--------+ | +-> | linalg | -+ | +--------+ | | .... | The easiest, cleanest, and most robust path to extend IREE is to make use of what MLIR is designed for: composing dialects and converting between them. IREE supports several input dialects such as tosa , mhlo , linalg , and the standard arith , math , tensor , and scf dialects. Any source IR that can be turned into that mix of dialects (directly or transitively) will work with the whole IREE pipeline for all deployment configurations and targets. If possible to express the computation in this form it will always be the best route to getting small deployments without the need to modify or include any additional code at runtime and run on all device types and execution modes. This mechanism can also be layered with any of the subsequent lower-level ones: if some part of the operation runs on the host and some part on device then decomposing it such that it contains as many standard ops for flow control as possible and linear algebra/custom ops for the dense math will reduce the engineering effort required on both sides and lead to an easier to maintain solution even if lower-level extension is required. A large majority of classic ML \"custom ops\" can be accomplished with this approach.","title":"1. Target IREE input dialects"},{"location":"extensions/#pros","text":"No IREE compiler or runtime code changes required. Can use standard IREE packaged releases and tools. No versioning issues at runtime. IREE's host/device partitioning can partition your code. Fusion and other compiler techniques (CSE/DCE/inlining/etc) work on your code. All target backends (CPU/GPU/accelerators/enclaves/etc) work.","title":"Pros"},{"location":"extensions/#cons","text":"Input dialects cannot natively represent all possible programs (such as file IO and other syscalls). Performance-sensitive host code (b-trees and other in-memory databases) will run through the slower VM paths if not authored as dense compute.","title":"Cons"},{"location":"extensions/#when-to-use","text":"Targeting multiple MLIR toolchains of which IREE is just one (as little to no IREE-specific code is required). Operation represents host code in addition to device code. All code is known statically or symbolically at compile-time (instead of independently versioned libraries at runtime). Complex high-performance code not representable as linear algebra. External runtime interactions (file/network/user IO). Use custom modules.","title":"When to use"},{"location":"extensions/#implementation","text":"To make use of this approach one just needs to follow the standard MLIR dialect conversion behavior: add a dialect with ops, add a conversion pass, and run that pass before providing the resulting IR to the IREE compiler. See Creating a Dialect . Think of this like authoring C++ sources with templates that you compile into your application: Clang (and LLVM beyond) don't know about your library details and instead just process it as it would any other code. You can take the same source and pass it to GCC and it'll be robust to underlying changes in the system.","title":"Implementation"},{"location":"extensions/#2-extend-host-code-with-custom-modules","text":"TL;DR Import MLIR functions in the compiler and custom modules at runtime. // Main user module compiled by IREE: module @model { func.func @predict() { %4 = call @my_custom_module::@some_func(%3) : (tensor<?xf32>) -> i32 ... } } // External module that will be available at runtime: module @my_custom_module { func.func @some_func(%input: tensor<?xf32>) -> i32 // note empty for extern } IREE provides dynamic linking at runtime via its VM interfaces. For code that runs on the host and requires syscalls or calling out to existing libraries - such as file IO, text processing, and JPEG decoding - this is an easy way to interop without paying attention to the more complex details of device code. An IREE module compiled using custom modules is portable and dynamically deployable so long as the custom module is registered at runtime. This approach conceptually matches what normal native binaries do in an OS: imports are declared and at runtime they are resolved based on the available exports of modules in the system. Just as with normal systems engineering design of the API between modules is up to the user and depending on rigor can have several pitfalls but these problems and their solutions are not IREE specific and anyone who has designed a shared library interface can apply the same rules here in IREE around versioning, performance, etc. One does not add 2 integers via a syscall and the same holds here: custom modules and the functions within should perform a large amount of work to hide overheads involved in the cross-module calls and users must be aware that the compiler cannot optimize across the call boundaries.","title":"2. Extend host code with custom modules"},{"location":"extensions/#pros_1","text":"No IREE compiler code changes required. Produced artifacts are portable across IREE deployment configurations. Full system access is allowed - the VM just calls external functions. Runtime modules can be implemented (via shims) in other languages/runtimes.","title":"Pros"},{"location":"extensions/#cons_1","text":"(Today) no asynchronous scheduling or pipelining. Future improvements will enable pipelining of host calls. Custom modules must be registered at runtime by the user. The VM custom module ABI goo must be authored by the user (such as with JNI or pybind to move between java/python and C). All custom module code must be compiled and deployed regardless of how much any modules use. The granularity of modules and their versioning is up to the user. Custom module code cannot be optimized by the IREE compiler to avoid host/device readbacks and unnecessary data type conversion.","title":"Cons"},{"location":"extensions/#when-to-use_1","text":"Interactions with large libraries or system calls. Performance-sensitive host code that cannot easily be represented as device code (like UTF-8 string transformation using libicu). (Today) Pipelining or asynchronous execution are required. Extensively using tensor resources.","title":"When to use"},{"location":"extensions/#implementation_1","text":"The current implementation of this requires conversion goo in the compiler to properly setup the VM interface. Future improvements will make the compiler portion automatic such that in the common case no additional compiler code is required beyond inserting the calls into the external modules which can even happen before passing the input into the IREE compiler. The runtime portion requires that the code be exported to the VM system by way of an iree_vm_module_t interface. A low-level native interface exists with minimal overhead and is used for example by the IREE HAL itself . There is also a C++ wrapper that is significantly easier to work with however it needs some performance improvements. A full end-to-end example can be found under samples/custom_modules/ , though it should not currently be considered representative of best practices.","title":"Implementation"},{"location":"extensions/#3-extend-target-specific-device-conversion-patterns","text":"TL;DR Add patterns to iree/Compiler/Codegen/ to emit target code. The easiest and most robust path for specializations of device code is to emit such code mixed with the IREE compiler generated code at the highest possible level of abstraction within the target pipeline. For example, if the code can be represented with the vector dialect then inserting conversion patterns between linalg and vector enables the emitted code to be specialized further based on user configuration and optimized with the full set of available passes that run in the pipeline. For each level lower one goes the more flexibility they gain such as being able to emit inline assembly blocks that do anything while trading off generality and multi-targeting applicability. How much the tradeoff matters is based on the behavior of the extension. If a pattern changing a transcendental function to an approximation can operate at the vector level then all IREE deployment targets can benefit from the pattern and as new targets are made available they will automatically receive the benefits. In contrast, a pattern at the vector level that turns generic vector operations into architecture-specific LLVM intrinsics by its nature only pertains to a single target family and can be done at a lower level. As a rule of thumb if a particular pattern is going to need ~N implementations for ~N targets that are all mostly the same it's better to try to move that higher in the stack. At this point the complexity of extending things is still fairly constrained: a C++ pass or pattern is verified with normal lit tests and can be upstreamed easily either into MLIR or IREE (a large number of IREE patterns are upstreamed, benefiting all users of MLIR). Cross-compilation and versioning are not a factor and the IREE artifacts can be considered durable at a coarse level (outside of major target architectural changes). Note that depending on the target there are various mechanisms for representing code in MLIR, up to including inline assembly snippets in IR via llvm.inline_asm .","title":"3. Extend target-specific device conversion patterns"},{"location":"extensions/#pros_2","text":"Not limited to what is possible to represent in any particular MLIR dialect. Rich target configuration available; multiple passes can contribute info. Produced executable binaries are hermetic and no runtime changes are required. Specialization can happen in MLIR dialects like linalg or vector as well as target-specific representations like SPIR-V and LLVM IR. The compiler can perform deep optimizations across both the generated code and the provided code (hoisting/loop invariant code motion/cse/etc).","title":"Pros"},{"location":"extensions/#cons_2","text":"Requires implementing the patterns as code in the IREE compiler or via TBD interfaces.","title":"Cons"},{"location":"extensions/#when-to-use_2","text":"Code that must be emitted during target lowering - such as something optimizing for a particular CPU architecture. Hot code mixed with generated code at a fine granularity (within the innermost loop). External existing hand-authored libraries. Either statically or dynamically link instead.","title":"When to use"},{"location":"extensions/#implementation_2","text":"There are several ways to author patterns and passes in MLIR. As examples: A majority of patterns are authored in C++ using PatternRewriter . PDL is an MLIR-based way to express rewrite operations with strong typing, compile-time verification, and easily-readable and less-verbose IR. linalg uses a python-based DSL for defining some of its extended ops. There are many examples within both MLIR and IREE, one specifically being the polynomial approximation expansion patterns .","title":"Implementation"},{"location":"extensions/#4-include-external-target-specific-device-code","text":"TL;DR Statically link external object files into IREE executables. For large bodies of existing device code or library calls that are available for static linkage the work involved to reimplement them at higher levels of the stack can be cost prohibitive even if it leads to better results. In these cases just as with a normal toolchain one would just want to declare an external function, call it, and add the object file to the linker command line. In IREE the same can be performed by way of taking compatible bitcode or native object files and linking them in with the generated code. An MLIR pattern would declare and emit the call and the target-specific IREE linker would pull in the objects. As the linking behavior varies per target (for example, some targets like SPIR-V don't have traditional linkers) how this is performed is up to the IREE target backends. The complexity involved in producing the object files to link will also vary per-backend and the complexity of the deployment: cross-compiling for multiple architectures or compilation modes (ASAN, etc) will require unique copies of the object files matching that precise configuration. At this point generality is largely out as is the ability to cleanly upstream such files. It should be apparent how a few dozen lines of C++ or PDL that avoids the need for any of this complexity is more appealing. In extremely specific cases of a single platform/architecture/version for a single program deployed via a specific artifact composition it's not so bad but IREE is designed such that extreme specificity is an optional mode of the more general solution. This does not mean this mechanism is not useful in some situations and only that it should be a last-resort when one of the easier to manage solutions is not viable - not a shortcut to avoid writing some C++ patterns.","title":"4. Include external target-specific device code"},{"location":"extensions/#pros_3","text":"Works with hand-authored code in compatible object files from any toolchain. No IREE runtime changes required. All deployment modes still work, including multi-targeting. No versioning concerns as custom code is included in artifacts.","title":"Pros"},{"location":"extensions/#cons_3","text":"Users must provide per-target precompiled object files on disk. IREE compiler changes are still needed for generating the external calls. Though LTO may be able to optimize across the calls it is not guaranteed.","title":"Cons"},{"location":"extensions/#when-to-use_3","text":"Existing math libraries or architecture-specific functions that cannot be ported into a more MLIR-friendly form. Mixing in hand-authored code written in C/rust/etc with generated code from MLIR. External code can be represented as either linalg , vector , or LLVM IR. Use target-specific conversion patterns instead. External code size is large and unlikely to benefit from link-time optimizations (such as something like libjpeg). Dynamically link instead.","title":"When to use"},{"location":"extensions/#implementation_3","text":"As the linking behavior varies per target backend there is no general solution at this level: if targeting the CPU then the system native linker or lld need to be provided the object files, while SPIR-V will need to merge the SPIR-V binaries directly, and Metal shader libraries will need to be constructed with the Apple-specific metallib tooling. Producing these files and performing the linking is outside the scope of IREE. If the files can be acquired then compiler changes will be required to emit calls to them and invoke the linker with the the files. On the CPU an alternative is to use the static library output mode where IREE produces an object file and then the user invokes the linker themselves; this still requires the compiler changes to emit the calls but avoids needing to teach the compiler how to link the files.","title":"Implementation"},{"location":"extensions/#5-dynamically-link-target-specific-device-code-cpu-only","text":"TL;DR Dynamically link external C functions at runtime from device code. It is pitch black. You are likely to be eaten by a grue. This is the lowest-level integration in the system and is designed to act as an escape hatch and - as with any emergency escape hatch - it's not designed for ergonomics. Users should try first to come in through the door and attempting to use this mechanism should trigger alarms about the approach being attempted. IREE's execution model for device code and native machine binary deployment mechanisms are designed with several constraints in order to make all of the above approaches possible and performant. Calling arbitrary C functions from deep within the system can introduce subtle (and not-so-subtle) bugs that are extremely difficult to track down and versioning between the compiler emitting the calls and the runtime providing the implementations can cause skew unless held carefully. Consider the methods added here like syscalls in that they must be extremely focused and if they are ever likely to change (including being removed) then care will be needed just as with versioning or redirecting a syscall. Designing good stable interfaces is hard and a classic pit of failure. Some things to note: Device code executes in a tiled fashion and single dispatches may invoke the same function many times from many threads concurrently to perform the larger work. Tiles may execute in any order and on any thread; performing fine-grained locking within the tile can lead to deadlocks. Device code is stateless in order to allow for access restrictions and caching across multiple loaded models - any library state required must be externally managed via process globals. Device code may be running out-of-process (sandbox/enclave) and the library functions must be available where the dispatches run and not where they are launched (such as being linked into the sandbox binary, if separate from the main process binary). The stack must be used to pass arguments/results to external calls via a single pointer and there is no libffi-like functionality for magically calling arbitrary C functions. Users must provide the shims they need. Thread-local storage is unavailable in the called code (it may be usable, but it is not guaranteed it'll work on all platforms and leaks are likely). No heap allocator is provided and the use of libc malloc is unsupported. Most of the constraints here come from the SPMD parallelism model, platform-agnostic deployment format, and overall data-oriented design of IREE. Code operating in this fashion has a certain shape and that is usually not the same as big legacy single-threaded CPU-focused BLAS libraries that perform their own caching, internal thread and state management, and other shenanigans. IREE is not designed to wrap such things and if any of these notes are issues it is more an indicator that the approach needs adjustment than anything else. Trying to bypass or workaround the constraints is possible - after all IREE is an open source project and any user is welcome to fork it - but unsupported by the core IREE team.","title":"5. Dynamically link target-specific device code (CPU only)"},{"location":"extensions/#pros_4","text":"Function resolution at runtime is orthogonal to compiler target specification. Machine code can be shared between the application and IREE artifacts.","title":"Pros"},{"location":"extensions/#cons_4","text":"IREE compiler and runtime must both be modified. Deeper integration with the IREE codegen compiler infrastructure required. ABI versioning complexity between compiler and runtime. Runtimes must ship the imports for the lifetime of any artifact compiled to use them. Humans are bad at predicting the future. Using the same artifact in different binaries at runtime requires changes to each binary - including those that may not be owned by the person producing the artifact. Weak imports and conditional usage can help but still leads to bloat.","title":"Cons"},{"location":"extensions/#when-to-use_4","text":"Calling into opaque closed-source BLAS-like microkernel libraries. Any other cases covered above can be used, especially microkernels that can be represented in MLIR or as statically linked libraries.","title":"When to use"},{"location":"extensions/#implementation_4","text":"The compiler is changed to produce calls to imports via a dynamic import table provided to each dispatch function. The import table is declared in the executable library for use at runtime. Runtime applications register an import provider to resolve named symbols in the import table to C functions that marshal arguments and results. The compiler-side needs some additional work but an example is included here: Issue 7504 . The runtime-side is complete and resolution is performed by a user-supplied iree_hal_executable_import_provider_t .","title":"Implementation"},{"location":"getting-started/","text":"Getting Started Guide \u00b6 Setup \u00b6 Use the following command for the default installation, or check out the comprehensive installation guide if your needs are more complex. python -m pip install \\ iree-compiler \\ iree-runtime \\ iree-tools-tf \\ iree-tools-tflite \\ iree-tools-xla Supported frameworks \u00b6 See end-to-end examples of how to execute a variety models on IREE. This covers the import, compilation, and execution of the provided model. TensorFlow TensorFlow Lite JAX Importing from PyTorch and other frameworks is planned - stay tuned! Samples \u00b6 Check out the samples in IREE's samples/colab/ directory , as well as the iree-samples repository , which contains workflow comparisons across frameworks. Import \u00b6 Importing models takes known file types and imports into a form that the core IREE compiler is able to ingest. This import process is specific to each frontend and typically involves a number of stages: Load the source format Legalize operations specific each specific frontend to legal IR Validate only IREE compatible operations remain Write the remaining IR to a file This fully legalized form can then be compiled without dependencies on the source model language. Compilation \u00b6 During compilation we load an MLIR file and compile for the specified set of backends (CPU, GPU, etc). Each of these backends creates custom native code to execute on the target device. Once compiled, the resulting bytecode is exported to an IREE bytecode file that can be executed on the specified devices. Execution \u00b6 The final stage is executing the now compiled module. This involves selecting what compute devices should be used, loading the module, and executing the module with the intended inputs. For testing, IREE includes a Python API. However, on mobile and embedded devices you will want to use the C API .","title":"Getting Started Guide"},{"location":"getting-started/#getting-started-guide","text":"","title":"Getting Started Guide"},{"location":"getting-started/#setup","text":"Use the following command for the default installation, or check out the comprehensive installation guide if your needs are more complex. python -m pip install \\ iree-compiler \\ iree-runtime \\ iree-tools-tf \\ iree-tools-tflite \\ iree-tools-xla","title":"Setup"},{"location":"getting-started/#supported-frameworks","text":"See end-to-end examples of how to execute a variety models on IREE. This covers the import, compilation, and execution of the provided model. TensorFlow TensorFlow Lite JAX Importing from PyTorch and other frameworks is planned - stay tuned!","title":"Supported frameworks"},{"location":"getting-started/#samples","text":"Check out the samples in IREE's samples/colab/ directory , as well as the iree-samples repository , which contains workflow comparisons across frameworks.","title":"Samples"},{"location":"getting-started/#import","text":"Importing models takes known file types and imports into a form that the core IREE compiler is able to ingest. This import process is specific to each frontend and typically involves a number of stages: Load the source format Legalize operations specific each specific frontend to legal IR Validate only IREE compatible operations remain Write the remaining IR to a file This fully legalized form can then be compiled without dependencies on the source model language.","title":"Import"},{"location":"getting-started/#compilation","text":"During compilation we load an MLIR file and compile for the specified set of backends (CPU, GPU, etc). Each of these backends creates custom native code to execute on the target device. Once compiled, the resulting bytecode is exported to an IREE bytecode file that can be executed on the specified devices.","title":"Compilation"},{"location":"getting-started/#execution","text":"The final stage is executing the now compiled module. This involves selecting what compute devices should be used, loading the module, and executing the module with the intended inputs. For testing, IREE includes a Python API. However, on mobile and embedded devices you will want to use the C API .","title":"Execution"},{"location":"getting-started/jax/","text":"JAX Integration \u00b6 Todo Issue#5454 : write this documentation","title":"JAX"},{"location":"getting-started/jax/#jax-integration","text":"Todo Issue#5454 : write this documentation","title":"JAX Integration"},{"location":"getting-started/tensorflow/","text":"TensorFlow Integration \u00b6 IREE supports compiling and running TensorFlow programs represented as tf.Module classes or stored in the SavedModel format . Prerequisites \u00b6 Install TensorFlow by following the official documentation : python -m pip install tf-nightly Install IREE pip packages, either from pip or by building from source : python -m pip install \\ iree-compiler \\ iree-runtime \\ iree-tools-tf Warning The TensorFlow package is currently only available on Linux and macOS. It is not available on Windows yet (see this issue ). Importing models \u00b6 IREE compilers transform a model into its final deployable format in several sequential steps. The first step for a TensorFlow model is to use either the iree-import-tf command-line tool or IREE's Python APIs to import the model into a format (i.e., MLIR ) compatible with the generic IREE compilers. From SavedModel on TensorFlow Hub \u00b6 IREE supports importing and using SavedModels from TensorFlow Hub . Using the command-line tool \u00b6 First download the SavedModel and load it to get the serving signature, which is used as the entry point for IREE compilation flow: import tensorflow.compat.v2 as tf loaded_model = tf . saved_model . load ( '/path/to/downloaded/model/' ) print ( list ( loaded_model . signatures . keys ())) Note If there are no serving signatures in the original SavedModel, you may add them by yourself by following \"Missing serving signature in SavedModel\" . Then you can import the model with iree-import-tf . You can read the options supported via iree-import-tf -help . Using MobileNet v2 as an example and assuming the serving signature is predict : iree-import-tf --tf-import-type = savedmodel_v1 \\ --tf-savedmodel-exported-names = predict \\ /path/to/savedmodel -o iree_input.mlir Tip iree-import-tf is installed as /path/to/python/site-packages/iree/tools/tf/iree-import-tf . You can find out the full path to the site-packages directory via the python -m site command. -tf-import-type needs to match the SavedModel version. You can try both v1 and v2 if you see one of them gives an empty dump. Afterwards you can further compile the model in iree_input.mlir for CPU or GPU . Training \u00b6 Todo Discuss training Samples \u00b6 Colab notebooks Training an MNIST digits classifier Edge detection module Pretrained ResNet50 inference TensorFlow Hub Import End-to-end execution tests can be found in IREE's integrations/tensorflow/e2e/ directory. Troubleshooting \u00b6 Missing serving signature in SavedModel \u00b6 Sometimes SavedModels are exported without explicit serving signatures . This happens by default for TensorFlow Hub SavedModels. However, serving signatures are required as entry points for IREE compilation flow. You can use Python to load and re-export the SavedModel to give it serving signatures. For example, for MobileNet v2 , assuming we want the serving signature to be predict and operating on a 224x224 RGB image: import tensorflow.compat.v2 as tf loaded_model = tf . saved_model . load ( '/path/to/downloaded/model/' ) call = loaded_model . __call__ . get_concrete_function ( tf . TensorSpec ([ 1 , 224 , 224 , 3 ], tf . float32 )) signatures = { 'predict' : call } tf . saved_model . save ( loaded_model , '/path/to/resaved/model/' , signatures = signatures ) The above will create a new SavedModel with a serving signature, predict , and save it to /path/to/resaved/model/ .","title":"TensorFlow"},{"location":"getting-started/tensorflow/#tensorflow-integration","text":"IREE supports compiling and running TensorFlow programs represented as tf.Module classes or stored in the SavedModel format .","title":"TensorFlow Integration"},{"location":"getting-started/tensorflow/#prerequisites","text":"Install TensorFlow by following the official documentation : python -m pip install tf-nightly Install IREE pip packages, either from pip or by building from source : python -m pip install \\ iree-compiler \\ iree-runtime \\ iree-tools-tf Warning The TensorFlow package is currently only available on Linux and macOS. It is not available on Windows yet (see this issue ).","title":"Prerequisites"},{"location":"getting-started/tensorflow/#importing-models","text":"IREE compilers transform a model into its final deployable format in several sequential steps. The first step for a TensorFlow model is to use either the iree-import-tf command-line tool or IREE's Python APIs to import the model into a format (i.e., MLIR ) compatible with the generic IREE compilers.","title":"Importing models"},{"location":"getting-started/tensorflow/#from-savedmodel-on-tensorflow-hub","text":"IREE supports importing and using SavedModels from TensorFlow Hub .","title":"From SavedModel on TensorFlow Hub"},{"location":"getting-started/tensorflow/#using-the-command-line-tool","text":"First download the SavedModel and load it to get the serving signature, which is used as the entry point for IREE compilation flow: import tensorflow.compat.v2 as tf loaded_model = tf . saved_model . load ( '/path/to/downloaded/model/' ) print ( list ( loaded_model . signatures . keys ())) Note If there are no serving signatures in the original SavedModel, you may add them by yourself by following \"Missing serving signature in SavedModel\" . Then you can import the model with iree-import-tf . You can read the options supported via iree-import-tf -help . Using MobileNet v2 as an example and assuming the serving signature is predict : iree-import-tf --tf-import-type = savedmodel_v1 \\ --tf-savedmodel-exported-names = predict \\ /path/to/savedmodel -o iree_input.mlir Tip iree-import-tf is installed as /path/to/python/site-packages/iree/tools/tf/iree-import-tf . You can find out the full path to the site-packages directory via the python -m site command. -tf-import-type needs to match the SavedModel version. You can try both v1 and v2 if you see one of them gives an empty dump. Afterwards you can further compile the model in iree_input.mlir for CPU or GPU .","title":"Using the command-line tool"},{"location":"getting-started/tensorflow/#training","text":"Todo Discuss training","title":"Training"},{"location":"getting-started/tensorflow/#samples","text":"Colab notebooks Training an MNIST digits classifier Edge detection module Pretrained ResNet50 inference TensorFlow Hub Import End-to-end execution tests can be found in IREE's integrations/tensorflow/e2e/ directory.","title":"Samples"},{"location":"getting-started/tensorflow/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"getting-started/tensorflow/#missing-serving-signature-in-savedmodel","text":"Sometimes SavedModels are exported without explicit serving signatures . This happens by default for TensorFlow Hub SavedModels. However, serving signatures are required as entry points for IREE compilation flow. You can use Python to load and re-export the SavedModel to give it serving signatures. For example, for MobileNet v2 , assuming we want the serving signature to be predict and operating on a 224x224 RGB image: import tensorflow.compat.v2 as tf loaded_model = tf . saved_model . load ( '/path/to/downloaded/model/' ) call = loaded_model . __call__ . get_concrete_function ( tf . TensorSpec ([ 1 , 224 , 224 , 3 ], tf . float32 )) signatures = { 'predict' : call } tf . saved_model . save ( loaded_model , '/path/to/resaved/model/' , signatures = signatures ) The above will create a new SavedModel with a serving signature, predict , and save it to /path/to/resaved/model/ .","title":"Missing serving signature in SavedModel"},{"location":"getting-started/tflite/","text":"TFLite Integration \u00b6 IREE supports compiling and running TensorFlow Lite programs stored as TFLite FlatBuffers . These files can be imported into an IREE-compatible format then compiled to a series of backends. Prerequisites \u00b6 Install TensorFlow-Lite specific dependencies using pip: python -m pip install \\ iree-compiler \\ iree-runtime \\ iree-tools-tflite Importing and Compiling \u00b6 IREE's tooling is divided into two components: import and compilation. The import tool converts the TFLite FlatBuffer to an IREE compatible form, validating that only IREE compatible operations remain. Containing a combination of TOSA and IREE operations. The compilation stage generates the bytecode module for a list of targets, which can be executed by IREE. Using Command Line Tools \u00b6 These two stages can be completed entirely via the command line. WORKDIR = \"/tmp/workdir\" TFLITE_URL = \"https://storage.googleapis.com/iree-model-artifacts/tflite-integration-tests/posenet_i8.tflite\" TFLITE_PATH = ${ WORKDIR } /model.tflite IMPORT_PATH = ${ WORKDIR } /tosa.mlir MODULE_PATH = ${ WORKDIR } /module.vmfb # Fetch the sample model wget ${ TFLITE_URL } -O ${ TFLITE_PATH } # Import the sample model to an IREE compatible form iree-import-tflite ${ TFLITE_PATH } -o ${ IMPORT_PATH } # Compile for the CPU backend iree-compile \\ --iree-input-type = tosa \\ --iree-hal-target-backends = llvm-cpu \\ ${ IMPORT_PATH } \\ -o ${ MODULE_PATH } Using the Python API \u00b6 The example below demonstrates downloading, compiling, and executing a TFLite model using the Python API. This includes some initial setup to declare global variables, download the sample module, and download the sample inputs. Declaration of absolute paths for the sample repo and import all required libraries. The default setup uses the CPU backend as the only target. This can be reconfigured to select alternative targets. import iree.compiler.tflite as iree_tflite_compile import iree.runtime as iree_rt import numpy import os import urllib.request from PIL import Image workdir = \"/tmp/workdir\" os . makedirs ( workdir , exist_ok = True ) tfliteFile = \"/\" . join ([ workdir , \"model.tflite\" ]) jpgFile = \"/\" . join ([ workdir , \"input.jpg\" ]) tfliteIR = \"/\" . join ([ workdir , \"tflite.mlir\" ]) tosaIR = \"/\" . join ([ workdir , \"tosa.mlir\" ]) bytecodeModule = \"/\" . join ([ workdir , \"iree.vmfb\" ]) backends = [ \"llvm-cpu\" ] config = \"local-task\" The TFLite sample model and input are downloaded locally. tfliteUrl = \"https://storage.googleapis.com/iree-model-artifacts/tflite-integration-tests/posenet_i8.tflite\" jpgUrl = \"https://storage.googleapis.com/iree-model-artifacts/tflite-integration-tests/posenet_i8_input.jpg\" urllib . request . urlretrieve ( tfliteUrl , tfliteFile ) urllib . request . urlretrieve ( jpgUrl , jpgFile ) Once downloaded we can compile the model for the selected backends. Both the TFLite and TOSA representations of the model are saved for debugging purposes. This is optional and can be omitted. iree_tflite_compile . compile_file ( tfliteFile , input_type = \"tosa\" , output_file = bytecodeModule , save_temp_tfl_input = tfliteIR , save_temp_iree_input = tosaIR , target_backends = backends , import_only = False ) After compilation is completed we configure the VmModule using the local-task configuration and compiled IREE module. config = iree_rt . Config ( \"local-task\" ) context = iree_rt . SystemContext ( config = config ) with open ( bytecodeModule , 'rb' ) as f : vm_module = iree_rt . VmModule . from_flatbuffer ( config . vm_instance , f . read ()) context . add_vm_module ( vm_module ) Finally, the IREE module is loaded and ready for execution. Here we load the sample image, manipulate to the expected input size, and execute the module. By default TFLite models include a single function named 'main'. The final results are printed. im = numpy . array ( Image . open ( jpgFile ) . resize (( 192 , 192 ))) . reshape (( 1 , 192 , 192 , 3 )) args = [ im ] invoke = context . modules . module [ \"main\" ] iree_results = invoke ( * args ) print ( iree_results ) Troubleshooting \u00b6 Failures during the import step usually indicate a failure to lower from TensorFlow Lite's operations to TOSA, the intermediate representation used by IREE. Many TensorFlow Lite operations are not fully supported, particularly those than use dynamic shapes. File an issue to IREE's TFLite model support project . Additional Samples \u00b6 The tflitehub folder in the iree-samples repository contains test scripts to compile, run, and compare various TensorFlow Lite models sourced from TensorFlow Hub . An example smoke test of the TensorFlow Lite C API is available here . Colab notebooks Text classification with TFLite and IREE Todo Issue#3954 : Add documentation for an Android demo using the Java TFLite bindings , once it is complete at not-jenni/iree-android-tflite-demo .","title":"TensorFlow Lite"},{"location":"getting-started/tflite/#tflite-integration","text":"IREE supports compiling and running TensorFlow Lite programs stored as TFLite FlatBuffers . These files can be imported into an IREE-compatible format then compiled to a series of backends.","title":"TFLite Integration"},{"location":"getting-started/tflite/#prerequisites","text":"Install TensorFlow-Lite specific dependencies using pip: python -m pip install \\ iree-compiler \\ iree-runtime \\ iree-tools-tflite","title":"Prerequisites"},{"location":"getting-started/tflite/#importing-and-compiling","text":"IREE's tooling is divided into two components: import and compilation. The import tool converts the TFLite FlatBuffer to an IREE compatible form, validating that only IREE compatible operations remain. Containing a combination of TOSA and IREE operations. The compilation stage generates the bytecode module for a list of targets, which can be executed by IREE.","title":"Importing and Compiling"},{"location":"getting-started/tflite/#using-command-line-tools","text":"These two stages can be completed entirely via the command line. WORKDIR = \"/tmp/workdir\" TFLITE_URL = \"https://storage.googleapis.com/iree-model-artifacts/tflite-integration-tests/posenet_i8.tflite\" TFLITE_PATH = ${ WORKDIR } /model.tflite IMPORT_PATH = ${ WORKDIR } /tosa.mlir MODULE_PATH = ${ WORKDIR } /module.vmfb # Fetch the sample model wget ${ TFLITE_URL } -O ${ TFLITE_PATH } # Import the sample model to an IREE compatible form iree-import-tflite ${ TFLITE_PATH } -o ${ IMPORT_PATH } # Compile for the CPU backend iree-compile \\ --iree-input-type = tosa \\ --iree-hal-target-backends = llvm-cpu \\ ${ IMPORT_PATH } \\ -o ${ MODULE_PATH }","title":"Using Command Line Tools"},{"location":"getting-started/tflite/#using-the-python-api","text":"The example below demonstrates downloading, compiling, and executing a TFLite model using the Python API. This includes some initial setup to declare global variables, download the sample module, and download the sample inputs. Declaration of absolute paths for the sample repo and import all required libraries. The default setup uses the CPU backend as the only target. This can be reconfigured to select alternative targets. import iree.compiler.tflite as iree_tflite_compile import iree.runtime as iree_rt import numpy import os import urllib.request from PIL import Image workdir = \"/tmp/workdir\" os . makedirs ( workdir , exist_ok = True ) tfliteFile = \"/\" . join ([ workdir , \"model.tflite\" ]) jpgFile = \"/\" . join ([ workdir , \"input.jpg\" ]) tfliteIR = \"/\" . join ([ workdir , \"tflite.mlir\" ]) tosaIR = \"/\" . join ([ workdir , \"tosa.mlir\" ]) bytecodeModule = \"/\" . join ([ workdir , \"iree.vmfb\" ]) backends = [ \"llvm-cpu\" ] config = \"local-task\" The TFLite sample model and input are downloaded locally. tfliteUrl = \"https://storage.googleapis.com/iree-model-artifacts/tflite-integration-tests/posenet_i8.tflite\" jpgUrl = \"https://storage.googleapis.com/iree-model-artifacts/tflite-integration-tests/posenet_i8_input.jpg\" urllib . request . urlretrieve ( tfliteUrl , tfliteFile ) urllib . request . urlretrieve ( jpgUrl , jpgFile ) Once downloaded we can compile the model for the selected backends. Both the TFLite and TOSA representations of the model are saved for debugging purposes. This is optional and can be omitted. iree_tflite_compile . compile_file ( tfliteFile , input_type = \"tosa\" , output_file = bytecodeModule , save_temp_tfl_input = tfliteIR , save_temp_iree_input = tosaIR , target_backends = backends , import_only = False ) After compilation is completed we configure the VmModule using the local-task configuration and compiled IREE module. config = iree_rt . Config ( \"local-task\" ) context = iree_rt . SystemContext ( config = config ) with open ( bytecodeModule , 'rb' ) as f : vm_module = iree_rt . VmModule . from_flatbuffer ( config . vm_instance , f . read ()) context . add_vm_module ( vm_module ) Finally, the IREE module is loaded and ready for execution. Here we load the sample image, manipulate to the expected input size, and execute the module. By default TFLite models include a single function named 'main'. The final results are printed. im = numpy . array ( Image . open ( jpgFile ) . resize (( 192 , 192 ))) . reshape (( 1 , 192 , 192 , 3 )) args = [ im ] invoke = context . modules . module [ \"main\" ] iree_results = invoke ( * args ) print ( iree_results )","title":"Using the Python API"},{"location":"getting-started/tflite/#troubleshooting","text":"Failures during the import step usually indicate a failure to lower from TensorFlow Lite's operations to TOSA, the intermediate representation used by IREE. Many TensorFlow Lite operations are not fully supported, particularly those than use dynamic shapes. File an issue to IREE's TFLite model support project .","title":"Troubleshooting"},{"location":"getting-started/tflite/#additional-samples","text":"The tflitehub folder in the iree-samples repository contains test scripts to compile, run, and compare various TensorFlow Lite models sourced from TensorFlow Hub . An example smoke test of the TensorFlow Lite C API is available here . Colab notebooks Text classification with TFLite and IREE Todo Issue#3954 : Add documentation for an Android demo using the Java TFLite bindings , once it is complete at not-jenni/iree-android-tflite-demo .","title":"Additional Samples"},{"location":"reference/glossary/","text":"Glossary \u00b6 IREE exists in an ecosystem of projects, each using their own terminology and interacting in various ways. Below is a summation of these projects with the problems they are built to address. JAX \u00b6 JAX is a front-end for writing and executing Machine Learning models, including support for Google Cloud TPUs. MLIR \u00b6 Multi Level Intermediate Representation is the compiler framework that IREE is built around. Beyond the tooling this includes a set of common dialects and transformations that IREE utilizes for its code generation system. For general discussion on MLIR see the project's discourse group. LinAlg \u00b6 Linalg is an MLIR dialect that defines how Linear Algebra operations can be described in a generalized fashion, including a set of commonly used operations. IREE's code generation defines tensor operations using the Linalg dialect, then uses it to generate the loop structures for the CPU and GPU backends. SPIR-V \u00b6 SPIR-V is a shader and kernel intermediate language for expressing parallel computation typically used for GPUs. It serves as a hardware agnostic assembly format for distributing complex, computationally intensive programs. It is the preferred method for shipping platform agnostic binaries to run on GPUs. TOSA \u00b6 The TOSA specification defines a set of common tensor operations to most machine learning frameworks. This simplifies model compilation as separate front-end frameworks can target TOSA's intermediate representation without compromising on the ability to achieve efficient execution across multiple device types. IREE uses the TOSA MLIR dialect as a prioritized ingestion format, transforming multiple ML-platform ingestion formats into a TOSA compatible set of operations. Changes to the TOSA specification require submitting a proposal on TOSA's platform development page TFLite \u00b6 TensorFlow lite is a model format and execution system for performing on device inference. IREE supports TFLite FlatBuffers translation into TOSA operations and compilation into the LinAlg dialect.","title":"Glossary"},{"location":"reference/glossary/#glossary","text":"IREE exists in an ecosystem of projects, each using their own terminology and interacting in various ways. Below is a summation of these projects with the problems they are built to address.","title":"Glossary"},{"location":"reference/glossary/#jax","text":"JAX is a front-end for writing and executing Machine Learning models, including support for Google Cloud TPUs.","title":"JAX"},{"location":"reference/glossary/#mlir","text":"Multi Level Intermediate Representation is the compiler framework that IREE is built around. Beyond the tooling this includes a set of common dialects and transformations that IREE utilizes for its code generation system. For general discussion on MLIR see the project's discourse group.","title":"MLIR"},{"location":"reference/glossary/#linalg","text":"Linalg is an MLIR dialect that defines how Linear Algebra operations can be described in a generalized fashion, including a set of commonly used operations. IREE's code generation defines tensor operations using the Linalg dialect, then uses it to generate the loop structures for the CPU and GPU backends.","title":"LinAlg"},{"location":"reference/glossary/#spir-v","text":"SPIR-V is a shader and kernel intermediate language for expressing parallel computation typically used for GPUs. It serves as a hardware agnostic assembly format for distributing complex, computationally intensive programs. It is the preferred method for shipping platform agnostic binaries to run on GPUs.","title":"SPIR-V"},{"location":"reference/glossary/#tosa","text":"The TOSA specification defines a set of common tensor operations to most machine learning frameworks. This simplifies model compilation as separate front-end frameworks can target TOSA's intermediate representation without compromising on the ability to achieve efficient execution across multiple device types. IREE uses the TOSA MLIR dialect as a prioritized ingestion format, transforming multiple ML-platform ingestion formats into a TOSA compatible set of operations. Changes to the TOSA specification require submitting a proposal on TOSA's platform development page","title":"TOSA"},{"location":"reference/glossary/#tflite","text":"TensorFlow lite is a model format and execution system for performing on device inference. IREE supports TFLite FlatBuffers translation into TOSA operations and compilation into the LinAlg dialect.","title":"TFLite"},{"location":"reference/optimization-options/","text":"Optimization Options \u00b6 This page documents various supported flags for optimizing IREE programs. Each is presented with its English name, flag to enable/disable, and default state. These flags can be passed to the: iree-compile command line tool extra_args=[\"--flag\"] argument to iree.compiler.tools Python wrappers In-process Python compiler API iree.compiler.transforms.iree-compile.CompilerOptions(\"--flag\", \"--flag2\") constructor ireeCompilerOptionsSetFlags() compiler C API function High level program optimizations \u00b6 Constant evaluation ( --iree-opt-const-eval (off)) \u00b6 Performs compile-time evaluation of any global initializers which produce the initial values for global constants, storing the global directly in the program as constant data. This extracts such constant program fragments and recursively compiles them, using the runtime to evaluate the results. Note that this only has any effect on computations in module initializer functions, not free-standing operations in the program which may produce constant-derived results. See --iree-opt-const-expr-hoisting for options to optimize these. Constant expression hoisting ( --iree-opt-const-expr-hoisting (off)) \u00b6 Identifies all trees of constant expressions in the program and uses a heuristic to determine which would be profitable to hoist into global initializers for evaluation at module load. Together with --iree-opt-const-eval , this will convert eligible trees of expressions to purely static data embedded in the module. The heuristic is currently relatively primitive, using static information to disable hoisting of leaf operations which are metadata only (i.e. broadcasts, etc) or are expected to fold away as part of operator fusion. Notably, the current heuristic is likely to pessimize module size in the case of complicated programs with trees of constant, large tensors. Numeric precision reduction ( --iree-opt-numeric-precision-reduction (off)) \u00b6 Analyzes program constant data and program flow to identify math operations which can be safely evaluated with reduced precision (currently with a minimum of 8bit integers but being extended to infer any bit depth) and inserts appropriate casts. In conjunction with Constant Expression Hoisting , Constant Evaluation and other automatic optimizations, this can produce programs where large amounts (up to the whole) have had their numeric operations and constant data rewritten to lower precision types. This feature is actively evolving and will be the subject of dedicated documentation when ready. Strip Debug Assertions ( --iree-opt-strip-assertions (off)) \u00b6 Strips all std.assert ops in the input program after useful information for optimization analysis has been extracted. Assertions provide useful user-visible error messages but can prevent critical optimizations. Assertions are not, however, a substitution for control flow and frontends that want to check errors in optimized release builds should do so via actual code - similar to when one would if (foo) return false; vs. assert(foo); in a normal program.","title":"Optimization Options"},{"location":"reference/optimization-options/#optimization-options","text":"This page documents various supported flags for optimizing IREE programs. Each is presented with its English name, flag to enable/disable, and default state. These flags can be passed to the: iree-compile command line tool extra_args=[\"--flag\"] argument to iree.compiler.tools Python wrappers In-process Python compiler API iree.compiler.transforms.iree-compile.CompilerOptions(\"--flag\", \"--flag2\") constructor ireeCompilerOptionsSetFlags() compiler C API function","title":"Optimization Options"},{"location":"reference/optimization-options/#high-level-program-optimizations","text":"","title":"High level program optimizations"},{"location":"reference/optimization-options/#constant-evaluation-iree-opt-const-eval-off","text":"Performs compile-time evaluation of any global initializers which produce the initial values for global constants, storing the global directly in the program as constant data. This extracts such constant program fragments and recursively compiles them, using the runtime to evaluate the results. Note that this only has any effect on computations in module initializer functions, not free-standing operations in the program which may produce constant-derived results. See --iree-opt-const-expr-hoisting for options to optimize these.","title":"Constant evaluation (--iree-opt-const-eval (off))"},{"location":"reference/optimization-options/#constant-expression-hoisting-iree-opt-const-expr-hoisting-off","text":"Identifies all trees of constant expressions in the program and uses a heuristic to determine which would be profitable to hoist into global initializers for evaluation at module load. Together with --iree-opt-const-eval , this will convert eligible trees of expressions to purely static data embedded in the module. The heuristic is currently relatively primitive, using static information to disable hoisting of leaf operations which are metadata only (i.e. broadcasts, etc) or are expected to fold away as part of operator fusion. Notably, the current heuristic is likely to pessimize module size in the case of complicated programs with trees of constant, large tensors.","title":"Constant expression hoisting (--iree-opt-const-expr-hoisting (off))"},{"location":"reference/optimization-options/#numeric-precision-reduction-iree-opt-numeric-precision-reduction-off","text":"Analyzes program constant data and program flow to identify math operations which can be safely evaluated with reduced precision (currently with a minimum of 8bit integers but being extended to infer any bit depth) and inserts appropriate casts. In conjunction with Constant Expression Hoisting , Constant Evaluation and other automatic optimizations, this can produce programs where large amounts (up to the whole) have had their numeric operations and constant data rewritten to lower precision types. This feature is actively evolving and will be the subject of dedicated documentation when ready.","title":"Numeric precision reduction (--iree-opt-numeric-precision-reduction (off))"},{"location":"reference/optimization-options/#strip-debug-assertions-iree-opt-strip-assertions-off","text":"Strips all std.assert ops in the input program after useful information for optimization analysis has been extracted. Assertions provide useful user-visible error messages but can prevent critical optimizations. Assertions are not, however, a substitution for control flow and frontends that want to check errors in optimized release builds should do so via actual code - similar to when one would if (foo) return false; vs. assert(foo); in a normal program.","title":"Strip Debug Assertions (--iree-opt-strip-assertions (off))"}]}