# Copyright 2021 The IREE Authors
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

# Tests for common transforms.

load("//build_tools/bazel:enforce_glob.bzl", "enforce_glob")
load("//build_tools/bazel:iree_lit_test.bzl", "iree_lit_test_suite")

package(
    features = ["layering_check"],
    licenses = ["notice"],  # Apache 2.0
)

iree_lit_test_suite(
    name = "lit",
    srcs = enforce_glob(
        [
            "assign_constant_ordinals.mlir",
            "conv_pipeline_test_cuda.mlir",
            "convert_to_nvvm.mlir",
            "convert_to_rocdl.mlir",
            "create_async_groups.mlir",
            "create_tile_sizes.mlir",
            "distribute_to_thread.mlir",
            "elementwise_pipeline.mlir",
            "cast_address_space_function.mlir",
            "cast_type_to_fit_mma.mlir",
            "config_custom_op.mlir",
            "config_horizontally_fused_ops.mlir",
            "config_matvec.mlir",
            "config_root_op_attribute.mlir",
            "config_winograd.mlir",
            "extract_address_computation_gpu.mlir",
            "gpu_set_num_workgroups.mlir",
            "gpu_pipeline_generalize_named_ops.mlir",
            "link_executables.mlir",
            "nvvm_extract_address_computation.mlir",
            "nvvm_pipeline_test.mlir",
            "nvvm_mma_sync_pipeline_test.mlir",
            "reduction_pipeline_cuda.mlir",
            "reduction_pipeline_rocm.mlir",
            "reduction_pipeline_softmax_rocm.mlir",
            "reuse_shared_memory_allocs.mlir",
            "rocdl_pipeline_test.mlir",
            "illegal_configuration.mlir",
            "legalize.mlir",
            "linalg_transform.mlir",
            "llvmgpu_bufferize.mlir",
            "pack_pipeline_test.mlir",
            "pack_shared_memory_alloc.mlir",
            "prefetch_shared_memory.mlir",
            "promote_matmul_to_fit_mma.mlir",
            "tensor_pad.mlir",
            "tensorcore_vectorization.mlir",
            "test_query_mma.mlir",
            "transform_dialect_bufferize.mlir",
            "transform_dialect_eliminate_gpu_barriers.mlir",
            "transform_dialect_hoist_allocs.mlir",
            "transform_dialect_pack_shared_memory_alloc.mlir",
            "transform_dialect_promote_operands.mlir",
            "transform_dialect_vector_distribution.mlir",
            "transform_dialect_vector_to_nvgpu_mma.mlir",
            "transform_distribute_forall.mlir",
            "transform_gpu_pipelining.mlir",
            "transform_vector_to_mma.mlir",
            "transpose_pipeline_test.mlir",
            "configure_tensor_layout.mlir",
            "vector_lowering.mlir",
            "vector_to_gpu.mlir",
            "winograd_pipeline_test.mlir",
        ],
        include = ["*.mlir"],
        # tensor_dialect_*_spec is a an MLIR file that specifies a
        # transformation, it needs to be included as data.
        exclude = [
            "transform_dialect_codegen_bufferize_spec.mlir",
            "transform_dialect_codegen_foreach_to_gpu_spec.mlir",
            "transform_dialect_codegen_vector_distribution_spec.mlir",
            "transform_dialect_codegen_vector_warp_execute_on_lane_0_spec.mlir",
        ],
    ),
    cfg = "//compiler:lit.cfg.py",
    data = [
        "transform_dialect_codegen_bufferize_spec.mlir",
        "transform_dialect_codegen_foreach_to_gpu_spec.mlir",
        "transform_dialect_codegen_vector_distribution_spec.mlir",
        "transform_dialect_codegen_vector_warp_execute_on_lane_0_spec.mlir",
    ],
    tools = [
        "//tools:iree-opt",
        "@llvm-project//llvm:FileCheck",
    ],
)
