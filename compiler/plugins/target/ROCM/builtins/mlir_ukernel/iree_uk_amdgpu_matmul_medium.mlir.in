// Copyright 2026 The IREE Authors
//
// Licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

//  RUN: iree-opt %s
// AUTO-GENERATED - DO NOT EDIT
// Generated from iree_uk_amdgpu_matmul_medium.mlir.in

// LHS types
!lhs_in_ty = tensor<1x${SUBGROUPS_M*INTRINSICS_M*16}x?x${ELEM_TYPE}>
!lhs_block_in = tensor<1x${SUBGROUPS_M*INTRINSICS_M*16}x${8*INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>
!lhs_flat_shared = memref<${SUBGROUPS_M*INTRINSICS_M*128*INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}, #gpu.address_space<workgroup>>
!lhs_shared = memref<${SUBGROUPS_M*INTRINSICS_M*16}x${8*INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}, #gpu.address_space<workgroup>>
!lhs_shared_exp = memref<${SUBGROUPS_M*INTRINSICS_M}x16x${2*INTRINSICS_K}x${4*INTERNAL_K}x${ELEM_TYPE}, #gpu.address_space<workgroup>>

// RHS types
!rhs_in_ty = tensor<${SUBGROUPS_N*INTRINSICS_N*16}x?x${ELEM_TYPE}>
!rhs_block_in = tensor<${SUBGROUPS_N*INTRINSICS_N*16}x${8*INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>
!rhs_flat_shared = memref<${SUBGROUPS_N*INTRINSICS_N*128*INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}, #gpu.address_space<workgroup>>
!rhs_shared = memref<${SUBGROUPS_N*INTRINSICS_N*16}x${8*INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}, #gpu.address_space<workgroup>>
!rhs_shared_exp = memref<${SUBGROUPS_N*INTRINSICS_N}x16x${2*INTRINSICS_K}x${4*INTERNAL_K}x${ELEM_TYPE}, #gpu.address_space<workgroup>>

#contraction_accesses = [
 affine_map<(i, j, k) -> (i, k)>,
 affine_map<(i, j, k) -> (j, k)>,
 affine_map<(i, j, k) -> (i, j)>
]

util.func private @pingpong_medium_${ELEM_TYPE}_expanded(%lhs_base: !lhs_in_ty, %rhs_base: !rhs_in_ty, %unused_acc: tensor<1x${SUBGROUPS_M*INTRINSICS_M*16}x${SUBGROUPS_N*INTRINSICS_N*16}xf32>) -> tensor<1x${SUBGROUPS_M*INTRINSICS_M*16}x${SUBGROUPS_N*INTRINSICS_N*16}xf32> {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c4 = arith.constant 4 : index
  %c8 = arith.constant 8 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c256 = arith.constant 256 : index

  %cst = arith.constant 0.0 : ${ELEM_TYPE}
  %lhs_shared_base = memref.alloc() : !lhs_flat_shared
  %rhs_shared_base = memref.alloc() : !rhs_flat_shared

  %dim = tensor.dim %rhs_base, %c1 : !rhs_in_ty
  %dim_stride = arith.muli %dim, %c${ELEM_BITS//8} overflow<nsw, nuw>: index
  %lhs = iree_gpu.buffer_resource_cast %lhs_base cacheSwizzleStride(%dim_stride) : !lhs_in_ty
  %rhs = iree_gpu.buffer_resource_cast %rhs_base cacheSwizzleStride(%dim_stride) : !rhs_in_ty

  %lhs_shared_swizzle = iree_codegen.swizzle_hint %lhs_shared_base[#iree_codegen.rotate_rows<${1024//ELEM_BITS}, ${64//ELEM_BITS}>] : !lhs_flat_shared
  %rhs_shared_swizzle = iree_codegen.swizzle_hint %rhs_shared_base[#iree_codegen.rotate_rows<${1024//ELEM_BITS}, ${64//ELEM_BITS}>] : !rhs_flat_shared

  %lhs_shared = memref.expand_shape %lhs_shared_swizzle [[0, 1]] output_shape [${SUBGROUPS_M*INTRINSICS_M*16}, ${8*INTRINSICS_K*INTERNAL_K}] : !lhs_flat_shared into !lhs_shared
  %rhs_shared = memref.expand_shape %rhs_shared_swizzle [[0, 1]] output_shape [${SUBGROUPS_N*INTRINSICS_N*16}, ${8*INTRINSICS_K*INTERNAL_K}] : !rhs_flat_shared into !rhs_shared

  %lhs_init = tensor.extract_slice %lhs [0, 0, 0] [1, ${SUBGROUPS_M*INTRINSICS_M*16}, ${8*INTRINSICS_K*INTERNAL_K}] [1, 1, 1] : !lhs_in_ty to !lhs_block_in
  %rhs_init = tensor.extract_slice %rhs [0, 0] [${SUBGROUPS_N*INTRINSICS_N*16}, ${8*INTRINSICS_K*INTERNAL_K}] [1, 1] : !rhs_in_ty to !rhs_block_in

  scf.forall (%id) in (${128*SUBGROUPS_M*INTRINSICS_M}) {
    %delin:2 = affine.delinearize_index %id into (${SUBGROUPS_M*INTRINSICS_M*16}, 8) : index, index
    $assert (INTRINSICS_K*INTERNAL_K) == (128//ELEM_BITS), "load 128 bits per instruction"
    %vec = arith.muli %delin#1, %c${INTRINSICS_K*INTERNAL_K} overflow<nsw, nuw> : index
    %lhs_thread_local = tensor.extract_slice %lhs_init [0, %delin#0, %vec] [1, 1, ${INTRINSICS_K*INTERNAL_K}] [1, 1, 1] : !lhs_block_in to tensor<1x1x${INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>
    %lhs_vec_local = vector.transfer_read %lhs_thread_local [%c0, %c0, %c0], %cst {in_bounds = [true, true]} : tensor<1x1x${INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>, vector<1x${INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>
    vector.transfer_write %lhs_vec_local, %lhs_shared[%delin#0, %vec] {in_bounds = [true, true]} : vector<1x${INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>, !lhs_shared
  } {mapping = [#gpu.thread<linear_dim_0>]}
  scf.forall (%id) in (${128*SUBGROUPS_N*INTRINSICS_N}) {
    %delin:2 = affine.delinearize_index %id into (${SUBGROUPS_N*INTRINSICS_N*16}, 8) : index, index
    $assert (INTRINSICS_K*INTERNAL_K) == (128//ELEM_BITS), "load 128 bits per instruction"
    %vec = arith.muli %delin#1, %c${INTRINSICS_K*INTERNAL_K} overflow<nsw, nuw> : index
    %rhs_thread_local = tensor.extract_slice %rhs_init [%delin#0, %vec] [1, ${INTRINSICS_K*INTERNAL_K}] [1, 1] : !rhs_block_in to tensor<1x${INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>
    %rhs_vec_local = vector.transfer_read %rhs_thread_local [%c0, %c0], %cst {in_bounds = [true, true]} : tensor<1x${INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>, vector<1x${INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>
    vector.transfer_write %rhs_vec_local, %rhs_shared[%delin#0, %vec] {in_bounds = [true, true]} : vector<1x${INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>, !rhs_shared
  } {mapping = [#gpu.thread<linear_dim_0>]}

  %lhs_shared_expand = memref.expand_shape %lhs_shared [[0, 1], [2, 3]] output_shape [${SUBGROUPS_M*INTRINSICS_M}, 16, ${2*INTRINSICS_K}, ${4*INTERNAL_K}] : !lhs_shared into !lhs_shared_exp
  %rhs_shared_expand = memref.expand_shape %rhs_shared [[0, 1], [2, 3]] output_shape [${SUBGROUPS_N*INTRINSICS_N}, 16, ${2*INTRINSICS_K}, ${4*INTERNAL_K}] : !rhs_shared into !rhs_shared_exp

  %0 = tensor.empty() : tensor<1x${SUBGROUPS_M*INTRINSICS_M}x16x${SUBGROUPS_N*INTRINSICS_N}x16xf32>
  $assert (SUBGROUPS_M*SUBGROUPS_N) == 8, "workgroup size must be 8x64=512"
  %1 = scf.forall (%id) in (${SUBGROUPS_M*SUBGROUPS_N*64}) shared_outs(%out = %0) -> tensor<1x${SUBGROUPS_M*INTRINSICS_M}x16x${SUBGROUPS_N*INTRINSICS_N}x16xf32> {
    %ids:4 = affine.delinearize_index %id into (${SUBGROUPS_M}, ${SUBGROUPS_N}, 4, 16) : index, index, index, index
    %m_outer_id = arith.muli %ids#0, %c${INTERNAL_M} overflow<nsw, nuw> : index
    %n_outer_id = arith.muli %ids#1, %c${INTERNAL_N} overflow<nsw, nuw> : index
    %inner_id = arith.muli %ids#2, %c${INTERNAL_K} overflow<nsw, nuw> : index
    %inner_id_acc = arith.muli %ids#2, %c4 overflow<nsw, nuw> : index

    %wt:3 = affine.delinearize_index %id into (${SUBGROUPS_M*SUBGROUPS_N}, 8, 8) : index, index, index
    %bpo_lhs = arith.muli %wt#0, %c{SUBGROUPS_M*INTRINSICS_M*16//SUBGROUPS_M//SUBGROUPS_N} overflow<nsw, nuw> : index
    %bpo_rhs = arith.muli %wt#0, %c{SUBGROUPS_N*INTRINSICS_N*16//SUBGROUPS_M//SUBGROUPS_N} overflow<nsw, nuw> : index
    %glb0_lhs = arith.addi %wt#1, %bpo_lhs overflow<nsw, nuw> : index
    %glb0_rhs = arith.addi %wt#1, %bpo_rhs overflow<nsw, nuw> : index
    $for j in range(1, SUBGROUPS_M*INTRINSICS_M*2//SUBGROUPS_M//SUBGROUPS_N):
      %glb${j}_lhs = arith.addi %glb${j-1}_lhs, %c8 overflow<nsw, nuw> : index
    $for j in range(1, SUBGROUPS_N*INTRINSICS_N*2//SUBGROUPS_M//SUBGROUPS_N):
      %glb${j}_rhs = arith.addi %glb${j-1}_rhs, %c8 overflow<nsw, nuw> : index
    %gko = arith.muli %wt#2, %c${INTRINSICS_K*INTERNAL_K} overflow<nsw, nuw> : index

    %2 = arith.constant dense<0.0> : vector<${INTRINSICS_M}x${INTRINSICS_N}x1x4xf32>

    %cmp0 = arith.cmpi slt, %id, %c${SUBGROUPS_M*SUBGROUPS_N*32} : index
    %cmp1 = arith.cmpi sge, %id, %c${SUBGROUPS_M*SUBGROUPS_N*32} : index
    scf.if %cmp0 {
      rocdl.s.barrier
    }
    %3 = scf.for %i = %c${8*INTRINSICS_K*INTERNAL_K} to %dim step %c${8*INTRINSICS_K*INTERNAL_K} iter_args(%iter = %2) -> vector<${INTRINSICS_M}x${INTRINSICS_N}x1x4xf32> {

      %lhs_vec_0 = vector.transfer_read %lhs_shared_expand[%m_outer_id, %ids#3, %c0, %inner_id], %cst {in_bounds = [true, true, true, true]} : !lhs_shared_exp, vector<${INTRINSICS_M}x1x${INTRINSICS_K}x${INTERNAL_K}x${ELEM_TYPE}>
      %rhs_vec_0 = vector.transfer_read %rhs_shared_expand[%n_outer_id, %ids#3, %c0, %inner_id], %cst {in_bounds = [true, true, true, true]} : !rhs_shared_exp, vector<${INTRINSICS_N}x1x${INTRINSICS_K}x${INTERNAL_K}x${ELEM_TYPE}>
      %lhs_vec_0_t = vector.transpose %lhs_vec_0, [0, 2, 1, 3] : vector<${INTRINSICS_M}x1x${INTRINSICS_K}x${INTERNAL_K}x${ELEM_TYPE}> to vector<${INTRINSICS_M}x${INTRINSICS_K}x1x${INTERNAL_K}x${ELEM_TYPE}>
      %rhs_vec_0_t = vector.transpose %rhs_vec_0, [0, 2, 1, 3] : vector<${INTRINSICS_N}x1x${INTRINSICS_K}x${INTERNAL_K}x${ELEM_TYPE}> to vector<${INTRINSICS_N}x${INTRINSICS_K}x1x${INTERNAL_K}x${ELEM_TYPE}>

      rocdl.sched.barrier 0

      // Global loads of rhs.
      %rhs_block = tensor.extract_slice %rhs [0, %i] [${SUBGROUPS_N*INTRINSICS_N*16}, ${8*INTRINSICS_K*INTERNAL_K}] [1, 1] : !rhs_in_ty to !rhs_block_in
      $for j in range(SUBGROUPS_N*INTRINSICS_N*2//SUBGROUPS_M//SUBGROUPS_N):
        %rhs_thread_${j} = tensor.extract_slice %rhs_block [%glb${j}_rhs, %gko] [1, ${INTRINSICS_K*INTERNAL_K}] [1, 1] : !rhs_block_in to tensor<1x${INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>
        %rhs_vec_local_${j} = vector.transfer_read %rhs_thread_${j} [%c0, %c0], %cst {in_bounds = [true, true]} : tensor<1x${INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>, vector<1x${INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>

      rocdl.sched.barrier 0

      %lhs_vec_2 = vector.transfer_read %lhs_shared_expand[%m_outer_id, %ids#3, %c2, %inner_id], %cst {in_bounds = [true, true, true, true]} : !lhs_shared_exp, vector<${INTRINSICS_M}x1x${INTRINSICS_K}x${INTERNAL_K}x${ELEM_TYPE}>
      %rhs_vec_2 = vector.transfer_read %rhs_shared_expand[%n_outer_id, %ids#3, %c2, %inner_id], %cst {in_bounds = [true, true, true, true]} : !rhs_shared_exp, vector<${INTRINSICS_N}x1x${INTRINSICS_K}x${INTERNAL_K}x${ELEM_TYPE}>
      %lhs_vec_2_t = vector.transpose %lhs_vec_2, [0, 2, 1, 3] : vector<${INTRINSICS_M}x1x${INTRINSICS_K}x${INTERNAL_K}x${ELEM_TYPE}> to vector<${INTRINSICS_M}x${INTRINSICS_K}x1x${INTERNAL_K}x${ELEM_TYPE}>
      %rhs_vec_2_t = vector.transpose %rhs_vec_2, [0, 2, 1, 3] : vector<${INTRINSICS_N}x1x${INTRINSICS_K}x${INTERNAL_K}x${ELEM_TYPE}> to vector<${INTRINSICS_N}x${INTRINSICS_K}x1x${INTERNAL_K}x${ELEM_TYPE}>

      rocdl.sched.barrier 0

      // Global loads of lhs.
      %lhs_block = tensor.extract_slice %lhs [0, 0, %i] [1, ${SUBGROUPS_M*INTRINSICS_M*16}, ${8*INTRINSICS_K*INTERNAL_K}] [1, 1, 1] : !lhs_in_ty to !lhs_block_in
      $for j in range(1, SUBGROUPS_M*INTRINSICS_M*2//SUBGROUPS_M//SUBGROUPS_N):
        %lhs_thread_${j} = tensor.extract_slice %lhs_block [0, %glb${j}_lhs, %gko] [1, 1, ${INTRINSICS_K*INTERNAL_K}] [1, 1, 1] : !lhs_block_in to tensor<1x1x${INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>
        %lhs_vec_local_${j} = vector.transfer_read %lhs_thread_${j} [%c0, %c0, %c0], %cst {in_bounds = [true, true]} : tensor<1x1x${INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>, vector<1x${INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>

      gpu.barrier
      rocdl.sched.barrier 0
      rocdl.s.setprio 1 { iree_gpu.swap_mfma = 1 }

      %dot0 = iree_codegen.inner_tiled ins(%lhs_vec_0_t, %rhs_vec_0_t) outs(%iter) {
        indexing_maps = #contraction_accesses,
        iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>, #linalg.iterator_type<reduction>],
        kind = #iree_gpu.mma_layout<${INTRINSIC}, col_major = true>,
        semantics = #iree_gpu.mma_semantics<distributed = true, opaque = false>
      } : vector<${INTRINSICS_M}x${INTRINSICS_K}x1x${INTERNAL_K}x${ELEM_TYPE}>, vector<${INTRINSICS_N}x${INTRINSICS_K}x1x${INTERNAL_K}x${ELEM_TYPE}> into vector<${INTRINSICS_M}x${INTRINSICS_N}x1x4xf32>

      rocdl.s.setprio 0
      gpu.barrier
      rocdl.sched.barrier 0

      $for j in range(SUBGROUPS_N*INTRINSICS_N*2//SUBGROUPS_M//SUBGROUPS_N):
        vector.transfer_write %rhs_vec_local_${j}, %rhs_shared [%glb${j}_rhs, %gko] {in_bounds = [true, true]} : vector<1x${INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>, !rhs_shared

      $for j in range(1, SUBGROUPS_M*INTRINSICS_M*2//SUBGROUPS_M//SUBGROUPS_N):
        vector.transfer_write %lhs_vec_local_${j}, %lhs_shared [%glb${j}_lhs, %gko] {in_bounds = [true, true]} : vector<1x${INTRINSICS_K*INTERNAL_K}x${ELEM_TYPE}>, !lhs_shared

      gpu.barrier
      rocdl.sched.barrier 0
      rocdl.s.setprio 1 { iree_gpu.swap_mfma = 1 }

      %dot2 = iree_codegen.inner_tiled ins(%lhs_vec_2_t, %rhs_vec_2_t) outs(%dot0) {
        indexing_maps = #contraction_accesses,
        iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>, #linalg.iterator_type<reduction>],
        kind = #iree_gpu.mma_layout<${INTRINSIC}, col_major = true>,
        semantics = #iree_gpu.mma_semantics<distributed = true, opaque = false>
      } : vector<${INTRINSICS_M}x${INTRINSICS_K}x1x${INTERNAL_K}x${ELEM_TYPE}>, vector<${INTRINSICS_N}x${INTRINSICS_K}x1x${INTERNAL_K}x${ELEM_TYPE}> into vector<${INTRINSICS_M}x${INTRINSICS_N}x1x4xf32>

      rocdl.s.setprio 0
      gpu.barrier
      rocdl.sched.barrier 0

      scf.yield %dot2 : vector<${INTRINSICS_M}x${INTRINSICS_N}x1x4xf32>
    }
    scf.if %cmp1 {
      rocdl.s.barrier
    }

    // Epilogue
    %lhs_vec_0 = vector.transfer_read %lhs_shared_expand[%m_outer_id, %ids#3, %c0, %inner_id], %cst {in_bounds = [true, true, true, true]} : !lhs_shared_exp, vector<${INTRINSICS_M}x1x${INTRINSICS_K}x${INTERNAL_K}x${ELEM_TYPE}>
    %rhs_vec_0 = vector.transfer_read %rhs_shared_expand[%n_outer_id, %ids#3, %c0, %inner_id], %cst {in_bounds = [true, true, true, true]} : !rhs_shared_exp, vector<${INTRINSICS_N}x1x${INTRINSICS_K}x${INTERNAL_K}x${ELEM_TYPE}>
    %lhs_vec_0_t = vector.transpose %lhs_vec_0, [0, 2, 1, 3] : vector<${INTRINSICS_M}x1x${INTRINSICS_K}x${INTERNAL_K}x${ELEM_TYPE}> to vector<${INTRINSICS_M}x${INTRINSICS_K}x1x${INTERNAL_K}x${ELEM_TYPE}>
    %rhs_vec_0_t = vector.transpose %rhs_vec_0, [0, 2, 1, 3] : vector<${INTRINSICS_N}x1x${INTRINSICS_K}x${INTERNAL_K}x${ELEM_TYPE}> to vector<${INTRINSICS_N}x${INTRINSICS_K}x1x${INTERNAL_K}x${ELEM_TYPE}>

    %dot0 = iree_codegen.inner_tiled ins(%lhs_vec_0_t, %rhs_vec_0_t) outs(%3) {
      indexing_maps = #contraction_accesses,
      iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>, #linalg.iterator_type<reduction>],
      kind = #iree_gpu.mma_layout<${INTRINSIC}, col_major = true>,
      semantics = #iree_gpu.mma_semantics<distributed = true, opaque = false>
    } : vector<${INTRINSICS_M}x${INTRINSICS_K}x1x${INTERNAL_K}x${ELEM_TYPE}>, vector<${INTRINSICS_N}x${INTRINSICS_K}x1x${INTERNAL_K}x${ELEM_TYPE}> into vector<${INTRINSICS_M}x${INTRINSICS_N}x1x4xf32>

    %lhs_vec_2 = vector.transfer_read %lhs_shared_expand[%m_outer_id, %ids#3, %c2, %inner_id], %cst {in_bounds = [true, true, true, true]} : !lhs_shared_exp, vector<${INTRINSICS_M}x1x${INTRINSICS_K}x${INTERNAL_K}x${ELEM_TYPE}>
    %rhs_vec_2 = vector.transfer_read %rhs_shared_expand[%n_outer_id, %ids#3, %c2, %inner_id], %cst {in_bounds = [true, true, true, true]} : !rhs_shared_exp, vector<${INTRINSICS_N}x1x${INTRINSICS_K}x${INTERNAL_K}x${ELEM_TYPE}>
    %lhs_vec_2_t = vector.transpose %lhs_vec_2, [0, 2, 1, 3] : vector<${INTRINSICS_M}x1x${INTRINSICS_K}x${INTERNAL_K}x${ELEM_TYPE}> to vector<${INTRINSICS_M}x${INTRINSICS_K}x1x${INTERNAL_K}x${ELEM_TYPE}>
    %rhs_vec_2_t = vector.transpose %rhs_vec_2, [0, 2, 1, 3] : vector<${INTRINSICS_N}x1x${INTRINSICS_K}x${INTERNAL_K}x${ELEM_TYPE}> to vector<${INTRINSICS_N}x${INTRINSICS_K}x1x${INTERNAL_K}x${ELEM_TYPE}>

    %dot2 = iree_codegen.inner_tiled ins(%lhs_vec_2_t, %rhs_vec_2_t) outs(%dot0) {
      indexing_maps = #contraction_accesses,
      iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>, #linalg.iterator_type<reduction>],
      kind = #iree_gpu.mma_layout<${INTRINSIC}, col_major = true>,
      semantics = #iree_gpu.mma_semantics<distributed = true, opaque = false>
    } : vector<${INTRINSICS_M}x${INTRINSICS_K}x1x${INTERNAL_K}x${ELEM_TYPE}>, vector<${INTRINSICS_N}x${INTRINSICS_K}x1x${INTERNAL_K}x${ELEM_TYPE}> into vector<${INTRINSICS_M}x${INTRINSICS_N}x1x4xf32>

    %tp = vector.transpose %dot2, [0, 2, 1, 3] : vector<${INTRINSICS_M}x${INTRINSICS_N}x1x4xf32> to vector<${INTRINSICS_M}x1x${INTRINSICS_N}x4xf32>
    %empty = tensor.empty() : tensor<1x${INTRINSICS_M}x1x${INTRINSICS_N}x4xf32>
    %4 = vector.transfer_write %tp, %empty[%c0, %c0, %c0, %c0, %c0] {in_bounds = [true, true, true, true]} : vector<${INTRINSICS_M}x1x${INTRINSICS_N}x4xf32>, tensor<1x${INTRINSICS_M}x1x${INTRINSICS_N}x4xf32>
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %4 into %out[0, %m_outer_id, %ids#3, %n_outer_id, %inner_id_acc] [1, ${INTRINSICS_M}, 1, ${INTRINSICS_N}, 4] [1, 1, 1, 1, 1] : tensor<1x${INTRINSICS_M}x1x${INTRINSICS_N}x4xf32> into tensor<1x${SUBGROUPS_M*INTRINSICS_M}x16x${SUBGROUPS_N*INTRINSICS_N}x16xf32>
    }
  } {mapping = [#gpu.thread<linear_dim_0>]}
  %collapse = tensor.collapse_shape %1 [[0], [1, 2], [3, 4]] : tensor<1x${SUBGROUPS_M*INTRINSICS_M}x16x${SUBGROUPS_N*INTRINSICS_N}x16xf32> into tensor<1x${SUBGROUPS_M*INTRINSICS_M*16}x${SUBGROUPS_N*INTRINSICS_N*16}xf32>
  util.return %collapse : tensor<1x${SUBGROUPS_M*INTRINSICS_M*16}x${SUBGROUPS_N*INTRINSICS_N*16}xf32>
}
